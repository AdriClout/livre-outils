% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{scrbook}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{siunitx}

  \newcolumntype{d}{S[
    input-open-uncertainty=,
    input-close-uncertainty=,
    parse-numbers = false,
    table-align-text-pre=false,
    table-align-text-post=false
  ]}
  
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Outils de recherche en sciences sociales numériques},
  pdfauthor={Chaire de leadership en enseignement des sciences sociales numériques (CLESSN)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Outils de recherche en sciences sociales numériques}
\author{Chaire de leadership en enseignement des sciences sociales
numériques (CLESSN)}
\date{2024-03-12}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, enhanced, frame hidden, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{avant-propos}{%
\chapter*{Avant-propos}\label{avant-propos}}
\addcontentsline{toc}{chapter}{Avant-propos}

\markboth{Avant-propos}{Avant-propos}

Ceci est un exemple de citation Adcock and Collier (2001) .

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

\bookmarksetup{startatroot}

\hypertarget{donnuxe9es-massives-causalituxe9-et-sciences-sociales-changements-et-ruxe9flexions-sur-lavenir}{%
\chapter{Données massives, causalité et sciences sociales : Changements
et réflexions sur
l'avenir}\label{donnuxe9es-massives-causalituxe9-et-sciences-sociales-changements-et-ruxe9flexions-sur-lavenir}}

L'apparition des données massives (\emph{big data}) dans le paysage
technologique représente un cas de phénomène hautement technique dont
les effets politiques et sociaux sont remarquables. Depuis quelques
années, la discussion publique s'est en effet rapidement emparée du
sujet, au point de transformer un développement technologique en
phénomène social. Les données massives se trouvent ainsi régulièrement
présentées dans l'espace public à la fois comme un moyen puissant de
développement et d'innovation technoscientifique, de même que comme une
menace à la stabilité de certaines normes sociales telles que la
confidentialité des informations privées. Il n'est d'ailleurs pas rare
que le discours public s'inquiète du danger que poseraient les données
massives à la séparation des sphères publique et privée, pourtant
centrale à la conception libérale du rôle de la politique qui structure
la majorité des débats sociaux, en amalgamant parfois de manière trop
rapide l'objet et l'utilisation qui en est faite. Toutefois, ce même
discours public s'emporte aussi rapidement à propos des gains
technologiques monumentaux réalisés par l'utilisation des données
massives.

Dans le domaine des sciences sociales, les avancées dues à l'utilisation
des données massives se font de plus en plus fréquentes et l'impact des
données massives dans le domaine de la recherche sociale est en ce sens
indéniable. Toutefois, d'un point de vue épistémologique, l'utilisation
des données massives en recherche en sciences sociales dans les
dernières années laisse plusieurs questions ouvertes dans son sillage.

Comment l'utilisation des données massives change-t-elle la pratique des
sciences sociales? Les données massives causeront-elles un changement de
paradigme scientifique?

Ce chapitre ne prétend pas offrir de réponses définitives à ces
questions, mais plutôt des pistes de réflexion par le biais d'une
introduction critique de certains points relatifs aux impacts des
données massives sur la recherche en sciences sociales. Premièrement,
nous présentons une conceptualisation des données massives.
Deuxièmement, nous nous penchons sur les impacts des données massives en
sciences sociales et soulignons tout particulièrement comment elles
affectent les enjeux de la \emph{validité} interne et externe dans le
domaine des sciences sociales. Cela nous offre aussi l'opportunité
d'aborder le sujet important de la différence entre les données
expérimentales et observationnelles. Finalement, nous proposons quelques
pistes de réflexion sur l'avenir des données massives en sciences
sociales en identifiant certains changements \emph{épistémologiques} que
ces données pourraient potentiellement entraîner.

\hypertarget{duxe9finition-des-donnuxe9es-massives}{%
\section{Définition des données
massives}\label{duxe9finition-des-donnuxe9es-massives}}

Il existe au moins trois approches conceptuelles permettant de définir
les « données massives » (voir Figure 1.1.).

\includegraphics{images/chapitre1_definitions.png} 1. Premièrement, les
données massives représentent une \textbf{\emph{quantité importante de
points d'information}} qui varient selon la nature, le type, la source,
etc. Ici, la distinction entre données massives et données plus
traditionnelles (ou « non-massives ») est simplement quantitative.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Deuxièmement, les données massives constituent un
  \textbf{\emph{ensemble de pratiques}} de collecte, de traitement et
  d'analyse de ces points d'information. Les données massives
  représentent une technique, c'est-à-dire une manière ou une méthode
  nouvelle de faire de la recherche.
\item
  Finalement, d'une perspective sociologique, les données massives
  représentent les impacts sociaux de ces importants développements
  technologiques. Cette perspective souligne le caractère
  essentiellement social des données massives, en portant notamment
  attention aux risques liés à la confidentialité des données, aux
  enjeux relatifs au consentement et à l'autorisation de collecte des
  informations, aux innovations en intelligence artificielle, etc.
\end{enumerate}

Dans les domaines scientifiques et technologiques, la définition
courante attribuée aux données massives intègre des éléments de ces
trois niveaux d'analyse en se référant à la composition et à la fonction
des données. Premièrement, la \emph{composition} des données massives
est généralement conceptualisée comme comprenant « 4V » : le volume, la
variété, la vélocité et la véracité. Cette conceptualisation jouit d'un
large consensus scientifique (Chen, Mao et Liu, 2014; Gandomi et Haider,
2015; Kitchin et McArdle 2016). Par ailleurs, plusieurs chercheurs ont
élargi cette définition de la composition des données massives en y
incluant, par exemple, la variabilité et la valeur des points de données
(Kitchen et McArdle 2016). Deuxièmement, la \emph{fonction} des données
massives comprend les innovations relatives à l'optimisation, à la prise
de décision et à l'approfondissement des connaissances qui résultent de
leur utilisation. Ces fonctions touchent des domaines sociaux
disparates, incluant le souci d'efficacité et de rendement des secteurs
privé et public ainsi que la recherche scientifique pure (Gartner 2012).

\hypertarget{les-donnuxe9es-massives-et-les-sciences-sociales}{%
\section{Les données massives et les sciences
sociales}\label{les-donnuxe9es-massives-et-les-sciences-sociales}}

Dans le domaine des sciences sociales, les changements causés par
l'utilisation des données massives en recherche sont significatifs.
Plusieurs n'hésitent d'ailleurs pas à les qualifier de changements de
paradigme dans l'étude des phénomènes sociaux (Anderson 2008; Chandler
2015; Grimmer 2015; Kitchin 2014; Monroe et al.~2015). Dans le cas qui
nous intéresse, deux dimensions majeures méritent d'être abordées : (1)
une première relative à la validité (interne et externe) des données
massives et (2) une seconde relative à la différence entre les données
expérimentales et les données observationnelles. Ces deux dimensions
sont présentées de manière simultanées dans les prochaines sections.

\hypertarget{la-validituxe9-de-la-mesure-en-sciences-sociales}{%
\subsection{La validité de la mesure en sciences
sociales}\label{la-validituxe9-de-la-mesure-en-sciences-sociales}}

La validité de la mesure constitue une exigence méthodologique centrale
à la recherche en sciences sociales. Les scientifiques cherchent
effectivement à s'assurer que ce qui est mesuré --- par un sondage, une
entrevue, un thermostat ou tout autre outil de mesure --- constitue bel
et bien ce qui est censé être mesuré. Adcock et Collier définissent plus
spécifiquement l'application de la validité de la mesure en sciences
sociales en affirmant que des scores (y compris les résultats de
classification qualitative) doivent capturer de manière significative
les idées contenues dans le concept correspondant (2001: 530).

Toutefois, les problèmes liés à la validité de la mesure sont nombreux
et ont une importance considérable. Dans l'étude des phénomènes sociaux
et humains, la validité de la mesure prend d'ailleurs une complexité
supplémentaire du fait que les données collectées par le biais d'une
mesure constituent le \emph{produit de l'observation} d'un phénomène,
mais non pas le phénomène en soi. Ainsi, lorsque, dans le contexte d'une
recherche, on propose de mesurer l'humeur de l'opinion publique (le
phénomène en soi) sur un enjeu politique, on utilise généralement un
sondage qui a pour fonction de mesurer le pouls d'un échantillon de la
population d'intérêt (ce qui est réellement observé). Cependant, ce que
ce sondage mesure ne constitue pas tout à fait l'opinion publique
elle-même, mais plutôt un segment populationnel qui se veut le plus
souvent représentatif de l'humeur de l'opinion publique. Ceci est tout
aussi vrai pour les sondages à petits échantillons que pour ceux
utilisant des données massives. Autrement dit, la mesure et les données
collectées ne représentent pas le phénomène --- l'opinion publique ---
en soi.

On a déjà mentionné que la validité de la mesure a de l'importance
puisqu'elle garantit que ce qui est mesuré représente réellement ce
qu'on croit mesurer. Toutefois, pour être plus spécifique, dans une
approche positiviste, la validité de la mesure se traduit généralement
par une logique de classification des valeurs attribuées aux différentes
manifestations distinctes d'un même phénomène. Par exemple, une mesure
de la démocratie comme celle proposée par \emph{Freedom House},
fréquemment utilisée en science politique, classifie les libertés
civiles et les droits politiques des États du monde par degré afin de
construire un index, ou une échelle, allant d'un autoritarisme complet à
une démocratie parfaite. Les scores représentent, dans ce contexte, une
mesure artificielle, mais ordonnée et logique, des idées contenues dans
le concept de démocratie telles que libertés civiles et droits
politiques. On peut ainsi dire que la question de la validité de la
mesure est un élément central de ce qui unit (1) le phénomène social
étudié (la démocratie), (2) son opérationnalisation (via les libertés
civiles et droits politiques) et (3) la méthode de mesure utilisée pour
observer et classifier d'une certaine façon le phénomène et les données
qui en découlent (dans le cas de \emph{Freedom House}, des codeurs
travaillant de manière indépendante les uns des autres).

\hypertarget{la-validituxe9-des-donnuxe9es-massives}{%
\subsection{La validité des données
massives}\label{la-validituxe9-des-donnuxe9es-massives}}

En ce qui a trait aux données massives, la question de la validité de la
mesure constitue un défi nouveau. Les données massives ont en effet
comme avantage d'offrir aux chercheur.e.s soit de nouveaux phénomènes à
étudier, soit de nouvelles manifestations et nouvelles formes à des
phénomènes déjà étudiés. Les données massives permettent donc d'agrandir
la connaissance scientifique.

L'étude de King et al.~(2013) représente un cas éclairant de phénomène
social que l'utilisation des données massives permet désormais
d'étudier. En se basant sur la collecte de plus de 11 millions de
publications en ligne, King et ses collègues ont pu mesurer la censure
exercée par le gouvernement chinois sur ces réseaux sociaux. En
utilisant des données massives nouvelles, les auteurs ont donc pu
observer une manifestation inédite de censure massive qui, sans de
telles données, serait probablement demeurée mal comprise d'une
perspective scientifique. Le nombre de recherches basées sur
l'utilisation des données massives similairement innovantes en sciences
sociales est par ailleurs en croissance constante (Beauchamp 2017; Bond
et al.~2012; Poirier et al.~2020; Bibeau et al.~2021).

Cependant, il faut aussi souligner que les données massives, en raison
de leur complexité, peuvent avoir pour désavantage d'embrouiller l'étude
des phénomènes sociaux. Les opportunités scientifiques liées aux données
massives s'accompagnent en effet de certaines difficultés
méthodologiques. Parmi ces difficultés, trois enjeux sont
particulièrement cruciaux : (1) la validité interne, (2) la validité
externe et (3) la question d'un changement de posture ou d'orientation
épistémologique en sciences sociales causé par les données massives.

\hypertarget{validituxe9-interne-des-donnuxe9es-massives}{%
\subsubsection{Validité interne des données
massives}\label{validituxe9-interne-des-donnuxe9es-massives}}

Premièrement, les données massives peuvent représenter un défi à la
validité interne des études en sciences sociales en rendant
pragmatiquement difficile l'établissement de \textbf{\emph{mécanismes
causaux clairs}}. Ce défi est notamment une conséquence du fait que la
plupart des données sont présentement issues d'un processus de
génération (\emph{data-generating process}) qui est hors du contrôle des
chercheur.e.s. Les données massives proviennent en effet habituellement
de sources diverses qui sont externes aux projets de recherche qui les
utilisent. Elles ne sont pas donc générées de manière aléatoire sous le
contrôle des chercheur.e.s.

Un des problèmes liés à cette situation est qu'il est difficile de
garantir une source \emph{exogène} de variation par laquelle les
chercheur.e.s éliminent l'effet potentiel des facteurs confondants
(\emph{confounders}). Règle générale, la distribution aléatoire d'un
traitement et d'un contrôle dans une expérience en laboratoire ou sur le
terrain représente le standard le plus élevé permettant de fournir cette
source exogène de variation, notamment parce qu'elle l'attribution
aléatoire du traitement ou du contrôle est entièrement sous le contrôle
du chercheur.e.s menant l'expérience. Cependant, en ce qui à trait à la
plupart des données massives, elles sont générées de manière
indépendante du contrôle du chercheur.e.s, et sont donc soumises aux
mêmes enjeux et problèmes (biais) que les données observationnelles
traditionnelles.

Pour le dire autrement, le défi de validité interne avec les données
massives constitue un enjeu relatif à la qualité des données. Ce n'est
évidemment pas un défi propre ou unique aux données massives. Ce défi
s'applique également aux autres types de données. Cependant, dans l'état
actuel des choses, le volume et la variété --- deux des 4V --- des
données massives --- textuelles, numériques, vidéos, etc. --- peuvent
miner la qualité de l'inférence causale entre une cause et une
conséquence que permet habituellement un processus contrôlé de
génération des données. En somme, la validité interne des données
massives est une fonction de la qualité de ces mêmes données.

\hypertarget{validituxe9-externe-des-donnuxe9es-massives}{%
\subsubsection{Validité externe des données
massives}\label{validituxe9-externe-des-donnuxe9es-massives}}

Deuxièmement, les données massives représentent aussi un défi important
pour la validité externe des recherches en sciences sociales (Tufekci
2014; Lazer et Radford 2017; Nagler et Tucker 2015). Un des problèmes
les plus évidents concerne la \textbf{\emph{représentativité}} des
données massives collectées.

Comme le soulignent Lazer et Radford (2017), la quantité de données, en
soi, ne permet pas de corriger pour la non-représentativité des données.
Les données massives sont ainsi soumises au même problème de biais de
sélection que les autres types de données observationnelles, tels un
sondage ou une série d'entrevues, traditionnellement utilisés en
sciences sociales.

Le cas célèbre de l'erreur de prédiction du \emph{Literary Digest} lors
de la campagne présidentielle américaine de 1936 illustre bien ce
problème. Lors de cette campagne, le \emph{Literary Digest} a prédit à
tort la victoire du candidat républicain Alf Landon sur le président
démocrate sortant Franklin D. Roosevelt, puisque son échantillon de
répondants surreprésentait les électeurs plus aisés, traditionnellement
plus républicains, au détriment des électeurs moins aisés, plus
généralement proches du Parti démocrate. Cette erreur de
surreprésentation dans l'échantillon est due au fait que le
\emph{Literary Digest} a effectué un échantillonnage basé sur les listes
téléphoniques et le registre des propriétaires de voitures, biaisant par
le fait même l'échantillon au détriment des électeurs plus pauvres ne
possédant pas de téléphone ou d'automobile, mais qui constituaient un
électorat favorable à Roosevelt (Squire 1981). Le biais de sélection du
sondage a ainsi sous-estimé le soutien populaire de Roosevelt de plus de
20 points de pourcentage.

Aujourd'hui, l'utilisation des données massives est soumise aux mêmes
enjeux méthodologiques. L'accumulation massive de données ne permet pas
de compenser pour la qualité des données. Les données massives, comme
les données plus traditionnelles, sont soumises aux conséquences
induites par le processus de génération des données (\emph{data
generating process}) comme un échantillonnage.

Toutefois, depuis quelques années, le développement de nouvelles
méthodes de pondération des données offre des pistes de solutions. La
grande quantité de données massives permet notamment d'appliquer des
méthodes de pondération bien plus efficaces pour corriger les
échantillons non-représentatifs (Wang et al.~2015).

\hypertarget{donnuxe9es-expuxe9rimentales}{%
\subsection{Données expérimentales}\label{donnuxe9es-expuxe9rimentales}}

La question du processus de génération des données devient plus claire
quand on considère comment les \emph{données observationnelles} et les
\emph{données expérimentales} permettent d'effectuer des inférences de
manière distincte (voir Figure 2). Toutefois, pour bien comprendre ce
point, il faut comprendre les notions de données expérimentales et
d'inférence causale, qui sont centrales au domaine de la causalité en
recherche.

En quelques mots, l'essence de la démarche causale se résume comme suit
: le processus de génération de données expérimentales a pour objectif
d'assurer la validité d'une inférence causale estimée sur un échantillon
sur l'ensemble de la population visée.

Plus spécifiquement, le processus de génération des données permet aux
chercheur.e.s de s'assurer que la distribution du traitement entre les
deux groupes, traitement et contrôle, est entièrement aléatoire. De
manière technique, cette distribution aléatoire du traitement entre les
deux groupes permet de garantir une source exogène (à l'opposé de
endogène) de variation sur la variable indépendant (\emph{x). Cette
source exogène de variation permet, quant à elle, d'éliminer
l'endogénéité entre la variable indépendante (}x\emph{) et le résidu
(}e*).

Autrement dit, le fait de distribuer au hasard le traitement entre les
membres du groupe traitement et ceux du groupe contrôle assure que la
variation dans les résultats ne vient pas d'autres facteurs
non-contrôlés (le résidu, \emph{e}), mais plutôt du traitement lui-même
(la variable indépendante, \emph{x}). En distribuant le traitement de
manière aléatoire, on s'assure que les différences dans les résultats
sont vraiment dues au traitement et non à d'autres facteurs.

Il s'agit là d'assurer le respect de la condition d'indépendance,
essentielle à la validité de l'identification de l'effet causal étudié.
Autrement dit, en éliminant l'endogénéité entre la \emph{x} et \emph{e},
on s'assure que l'effet observé n'est pas dû à une variable confondante.

Pour revenir aux données massives, celles-ci ne peuvent pas résoudre les
enjeux liés aux inférences causales ou explicatives (Grimmer, 2015).
Elles sont en effet également soumises aux mêmes impératifs issus du
processus de génération des données.

\hypertarget{donnuxe9es-observationnelles}{%
\subsection{Données
observationnelles}\label{donnuxe9es-observationnelles}}

En ce qui a trait aux données observationnelles, il y a deux points
importants. Premièrement, des méthodes d'inférence basées sur des
approches par design (\emph{design-based methods}) comme une méthode de
régression sur discontinuité ou de variable instrumentale peuvent
également garantir des inférences explicatives et causales valides.
Elles nécessitent toutefois plusieurs postulats plus restrictifs dont
l'objectif est d'imiter ou de récréer, de la manière la plus fidèle
possible, une distribution aléatoire du traitement -- ce que la
littérature appelle un \emph{as-if random assignment} (comme si
l'attribution était aléatoire) (Dunning, 2008).

Dans un contexte observationnel, les données massives peuvent donc
permettre d'augmenter la précision des estimations causales.
Effectivement, comme dans un modèle de régression linéaire, plus
l'échantillon est grand, plus l'estimation du coefficient causal ou
probabiliste est précise. Par exemple, un échantillon large dans un
modèle de régression sur discontinuité permet de restreindre la largeur
de bande autour du seuil, garantissant ainsi une distribution presque
parfaitement aléatoire des données et une validité plus élevée à
l'estimation de l'effet causal.

Un autre exemple pourrait être l'utilisation du « matching », souvent
utilisé dans les études économétriques. Supposons que vous souhaitez
estimer l'effet d'un programme éducatif sur les résultats scolaires
d'étudiant.e.s. Le devis de recherche idéal serait d'assigner
aléatoirement les étudiant.e.s au programme (le groupe traitement) ou
non (le groupe contrôle). Toutefois, puisque ce devis idéal peut être
difficilement réalisable, un grand nombre de données pourrait permettre
de trouver pour chaque étudiant.e dans le groupe traitement un
étudiant.e « jumeau » dans le groupe contrôle. Ce « jumeau » serait
similaire en âge, sexe, antécédents socio-économiques, etc. Il serait
ensuite possible de comparer les résultats scolaires de ces jumeaux pour
estimer l'effet du programme. Plus l'échantillon est grand, plus
l'estimation sera précise et fiable, parce qu'il y aura plus de jumeaux
possibles à apparier, réduisant ainsi le biais dû aux variables non
observées.

Il s'agit d'un exemple où les données massives augmentent la validité
interne de l'étude, même si les données sont de nature observationnelle
et non expérimentale.

Deuxièmement, un échantillon de données massives observationnelles
issues d'une plateforme comme X --- anciennement Twitter --- ou Facebook
peut fournir une \emph{description} plus fine de certaines dynamiques
sociales observées sur les réseaux sociaux. Cependant, c'est la manière
dont sont collectées les données de cet échantillon de données massives
qui garantit la représentativité de l'échantillon --- avec pour objectif
l'absence d'un biais de sélection --- et non pas la quantité de données.
Généralement, le biais d'un échantillon est une conséquence de la
non-représentativité des répondants; dans notre exemple, les
utilisateurs des médias sociaux ne sont généralement pas représentatifs
de la population entière.

Dans un tel cas, des méthodes de pondération sur des données
observationnelles peuvent compenser pour la sur- ou la
sous-représentativité de sous-groupes dans un échantillon afin d'assurer
la validité de l'inférence entre échantillon et population. Les données
massives ont ici une importance puisqu'une pondération fiable nécessite
une quantité substantielle d'observations. Une pondération \emph{a
posteriori} sera donc plus fiable plus l'échantillon est grand. Les
données massives ont ainsi une valeur ajoutée afin d'établir des
inférences descriptives plus précises et sophistiquées.

\hypertarget{validituxe9-uxe9cologique-et-observation-par-sous-groupes}{%
\subsection{Validité écologique et observation par
sous-groupes}\label{validituxe9-uxe9cologique-et-observation-par-sous-groupes}}

Les données massives peuvent aussi jouer d'autres rôles importants
relatifs à la validité externe. Premièrement, les données massives
facilitent effectivement la validité externe de certaines études en
accroissant la validité écologique (\emph{ecological validity}) des
tests expérimentaux, c'est-à-dire le réalisme de la situation
expérimentale (Grimmer, 2015: 81). En effet, la variété des sources et
des formats de données permet aux chercheurs d'imiter plus fidèlement la
réalité sur le terrain vécue par les participants aux études.

Deuxièmement, la quantité importante de données rend possible
l'observation d'effets précis, spécifiques et inédits par sous-groupes
(Grimmer 2015: 81). Alors qu'auparavant, la taille réduite des
échantillons ne permettait pas d'effectuer des inférences valides pour
des sous-groupes de la population --- les écarts-types par sous-groupes
étaient trop grands, rendant difficile l'estimation précise d'un
paramètre comme la moyenne et impossible celle d'un coefficient ---, la
taille énorme des échantillons de données massives permet aux chercheurs
d'estimer des paramètres qui étaient demeurés extrêmement imprécis
jusqu'à aujourd'hui. Notre compréhension des phénomènes sociaux s'en
trouve par le fait même approfondie de façon considérable.

\begin{figure}

{\centering \includegraphics{images/chapitre1_tableau.png}

}

\caption{image2\_2}

\end{figure}

\hypertarget{conclusion-trois-questions-ouvertes-pour-le-futur}{%
\section{Conclusion : trois questions ouvertes pour le
futur}\label{conclusion-trois-questions-ouvertes-pour-le-futur}}

Comme nous venons de le voir, la quantité et la variété nouvelle des
données massives permettent à la fois un approfondissement de l'analyse
de certains phénomènes et l'ouverture de nouvelles avenues de recherche.
L'analyse des données massives peut permettre de mettre en lumière des
tendances subtiles échappant aux ensembles d'informations plus
restreints.

Il faut toutefois souligner que les données massives représentent une
complexification de l'analyse des phénomènes en sciences sociales d'une
perspective non pas seulement méthodologique/technique mais également
épistémologique.

Cela soulève au moins trois questions d'importance, dont les réponses ne
nous sont pas encore accessibles, pour l'avenir de la recherche en
sciences sociales : (1) les données massives entrent-elles
(partiellement du moins) en conflit avec l'impératif de parcimonie qui
caractérise la science moderne?; (2) ces données sont-elles dans la
continuité ou représentent-elles une coupure dans la tradition
béhavioraliste en sciences sociales (et en science politique tout
particulièrement)?; (3) et finalement, de manière reliée, les données
massives proposent-elles ou non une manière de dépasser l'individualisme
méthodologique qui caractérise les sciences sociales contemporaines?

\bookmarksetup{startatroot}

\hypertarget{le-monde-du-libre}{%
\chapter{Le monde du libre}\label{le-monde-du-libre}}

\begin{center}

"Vers une science numérique plus transparante: l'apport du logiciel libre et du code ouvert dans les sciences sociales"
author: "Catherine Ouellet et Jozef Rivest"

Catherine Ouellet et Jozef Rivest

\end{center}

Ce chapitre vise à initier les lecteurs et lectrices aux concepts
fondamentaux du logiciel libre. Pour ce faire, nous présenterons, dans
un premier temps, l'historique de ce mouvement afin de pouvoir le situer
temporellement. De cette façon, nous pourrons mieux comprendre les
motivations derrière ce mouvement, mais aussi ses influences actuelles.
Ensuite, nous distinguerons le logiciel libre du code ouvert. Bien que
les deux soient très près l'un de l'autre, il est important de les
distinguer puisqu'ils ne renvoient pas aux mêmes caractéristiques et aux
mêmes fondements. Après coup, nous utiliserons un exemple concret pour
illustrer le propos: \texttt{R}, et ses différentes librairies. La
dernière section du chapitre présentera les avantages et les
inconvénients, en plus de défis qui se posent. En guise de conclusion,
nous souhaitons mettre l'accent sur l'apport du logiciel libre et du
code ouvert afin d'assurer la transparence, la reproductibilité ainsi
que la qualité des recherches scientifiques.

\emph{« Vous n'avez pas à suivre une recette avec précision. Vous pouvez
laisser de côté certains ingrédients. Ajouter quelques champignons parce
que vous en raffolez. Mettre moins de sel car votre médecin vous le
conseille --- peu importe. De surcroît, logiciels et recettes sont
faciles à partager. En donnant une recette à un invité, un cuisinier n'y
perd que du temps et le coût du papier sur lequel il l'inscrit. Partager
un logiciel nécessite encore moins, habituellement quelques clics de
souris et un minimum d'électricité. Dans tous les cas, la personne qui
donne l'information y gagne deux choses : davantage d'amitié et la
possibilité de récupérer en retour d'autres recettes intéressantes. »} -
Richard Stallman (Williams, Stallman, and Masutti 2010)

Cette analogie illustre bien trois concepts au coeur de la philosophie
de Richard Stallman, souvent considéré comme le père fondateur du
logiciel libre : liberté, égalité, fraternité. Les utilisateurs de ces
logiciels sont libres, égaux, et doivent s'encourager mutuellement à
contribuer à la communauté. Ainsi, un logiciel libre est généralement le
fruit d'une collaboration entre développeurs qui peuvent provenir des
quatre coins du globe. Une réflexion éthique est au coeur du mouvement
du logiciel libre, dont les militants font campagne pour la liberté des
utilisateurs dès le début des années 1980. La Free Software Foundation
(FSF), fondée par Richard Stallman en 1985, définit rapidement le
logiciel «libre» {[}free{]} comme étant garant de quatre libertés
fondamentales de l'utilisateur: la liberté d'utiliser le logiciel sans
restrictions, la liberté de le copier, la liberté de l'étudier, puis la
liberté de le modifier pour l'adapter à ses besoins puis le
redistribuer\footnote{La redistribution doit évidemment respecter
  certaines conditions précises, dont l'enfreint peut mener à des
  condamnations
  {[}http://www.softwarefreedom.org/resources/2008/shareware.html{]}}.
Il s'agit ainsi d'un logiciel dont le code source\footnote{Pour rester
  dans les analogies culinaires, le code source est au logiciel est ce
  que la recette est à un plat: elle indique les actions à effectuer,
  une par une, pour arriver à un résultat précis. Encore une fois, cette
  dernière peut-être adaptée, modifiée, bonifiée.} est disponible, afin
de permettre aux internautes de l'utiliser tel quel ou de le modifier à
leur guise. Puisque le langage machine est difficilement lisible par
l'homme et rend la compréhension du logiciel extrêmement complexe,
l'accès au code source devient essentiel afin de permettre à
l'utilisateur de savoir ce que le programme fait réellement. Seulement
de cette façon, l'utilisateur peut \emph{contrôler} le logiciel, plutôt
que de se faire contrôler par ce dernier (Stallman 1986).

\hypertarget{uxe9mergence-et-suxe9mantique-du-libre}{%
\section{\texorpdfstring{Émergence et sémantique du
\emph{libre}}{Émergence et sémantique du libre}}\label{uxe9mergence-et-suxe9mantique-du-libre}}

Plusieurs situent les débuts du mouvement du logiciel libre avec la
création de la licence publique générale GNU, en 1983, à partir de
laquelle va se développer une multitude de programmes libres. Parmi les
plus populaires, on retrouve notamment le navigateur Firefox, la suite
bureautique OpenOffice et l'emblématique système d'exploitation Linux,
qui se développe d'ailleurs à partir de la licence GNU. Aujourd'hui, il
s'agit d'un véritable phénomène sociétal: des milliers d'entreprises,
d'organisations à but non lucratif, d'institutions ou encore de
particuliers adoptent ces logiciels, dont la culture globale et les
valeurs (entraide, collaboration, partage) s'arriment avec le virage
technologique de plusieurs entreprises. Les logiciels libres ont
différents usages, en passant par la conception Web, la gestion de
contenu, les systèmes d'exploitation, la bureautique, entre autres. Ils
permettent donc de répondre à plusieurs types de besoins numériques et
informatiques.

Attention, le logiciel libre est avant tout une philosophie, voire un
mouvement de société. C'est une façon de concevoir la communauté du
logiciel, où le respect de la liberté de l'utilisateur est un impératif
éthique (Williams, Stallman, and Masutti 2010). Par conséquent, le terme
libre, \emph{free} en anglais, porte à confusion. Celui-ci ne signifie
pas qu'un logiciel libre est nécessairement gratuit. Certes, plusieurs
sont effectivement téléchargeables gratuitement. Toutefois, il est aussi
possible de (re)distribuer des logiciels libres payant. Par ailleurs,
aucun logiciel libre n'est réellement « gratuit » dans la mesure où son
déploiement et son utilisation nécessitent généralement différents
coûts, dont les degrés sont variables en fonction des compétences et de
l'infrastructure dont disposent les utilisateurs (coût d'apprentissage,
coûts d'entretien, etc.). Enfin, il est important de garder en tête que
les logiciels libres possèdent eux aussi une licence - cette dernière
est d'ailleurs garante des libertés que confèrent les logiciels libres
aux utilisateurs.

\hypertarget{logiciel-libre-et-code-ouvert}{%
\section{Logiciel libre et code
ouvert}\label{logiciel-libre-et-code-ouvert}}

\textbf{À voir}

Liste de logiciel libre répertorié par le gouvernement du Canada:
https://code.open.canada.ca/fr/logiciels-libres.html\#

Parallèlement au logiciel libre, il y a aussi le code ouvert, ou
\emph{open source}. A priori, la dénomination du logiciel libre et celle
du \emph{code ouvert} semble suggérer qu'il s'agit de synonymes. Dans
les deux cas on dirait que l'on fait référence à des logiciels, par
exemple, qui sont exempts de restrictions d'utilisations et auxqels les
utilisateurs peuvent participer au développement. Cependant, il y a une
distinction importante entre les deux.

Bien que les deux renvoient sensiblement aux mêmes types de logiciels,
les tenants de ces approches ne partagent pas la même perspective. Comme
Stallman (2022) l'explique, le logiciel libre est d'abord et avant tout
un mouvement qui fait ``campagne pour la liberté des utilisateurs de
l'informatique''. Le code ouvert, quant à lui, met l'accent sur les
avantages pratiques, plutôt que de militer pour des principes.

Le terme \emph{code ouvert} sera introduit seulement en 1998 afin de
clarifier l'ambiguité dans la dénomination ``logiciel
libre''\footnote{Soit ceux qui ont été conçus suivant les principes
  philisophiques et ``moraux'' qui sous-tendent ce mouvement.},
\emph{free software} en anglais, afin de spécifier que le code source
était accessible, et non pas que le logiciel était ``gratuit''
(Ballhausen 2019). De plus, les logiciels code ouvert, doivent respecter
certains critères quant à la distribution de leurs logiciels (Open
Source Initiative 2006). Nous aborderons ces critères dans le prochain
paragraphe.

Afin de mieux distinguer les deux, il est utile de faire référence aux
critères qui composent ces deux éléments, et qui constituent la base de
leur définition. Tout d'abord, le logiciel libre se définit sur la base
de quatre libertés: 1) liberté d'utiliser le programme tel que désiré;
2) liberté d'étudier le fonctionnement du programme et de le modifier
pour ses propres besoins; 3) liberté de re-distribuer des copies; 4)
liberté de distribuer des copies de la version ``améliorer'' du
programme pour ses pairs (Ballhausen 2019). Concernant le \emph{code
ouvert}, tout logiciel qui souhaite être inclut sous cette appellation
doit respecter dix critères: 1) Redistribution gratuite; 2) doit inclure
le code source; 3) doit permettre les modifications et les travaux
dérivés; 4) intégrité du code source; 5) ne doit pas discriminer des
personnes et/ou groupes; 6) ne doit pas restreindre personne dans
l'utilisation du logiciel pour un domaine d'activité; 7) distribution
d'une license pour l'utilisation; 8) la license ne doit pas être
spécifique pour un produit; 9) la license ne doit pas placer de
restriction sur d'autres programmes; 10) la license doit être
technologiquement neutre\footnote{Pour plus d'informations sur ces
  caractéristiques, nous encourageons les lecteurs à se référer au lien
  web de Open Source Initiative (2006). Ils y trouveront un contenu
  détaillé pour chacunes des caractéristiques sus-mentionnées.} (Open
Source Initiative 2006).

Il est aussi utile de les distinguer des logiciels ``non-libres'', soit
les logiciels propriétaires: ``Son utilisation, sa redistribution ou sa
modification sont interdites, ou exigent une autorisation spécifique, ou
sont tellement restreintes qu'en pratique vous ne pouvez pas le faire
librement'' (Système d'exploitation GNU 2023). Par contraste, la licence
libre confère des droits de propriétaire. L'utilisateur a le droit
d'installer le logiciel sur autant d'ordinateurs que désiré, le modifier
selon ses besoins et le distribuer avec ou sans ses modifications. Il
peut même demander d'être payé pour distribuer des copies, avec ou sans
ses modifications. Par exemple, le logiciel Ubuntu, une version de
Linux, peut être téléchargé gratuitement du site Ubuntu.com. Il est
aussi vendu par Amazon.com pour 12\$ la copie, plus les frais
d'expédition!

Comme nous le constatons, le logiciel libre et le \emph{code ouvert} ont
certaines similitudes puisqu'ils adhèrent tous les deux à la même vision
du logiciel, ainsi que de son accessibilité. Toutefois, il est important
tout de même de les distinguer puiqu'ils ont des origines différentes,
et qu'ils mènent à certaines pratiques qui sont différentes. La
prochaine section utilise un cas concret afin d'expliquer l'effet du
libre, et l'utilité que cela peut avoir.

\hypertarget{les-sciences-sociales-uxe0-luxe8re-du-numuxe9rique-les-enseignements-de-la-philosophie-du-logiciel-libre}{%
\section{Les sciences sociales à l'ère du numérique: les enseignements
de la philosophie du logiciel
libre}\label{les-sciences-sociales-uxe0-luxe8re-du-numuxe9rique-les-enseignements-de-la-philosophie-du-logiciel-libre}}

En quoi est-ce que ces deux concepts, issus du monde de l'informatique,
sont-ils intéressants et/ou important pour les sciences sociales? Pour
répondre à cette question, il est important de retourner à la base, soit
de se questionner sur que sont les sciences sociales.

Ce chapitre à voulu mettre de l'avant le logiciel libre afin d'initier
les lecteurs et lectrices à ce monde. Le but n'était pas de présenter de
manière exhaustive tout ce champ. Plutôt, nous avons préféré nous
limiter aux bases de compréhension, ainsi qu'à quelques exemples. Par
conséquent, nous souhaitions qu'à la lecture du chapitre, les lecteurs
et lectrices soient mieux outillés pour comprendre et réfléchir par
rapport à ce monde, et ainsi insérer ces réflexions dans leurs démarches
scientifiques. Générer des idées et des débats nous paraît bien plus
promoteur pour l'avenir que d'apprendre par coeur.

En guise de conclusion, nous souhaitons résumer ce chapitre tout en
situant ces différents éléments dans les sciences sociales à l'ère du
numérique. Le livre de Marres (2017) est très intéressant à ce sujet.
Face au constat que la vie sociale se trouve affectée par les
changements numériques, il nous faut en tant que chercheur du monde
social réfléchir à notre façon de comprendre les changements qui sont
entrain de s'opérer. Bien que ces réflexions ratissent large
\footnote{Allant de nos postulats ontologiques, épistémologiques et
  méthodologiques.}, nous nous concentrons ici sur la dimension
méthodologique.

Comme nous l'avons présenté ci-haut, les bas coûts associés à
l'utilisation ainsi que la facilité du partage avec la communauté nous
semble être deux avantages importants pour l'avenir des sciences
sociales numériques. Notamment parce qu'ils ont le potentiel d'améliorer
la transparence des protocoles scientifiques. Dans \emph{Designing
Social Inquiery}, l'un des livres les plus influents en science
politique depuis les trente dernières années, les auteurs définissent
quatre caractéristiques que chaque recherche doit posséder afin d'être
considérée comme scientifique. L'une d'elles, est que la \emph{procédure
doit être publique}: ``La recherche scientifique utilise des méthodes
explicites, codifiées et publiques afin de générer et analyser des
données sur lesquelles la fiabilité peut ensuite être déterminer''
(King, Keohane, and Verba 2021, 6). Chaque individu qui souhaite
contribuer à la connaissance et à la compréhension globale que nous
avons de la réalité sociale doit garder en tête cette caractéristique
fondamentale. Comme nous l'avons exposé, le partage du code devient un
impératif pour assurer la transparence, la réplicabilité ainsi que la
qualité des recherches.

\hypertarget{avantages}{%
\subsection{Avantages}\label{avantages}}

\hypertarget{le-partage-et-co-construction-des-connaissances}{%
\subsubsection{Le partage et co-construction des
connaissances}\label{le-partage-et-co-construction-des-connaissances}}

La grande liberté que ce type de logiciel offre favorise la
collaboration entre les utilisateurs, et ce à une échelle pouvant être
internationale. Les interactions entre les chercheurs crées une
dynamique d'« innovation ascendante » et d'entraide (Couture 2014). Ce
résultat constitue un avantage important pour le développement de ces
logiciels. Selon certains, et comparativement aux logiciels privés, les
logiciels libres ont un niveau plus élevé d'innovation (Smith 2002).
Contrairement à ceux qui se développent de manière privé et fermée, les
logiciels libres permettent à tous les utilisateurs de participer au
développement. Ceux-ci partagent ensuite leurs améliorations, ce qui
stimulent à son tour de nouvelles initiatives. Ainsi, un certain savoir
est généré dans cette situation. De plus, il est raisonable de penser
que l'utilité des améliorations, ainsi que leur utilisation par les
utilisateurs en fonction de leur besoin, comme dans le cas de la
recherche sociale avec \texttt{R} permet de générer un savoir
collaboratif (Couture 2020). Amélioration constante, entraide, savoir
partagé et plusieurs milliers de contributeurs (Couture 2014), ces
éléments résument très bien la philosophie du logiciel libre.

Comme nous le verrons dans la section suivante, cet avantage est couplé
avec ceux économiques. Les bas coûts démocratise l'accès à plusieurs
logiciels qui sont utiles pour mener des analyses scientifiques. Et ce,
pour tous les utilisateurs dans le monde.

\hypertarget{avantages-uxe9conomiques-une-plus-grande-accessibilituxe9-pour-tous}{%
\subsubsection{Avantages économiques : Une plus grande accessibilité
pour
tous}\label{avantages-uxe9conomiques-une-plus-grande-accessibilituxe9-pour-tous}}

Le principal avantage économique des logiciels libres est son faible
d'acquisition et de renouvellement pour les particuliers. Cet avantage
individuel génère plusieurs externalités positives.

Tout d'abord, certains logiciels statistiques et programmes
informatiques, tel que Stata et SPSS, coûtent plusieurs centaines, voir
des miliers de dollar. De plus, la license doit être renouvelée
annuellement. Ce qui augmente les coûts associés à l'utilisation du
logiciel et par conséquent limite son accessibilité. Comparativement,
pour les logiciels libres, la license d'acquisition coûte bien souvent
moins cher, et aucun renouvellement de licence n'est demandé dans la
plus part des cas. Étant donné que les chercheurs doivent souvent faire
face à des contraintes budgétaires, les logiciels libres deviennent des
outils intéressant afin de minimiser les coûts de la recherche (Yu and
Muñoz-Justicia 2022). Avantage encore plus important pour les chercheurs
dans les pays du Sud global (Santillán-Anguiano and González-Machado
2023). L'accessibilité de ces ressources permet donc de réduire l'écart
dans la production scientifique entre les pays du Sud et ceux du Nord.
De plus, elle permet à tous de bénéficier d'outils pédagogiques
accessibles, ce qui favorise l'acquisition ainsi que le développement de
compétences méthodologiques.

Dans le cadre d'une formation universitaire, il peut être pertinent
d'enseigner aux étudiants à se servir de logiciel statistique ou
d'analyse de texte. L'acquisition de ces compétences peut être précieux
tant pour ceux et celles qui souhaitent se diriger vers le milieu
académique, que pour ceux et celles qui visent le marché professionnel.
D'ailleurs sur le site web de la banque d'emplois du gouvernement du
Canada\footnote{Ces informations proviennent du site web suivant:
  https://www.jobbank.gc.ca/marketreport/outlook-occupation/17882/ca},
les conditions d'emplois sont en ce moment\footnote{En date d'écire ces
  lignes, septembre 2023.} très bonnes, et une pénurie de main d'oeuvre
est anticipé, entre 2022-2031, dans les emplois en analyse de données.
Ces compétences sont d'autant plus précieuses aujourd'hui, dans le monde
de données dans lequel nous vivons.

Ensuite, le logiciel libre est adaptable et modifiable. Ces coût
techniques de développement restent néanmoins nettement inférieurs aux
coûts de renouvellement et de mise à jour des logiciels propriétaires
dans bien des cas. L'argent sauvée des licences peut alors être investie
dans le développement du logiciel libre (Béraud 2007). Cependant, une
transition vers les logiciels libres ne doit pas se faire seulement sur
des bases économiques, mais dans une perspective globale de changement
de cultures.Changer pour des raisons purements économiques viendrait à
violer l'essence même de la philosophie du logiciel libre, qui se veut
davantage être un esprit de collaboration et de transparence. Par
conséquent, il est important d'incorporer aussi les valeurs et la
philosophie dans notre utilisation

Pour résumer, les logiciels libres permettent donc une plus grande
égalité dans l'accès aux nouvelles technologies, puisqu'ils ont dans la
majorité des cas, des coûts d'acquisition nettement moindre. (Oui et
non, l'acquisition financière est une chose, mais il y a d'autres
barrières à l'utilisation tel que l'apprentissage à faire pour apprendre
un language de programmation, l'achat de matériel informatique, etc. )
Cependant, considérant cela, donner l'exemple de l'étude qui montre que
c'est beaucoup plus économique, même si l'on doit compter les coûts de
formation, le soutien technique, l'entretien et la maintenance. (Couture
2014; Karjalainen 2010).

\hypertarget{inconvuxe9nients-et-duxe9fis}{%
\subsection{Inconvénients et
défis:}\label{inconvuxe9nients-et-duxe9fis}}

\hypertarget{couxfbteux-en-temps}{%
\subsubsection{Coûteux en temps}\label{couxfbteux-en-temps}}

Dans leur texte, Paura and Arhipova (2012) soulèvent une critique faite
envers certains logiciels libres, notamment envers \texttt{R}. Le
problème principal d'enseigner les statistiques avec des logiciels
libres est qu'ils sont compliqués à apprendre ainsi qu'à utiliser; par
conséquent, les étudiants passeraient plus de temps à tenter de résoudre
les erreurs de programmation plutôt que d'apprendre les statistiques. Il
est vrai que ces logiciels demandent un investissement en temps, afin
d'être en mesure de mener ses propres analyses statistiques. Par
exemple, \texttt{R} demande l'apprentissage d'un language de
programmation afin de pouvoir utiliser le logiciel à son plein
potentiel.

La synthaxe de certaines libraries demandent aussi un certain temps
d'adaptation. Par exemple, je souhaite recoder la variable femme, de
l'ensemble de données \texttt{titanic}, afin de remplacer les valeurs
numériques actuelles (0, 1) par des valeurs nominales (homme, femme). La
section de code ci-dessous réalise cette tâche avec les commandes de
base de \texttt{R} et celle du \texttt{tidyverse}.

Toutefois, l'orsque l'on compare le coût d'apprentissage avec les
bénéfices tirés, il est plus difficile de soutenir qu'il s'agit d'un
désavantage. L'habileté que nous développons devient très utile par la
suite, puisqu'elle nous permet de manipuler ainsi que d'analyser des
données. Surtout, ces compétences s'inscrivent dans la longue durée,
alors que l'apprentissage est plutôt de courte à moyenne durée. Surtout,
la logique derrière la synthaxe de base de \texttt{R} et celle d'une
nouvelle librairie reste sensiblement inchangée. Par conséquent, lorsque
nous avons une bonne compréhension du fonctionnement de base de
\texttt{R}, l'apprentissage d'une nouvelle librairie se fait
relativement rapidement. Certaines, comme \texttt{dplyr} du
\texttt{tidyverse} facilite grandement la manipulation des données
comparativement aux commandes de base.

Pour résumer, bien que l'apprentissage d'un language de programmation
demande un investissement en temps, les bénéfices générées par ces
nouvelles compétences dépassent le coût initial.

\hypertarget{probluxe8me-de-transparence}{%
\subsubsection{Problème de
transparence}\label{probluxe8me-de-transparence}}

L'arrivé des sciences informatiques à fait émerger des problèmes de
reproductibilité des protocoles scientifiques (Janssen 2017). Le
problème principal est relatif à l'accès au code utilisé par les
chercheurs. Par exemple, il est possible de réaliser des analyses
statistiques avec \texttt{R} sans partager le code utilisé, ce qui
limite la transparence du processus scientifique. Dans cette situation,
il est difficile de savoir si des erreurs de codage ont été commises,
volontairement ou involontairement, affectant ainsi les résultats
partagés.

Afin de remédier à ce problème, certains logiciels tel que
GitHub\footnote{une plateforme publique \emph{code ouvert} sur laquelle
  nous pouvons héberger et partager notre code.} participent à la
transparence des résultats scientifiques (Fortunato and Galassi 2021).
Ce logiciel permet aux chercheurs de partager leur code afin qu'il
puisse être accessible pour tous. Il est important de mentionner ici que
l'installation et la configuration de GitHub peut s'avérer difficile
pour ceux et celles qui ne sont pas ignitié à l'informatique. Cela
constitue une certaine barrière dans l'utilisation de ce logiciel.
Toutefois, nous souhaitons tout de même présenter l'utilité de ce
logiciel puisqu'il permet de rendre les processus ainsi que les
résultats de recherche plus transparent.

Par exemple, si l'on réalise une analyse statistique de la relation
entre l'économie et le vote, nous pourrions partager l'ensemble du code
que nous avons utilisé sur GitHub. D'une part cela permettrait aux
utilisateurs de vérifier si les résultats sont honnêtes, et d'autre part
de réutiliser le code pour mener leurs propres analyses.

Cependant, le partage du code utilisé reste encore majoritairement
volontaire. Janssen, Pritchard, and Lee (2020) soutiennent que plus
d'effort et d'actions concertés doivent être mise en place afin
d'améliorer l'accessibilité aux codes. Toujours selon ces auteurs, les
journaux scientifiques pourraient exiger que les auteurs rendent leur
code publique lors du processus de publication. D'ailleurs, les
résultats d'une expérience sur les facteurs qui influencent les
chercheurs à partager leur code démontre que les initiatives
individuelles ne seront pas suffisantes pour une agmentation du partage
du code (Krähmer, Schächtele, and Schneck 2023). Par conséquent, rendre
le code accessible devrait devenir un standard institutionnalisé.

\hypertarget{appropration-capitaliste}{%
\subsubsection{Appropration
capitaliste}\label{appropration-capitaliste}}

Dans ce cas-ci, il s'agit plutôt d'un défis auquel le logiciel libre est
confronté plutôt qu'une critique quant aux limites de son utilisation.
En fait, l'accès au code source ainsi que la liberté et la possibilité
de contribuer au développement du logiciel constitue un avantage
intéressant pour les compagnies privées. Par conséquent, nous avons
assisté à une intégration partielle du logiciel libre dans la logique
capitaliste (Broca 2013; Bessen 2002). Certaines d'entre elles utilisent
les utilisateurs comme une main d'oeuvre gratuite afin de bonifier leur
logiciel, ce qui permet, dans certains cas, de générer des revenus
commerciaux dont l'entreprise est la seule bénéficiaire (Couture 2020).
Attention, il ne faut pas penser que toutes les compagnies agissent de
manière prédatrice. Le but ici est de souligner que certaines pratiques
commerciales trouble l'essence du mouvement du logiciel libre, qui se
veut davantage être un outil de collaboration accessible, plutôt qu'un
moyen pour générer des profits. Il est important de garder en tête les
valeurs et la philosophie qui a donné lieu à ce mouvement.

\hypertarget{crituxe8res-de-suxe9lection}{%
\section{Critères de sélection}\label{crituxe8res-de-suxe9lection}}

\newpage{}

\bookmarksetup{startatroot}

\hypertarget{sec-chap4}{%
\chapter{R ou ne pas R?}\label{sec-chap4}}

Plusieurs notions liées à l'ère numérique, notamment à ce qui a trait
aux opportunités et difficultés que cette dernière peut amener, ont été
présentées par l'entremise du chapitre précédent. C'est un monde de
possibilité qui s'offre à ceux qui maîtrisent les nouveaux outils des
temps modernes. Mais comment en arriver là ? Le présent chapitre a pour
but de présenter certains outils flexibles et péreins permettant la
réalisation de nombreuses tâches. Une des premières étapes permettant de
notamment réaliser la collecte, l'analyse et la visualisation graphique
de données ainsi que la rédaction de documents est l'apprentissage d'un
langage de programmation. Bien que plusieurs langages de programmation
existent, le présent ouvrage priorise le langage \textbf{R}. Les
sections suivantes présentent ce langage de programmation, ces forces et
ces faiblesses ainsi que les raisons de son utilisation. Enfin, la
dernière section présente un environnement de programmation qui se prête
bien à son utilisation.

\hypertarget{pourquoi-r}{%
\section{Pourquoi R?}\label{pourquoi-r}}

Comme mentionné précédemment, il existe plusieurs langages de
programmation. \textbf{R} a deux types de compétiteurs : les logiciels à
licences comme SAS, STATA et SPSS, et les langages \emph{OpenSource}
tels que Python et Julia. \textbf{R} est un langage de programmation
\emph{OpenSource} développé par des statisticiens, pour des
statisticiens, dans les années 1990 (Tippmann 2015). \textbf{R} prend
ses racines dans le langage de programmation S, créé notamment par Ross
Ihaka et Robert Gentleman. Ces derniers ont fait des choix non
orthodoxes lors de l'élaboration du langage, qui font aujourd'hui la
popularité de ce logiciel auprès d'un large pan de la communauté
académique. En effet, Morandat et al. (2012) rapporte que le langage a
été élaboré afin qu'il soit intuitif et qu'il permette aux nouveaux
utilisateurs de rapidement réaliser des analyses.

Le langage de programmation \textbf{R} a plusieurs avantages qui font de
lui un outil puissant et utile pour tout chercheur. L'un de ses grands
avantages est qu'il est \emph{OpenSource}. Ayant déjà abordé le sujet
dans le chapitre précédent, il sera question ici de simplement rappeler
les grandes lignes de l'argument, à savoir que : 1) l'\emph{OpenSource}
est gratuit d'utilisation; 2) l'\emph{OpenSource} est développé de façon
bottom-up, ce qui lui procure une grande flexibilité; et 3) il permet
aux utilisateurs de créer leurs propres fonctions. À l'inverse, les
logiciels à licences sont coûteux, rigides et l'ajout de fonctionnalités
se fait par les développeurs internes à la compagnie. Ces formalités
rendent le processus plus lent et réduisent l'éventail des possibilités
pour la personne chercheuse. Ceci étant dit, certains avanceront que
c'est justement ce processus interne lent qui assure la validité et la
fiabilité des analyses effectuées par SAS, STATA ou SPSS. Or, dans son
livre dédié aux utilisateurs de SPSS et de SAS, Muenchen (2011) soulève
le point que bien souvent, ce sont des individus atomisés qui
développent les nouvelles fonctionnalités de ces langages et que le
processus de révisions se fait ensuite par des comités internes de
testeurs. Il en va de même pour le développement des \emph{packages} R
dans la mesure où ce dernier se voit testé et amendé par plusieurs
programmeurs indépendants dans un processus itératif des plateformes
telles que GitHub. De plus, bien des nouvelles techniques statistiques
sont développées pour R par des chercheurs qui publient leur travail
dans des journaux académiques revus par des pairs, assurant la qualité
du procédé. Le fait que SAS et SPSS permettent à leur utilisateur
d'intégrer des routines R à leur programme est un indicateur fort ne
serait-ce que de l'utilité de R (Muenchen 2011). Le langage de
programmation \textbf{R} permet également de réaliser une grande
quantité de tâches de recherche. En effet, les personnes programmant en
\textbf{R} peuvent notamment manipuler et visualiser des données, faire
différents types d'analyses, créer des fonctions et faire des boucles en
plus de pouvoir combiner \textbf{R} avec certains langages de balisages.

D'un autre côté, l'utilisation du langage de programmation \textbf{R}
peut être perçue comme ayant certains inconvénients. Plusieurs disent
que la courbe d'apprentissage peut être plus grande que celle de
programmes à licences. La véridicité de cet argument est discutable. Les
programmes demandant des licences ont également un coût d'entrée. De
plus, les nouvelles itérations de ces logiciels amènent des changements
demandant une période d'adaptation pour la personne chercheuse. D'autres
disent que le développement \emph{OpenSource}, spécifiquement celui du
langage de programmation \textbf{R}, se fait de façon anarchique. Cela
est davantage une question d'opinion et de conception du monde qu'une
vérité. Le développement de \emph{package} se fait effectivement de
manière décentralisée et toute personne sachant programmer en \textbf{R}
peut collaborer à cette communauté. Bien qu'il n'y ait pas d'autorité
centrale, les \emph{packages} sont regroupés sur le \emph{Comprehensive
R Archive Network} (CRAN) (voir le https://cran.r-project.org/ pour plus
d'information). Le site a une politique de dépôt stricte, ainsi les
\emph{packages} doivent être suffisamment documentés. Il est également
possible d'y télécharger le langage de programmation \textbf{R}. Ce
langage, ainsi que ces différents \emph{packages}, sont disponible sur
Windows, macOS et Linux.

\hypertarget{ouxf9-coder-en-r}{%
\section{Où coder en R ?}\label{ouxf9-coder-en-r}}

Un environnement de développement intégré (IDE) permet aux programmeurs
de consolider les différents aspects de l'écriture d'un programme
informatique. Ils permettent de réaliser toutes les activités courantes
d'un programmeur -- l'édition du code, la construction des exécutables
et le débogage -- au même endroit. Les environnements de développement
intégrés sont conçus pour maximiser la productivité du programmeur. Ils
fournissent de nombreuses fonctionnalités -- notamment la coloration
syntaxique ainsi que le contrôle de version -- pour créer, modifier et
compiler du code. Certains environnements de développement intégré sont
dédiés à un langage de programmation spécifique. Par conséquent, ils
contiennent des fonctionnalités qui sont plus compatibles avec les
paradigmes de programmation du langage auquel ils sont associés. Enfin,
il existe de nombreux environnements de développement intégré
multilingues.

Comme mentionné précédemment, R est un des langages de statistiques et
d'exploration de données les plus populaires en sciences sociales. R est
pris en charge par de nombreux environnements de programmation.
Plusieurs ont été spécialement conçus pour la programmation en R -- le
plus notable étant RStudio -- tandis que d'autres sont des
environnements de programmation universels -- tels que Visual Studio
Code -- et prennent en charge R via des plugins. Il est également
possible de coder en R à partir d'une interface en ligne de commande.
Une telle méthode permet la communication entre l'utilisateur et son
ordinateur. Cette communication s'effectue en mode texte : l'utilisateur
tape une « ligne de commande » -- c'est-à-dire du texte dans le
\emph{terminal} -- pour demander à son ordinateur d'effectuer une
opération précise, telle que rouler un fichier de code R.

La suite du chapitre présente RStudio, notamment à travers ses avantages
et inconvénients, mais également des exemples de ses fonctionnalités.

\hypertarget{quest-ce-que-rstudio}{%
\section{Qu'est-ce que RStudio ?}\label{quest-ce-que-rstudio}}

RStudio est un projet open source destiné à combiner les différentes
composantes du langage de programmation R en un seul outil (Allaire,
2011). RStudio fonctionne sur tous les systèmes d'exploitation, y
compris Windows, Mac OS et Linux. En plus de l'application de bureau,
RStudio peut être déployé en tant que serveur pour permettre l'accès Web
aux sessions R s'exécutant sur des systèmes distants (Allaire, 2011).
RStudio facilite l'utilisation du langage de programmation R en offrant
de nombreux outils permettant à son utilisateur d'aisément réaliser ses
tâches. Parmi les plus utiles, on retrouve notamment une fenêtre d'aide,
de la documentation sur les différents packages R, un navigateur
d'espace de travail, une visionneuse de données et une prise en charge
de la coloration syntaxique (Horton, Kleinman, 2015). De plus, RStudio
permet de coder dans plusieurs langages et de supporter une grande
quantité de formats. Il fournit également un support pour plusieurs
projets ainsi qu'une interface pour utiliser des systèmes de contrôle,
tels que GitHub (Horton, Kleinman, 2015).

RStudio a plusieurs avantages. Son utilisation est facile à apprendre
pour les débutants. Les principaux éléments d'un IDE sont intégrés dans
une disposition à quatre volets (Verzani, 2011). Cette disposition
comprend une console, un éditeur de code source à onglets pour organiser
les fichiers d'un projet, un espace pour l'environnement de travail et
un quatrième volet où il est notamment possible d'afficher des
graphiques ou de la documentation sur différents packages. Ce volet
permet d'ailleurs d'accéder au répertoire des \emph{packages}
disponibles pour \emph{R} en plus de permettre à l'utilisateur de
consulter l'arborescence de ses fichiers. De plus, on y retrouve la
possibilité de créer plusieurs espaces de travail -- appelés projets --
qui facilitent l'organisation de différents \emph{workflows}.

Il y a plusieurs autres aspects de RStudio que les programmeurs
apprécient. Parmi ceux-ci se trouve le fait qu'il peut être utilisé via
un navigateur Web pour un accès à distance (Verzani, 2011). De plus,
RStudio supporte plusieurs langages de programmation ainsi que
différents langages de balisage. Qui plus est, de nouvelles
fonctionnalités sont régulièrement ajoutées pour satisfaire les besoins
de la communauté scientifique. Enfin, R logiciel est également souvent
mis à jour.

Parmi ce que certains considèrent comme étant les points faibles de
RStudio, on retrouve des éléments liés à la configuration. Certains
utilisateurs trouvent que le nombre de raccourcis est limité. D'autres
trouvent que le \emph{set up} des différents panneaux n'est pas
ergonomique, ou même qu'il n'est pas possible de pouvoir suffisamment
personnaliser l'environnement de programmation. De plus, certains
utilisateurs ont rapporté que RStudio était plus lent que d'autres
alternatives pour quelques opérations, surtout celles comprenant de
longs codes.

\hypertarget{comment-utiliser-rstudio}{%
\section{Comment utiliser RStudio ?}\label{comment-utiliser-rstudio}}

Bien que de nombreux éléments puissent être personnalisés, la
disposition par défaut de RStudio est composée de quatre volets
principaux (Verzani, 2011). Dans le coin supérieur gauche se trouve le
cadran principal. C'est dans celui-ci que l'utilisateur passera la plus
grande partie de son temps. On y modifie des fichiers de différents
formats et il est possible d'y afficher des bases de données. Dans le
coin inférieur gauche se trouve la console ainsi que le terminal. Dans
cette première, on peut interagir avec R de la même manière que dans le
cadran principal, mais le code ne sera pas enregistré. Le terminal, pour
sa part, est le point d'accès de communication entre un usager et son
ordinateur. Bien que les différents systèmes d'exploitation viennent
avec un terminal déjà intégré, il est aussi possible d'y accéder à
partir de RStudio.

On retrouve, dans le coin supérieur droit, l'espace de travail. Ce
cadran contient trois éléments : l'\emph{environnement global,
l'historique et les connections}. L'\emph{environnement global} est
l'endroit où l'utilisateur peut voir les bases de données, les fonctions
et les différents autres objets R qui sont actifs. Il peut cliquer sur
les divers éléments actifs pour les consulter. L'onglet
\emph{historique} permet à l'utilisateur de consulter les derniers
morceaux de code R qu'il a roulé ainsi que les dernières commandes
écrites dans la console. L'onglet \emph{connections}, pour sa part,
permet de connecter son IDE à une variété de sources de données et
d'explorer les objets et les données qui la composent. Il est conçu pour
fonctionner avec une variété d'autres outils pour travailler avec des
bases de données en R dans RStudio.

Le cadran dans le coin inférieur droit, pour sa part, contient plusieurs
outils très utiles pour les usagers de RStudio. L'onglet \emph{Files}
permet à l'utilisateur de naviguer dans les fichiers que contient son
ordinateur sans avoir à sortir de RStudio. L'onglet \emph{Plots} permet
de visualiser les graphiques générer à partir de R, que ce soit en
utilisant \emph{ggplot2, lattice ou base R}. L'onglet \emph{Packages}
permet de consulter les packages installés précédemment par
l'utilisateur en plus de pouvoir en consulter la documentation. C'est
aussi un des différents endroits à partir d'où il est possible
d'installer des packages avec RStudio. L'onglet \emph{Help} permet à
l'utilisateur de chercher et de consulter de la documentation sur de
nombreux sujets, notamment sur les différentes fonctions en R ainsi que
sur les packages. Pour sa part, l'onglet \emph{Viewer} permet la
visualisation de contenu web local.

Enfin, l'utilisateur peut modifier les dimensions par défaut pour chacun
des quatre cadrans principaux. En cliquant sur la division des sections,
il est possible d'ajuster l'allocation horizontale de l'espace. De plus,
chaque côté dispose d'un autre séparateur pour ajuster l'espace
vertical. Qui plus est, la barre de titre de chaque cadran comporte des
icônes pour ombrer un composant, maximiser un cadran verticalement ou
modifier la taille des l'espace de travail (Verzani, 2011; Nierhoff et
Hillebrand, 2015).

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Le langage de programmation R est un outil très utile pour toutes sortes
de tâches notamment reliées aux statistiques et à la visualisation
graphiques. Sa maîtrise est requise pour accéder à plusieurs emplois,
autant dans le monde académique que dans les secteurs publics et privés.
Avec un peu de chance, le présent chapitre vous a éclairé sur son
utilité et sa pertinence dans le monde du travail contemporain. Bien que
le langage de programmation R ne doivent pas obligatoirement être
utilisé avec RStudio, nous pensons que pour la plupart des usagers, leur
utilisation conjointe est bénéfique et souhaitée. RStudio permet
également d'utiliser différents langages de balisage compatibles avec R,
facilitant l'utilisation de plusieurs outils complémentaires.
L'apprentissage du langage de programmation R apparaît également être
une valeur sure. Sa longitivité dans plusieurs sphères ainsi que la
forte croissance de sa base d'usagers laisse présager que d'en connaître
au moins les bases est un énorme avantage pour tout le monde. Pour ceux
qui sont particulièrement intéressés par le langage de programmation R
et qui désirent s'impliquer dans sa communauté, il existe plusieurs
conférences internationales et nationales sur R -- notamment
\emph{RConference} and \emph{useR!} -- et un journal académique,
\emph{The R Journal}. On retrouve également différentes communautés
telle que \emph{R-Ladies} qui met de l'avant la diversité des genres
dans la communauté du langage de programmation R. Le langage de
programmation R est plus qu'un simple outil statistique, il est le
centre d'une grande communauté de gens qui ont à coeur des principes
liés à l'inclusion et à l'avancement humain.

\bookmarksetup{startatroot}

\hypertarget{sec-chap8}{%
\chapter{À la quête de l'optimisation}\label{sec-chap8}}

Le monde de la recherche en sciences sociales numériques est en
constante évolution, offrant de nouvelles opportunités mais aussi des
défis uniques. Dans cette quête incessante pour optimiser notre
efficacité et notre collaboration, l'utilisation des bons outils devient
la clé de la réussite. Que vous soyez un chercheur en herbe ou un
professionnel chevronné, la manière dont vous organisez vos méthodes de
travail et gérez vos ressources peut déterminer la qualité et l'impact
de vos résultats.

\hypertarget{limportance-dune-muxe9thode-de-travail-efficace}{%
\section{L'importance d'une méthode de travail
efficace}\label{limportance-dune-muxe9thode-de-travail-efficace}}

Avant même de plonger dans les détails des méthodes de recherche et des
analyses, il est crucial de poser les bases d'une méthode de travail
efficace. Qu'il s'agisse de travailler en solitaire ou en équipe,
l'ordre et la structure sont des éléments essentiels. Des dossiers bien
organisés, une arborescence claire et un entreposage sécurisé deviennent
les piliers sur lesquels repose votre productivité. Après tout, un
environnement de travail organisé engendre des résultats ordonnés.

Ce chapitre vous emmènera à découvrir une gamme d'outils conçus pour
répondre aux besoins spécifiques des chercheurs en sciences sociales
numériques. Dans une quête pour maximiser votre temps, améliorer vos
flux de travail et renforcer vos collaborations, nous explorerons trois
types d'outils qui vous guideront dans cette quête d'optimisation~:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Logiciels de communication}~: La communication transparente
  est le cœur d'une collaboration réussie. Nous explorerons des outils
  tels que Slack qui facilitent les échanges en temps réel, connectant
  les chercheurs, même à distance, pour un partage rapide d'idées et
  d'informations.
\item
  \textbf{Logiciels de gestion de versions décentralisé}~: Nous
  plongerons dans le monde de Git et GitHub, des outils indispensables
  pour le suivi des versions et la collaboration efficace sur le code
  source.
\item
  \textbf{Outils d'entreposage de données}~: Que vous traitiez des
  données sensibles ou non, la conservation sécurisée de vos
  informations est primordiale. Des plateformes telles que Dropbox et
  Amazon Web Services (AWS) offrent des espaces sécurisés pour
  entreposer et partager vos données avec votre équipe.
\end{enumerate}

Chacun de ces outils est une pièce du puzzle, conçue pour vous aider à
gagner du temps, à collaborer de manière plus fluide et à renforcer la
qualité de votre recherche en sciences sociales numériques. Plongeons
dans ces outils avec un désir commun d'optimisation et d'excellence dans
notre travail.

\hypertarget{logiciel-de-gestion-de-communication-slack}{%
\section{Logiciel de gestion de communication
(Slack)}\label{logiciel-de-gestion-de-communication-slack}}

Dans tout bon projet de recherche, la communication est primordiale. Que
ce soit pour décrire les avancements, discuter des étapes à venir,
entretenir un partenariat avec des partenaires ou simplement structurer
ses pensées, la plateforme par laquelle vous communiquez vous accompagne
à chacune des étapes du travail. Il est donc important de choisir un
outil qui convient bien à vos projets et de prendre le temps de
l'apprivoiser et d'optimiser son utilisation.

Il y a tellement de plateformes différentes pour communiquer qu'il faut
être prudent par rapport au nombre utilisé. Si vous ne faites pas un
choix, vous pouvez, sans vous en rendre compte, mêler Teams, courriels,
Zoom et autres. Rapidement, vous perdez le contrôle de ce qui est dit.
Nous vous proposons d'opter pour un logiciel de gestion de
communication. Il existe plusieurs logiciels du genre, tels que
Microsoft Teams, Slack, Google Workspace et Workplace. Toutes ces
options peuvent vous permettre de collaborer efficacement en équipe.
Dans le cadre de nos travaux, nous utilisons Slack. C'est donc
principalement de cet outil que nous parlerons dans cette section, mais
n'hésitez pas à vérifier quelle plateforme correspond le mieux à vos
besoins.

\hypertarget{pourquoi-utiliser-une-de-ces-plateformes}{%
\subsection{Pourquoi utiliser une de ces
plateformes}\label{pourquoi-utiliser-une-de-ces-plateformes}}

Peu importe votre niveau d'implication, la collaboration et la
communication sont inévitables en recherche. La science n'est pas une
discipline qui se développe en solitaire, elle nécessite des échanges et
des débats. Les équipes de recherche sont souvent dispersées
géographiquement. Même si vous travaillez actuellement seulement avec
votre directeur, il est certain que plusieurs équipes de recherche dans
votre département utilisent un tel logiciel. Un courriel peut faire
l'affaire pour une discussion ponctuelle qui se règle rapidement.
Cependant, dans une équipe de travail dynamique, où plusieurs membres
participent à divers projets, les courriels deviennent rapidement
chaotiques, il est difficile de retracer ce qui a été dit et de
conserver les pièces jointes. Les discussions deviennent rapidement trop
complexes pour le médium utilisé.

Les logiciels de gestion de communication ont été conçus spécifiquement
pour répondre aux besoins des équipes collaboratives. Vous y trouverez
leur facette la plus attrayante~: une structure simple et adaptée. Les
chaînes et fils de discussions permettent de garder des traces et de se
retrouver facilement dans ce qui a été dit. Une autre force de ces
logiciels est la centralisation des outils de travail. Sur Slack, comme
sur Teams, vous pouvez faire des appels en visioconférence à l'endroit
où vos conversations écrites se trouvent. Il est aussi possible d'y
télécharger l'application mobile, ce qui facilite l'accessibilité et la
connexion des membres de l'équipe. Tout avoir structuré à son goût au
même endroit et à portée de main, cela permet de structurer sa pensée
plus efficacement, d'éviter les oublis et de réduire le stress.

\hypertarget{comment-utiliser-votre-logiciel-efficacement}{%
\subsection{Comment utiliser votre logiciel
efficacement}\label{comment-utiliser-votre-logiciel-efficacement}}

Une fois que vous êtes convaincu d'aller de l'avant avec un de ces
outils, vous devrez apprendre à bien vous en servir. Voici quelques
trucs qui pourront vous aider à optimiser son utilisation. Les points
ci-dessous font référence à Slack, mais peuvent très bien être adaptés à
d'autres plateformes.

\hypertarget{structuration}{%
\subsubsection{Structuration}\label{structuration}}

Il est important de bien réfléchir à la structuration de vos chaînes. Si
vous ne faites pas ce travail, les chaînes peuvent se multiplier
rapidement et les conversations se mettent alors à s'entrecroiser, vous
faisant ainsi perdre le fil. L'objectif de ces outils étant d'évitez ces
problèmes, vous ne voulez pas perdre l'avantage comparatif que vous
venez tout juste de gagner face aux courriels! La structuration des
chaînes devrait être similaire à celle de votre équipe de recherche. Si
vous utilisez Notion ou un autre logiciel du genre, la structure des
deux outils devrait être la même. Nous vous proposons d'avoir une chaîne
pour chacun des projets. Si le projet est trop gros et que la
conversation devient chaotique, pensez à créer une sous-chaîne (un
sous-projet) qui vous permettra d'aborder un sujet précis, sans mêler
les discussions. Pour faciliter la structuration des chaînes, vous
pouvez utiliser des préfixes, pour classer les chaînes par thème, ou
autre typologie qui vous convient. Également, utilisez les Espaces
d'équipe. Chaque équipe devrait avoir son propre espace, avec ses
propres chaînes. Vous pouvez faire partie de plusieurs équipes et
naviguer à travers les espaces. Si plusieurs équipes partagent un même
espace de travail, vous pourriez perdre le contrôle de sa structure.

\hypertarget{maintenance}{%
\subsubsection{Maintenance}\label{maintenance}}

Slack est un espace dynamique, tout comme votre équipe! La structure que
vous avez choisie n'est pas permanente. Vous devriez rapidement vous
questionner à savoir si elle convient toujours à vos activités. Votre
espace d'équipe est comme votre réel lieu de travail, faites-y
régulièrement le ménage pour vous assurer que tout est propre et en
ordre. Archivez les chaînes qui ne sont plus pertinentes ou actives,
puisque vous pourrez toujours les désarchiver quand cela sera
nécessaire. Épinglez des messages importants et des documents utiles aux
projets dans les chaînes appropriées. Faites le tour de ce qui est
épinglé à l'occasion pour vérifier si c'est encore pertinent. Cela peut
paraître énergivore, mais l'efficacité de votre travail d'équipe va en
bénéficier. Également, rappelez aux membres de votre équipe d'utiliser
les bonnes chaînes pour chacune des discussions. Il ne faut pas que les
conversations se croisent à travers les chaînes. Chaque chaîne a son
utilité et doit être utilisée en conséquence. Les appels d'équipe
doivent aussi se faire dans les bonnes chaînes. Quand vous êtes en
appel, utilisez le fil de discussion pour conserver des traces écrites
des points abordés dans la réunion. Les fils de discussions sont en
général un bon outil pour ne pas se perdre dans un discussion. Si
l'usage des mauvaises chaînes est un problème récurent, il est possible
que la structure que vous employez est mal adaptée à vos travaux. Vous
pouvez alors retourner à la planche à dessin. Assurez-vous que toute
l'équipe comprenne bien comment utiliser Slack. Si ce n'est pas le cas,
formez-les. Une structure adaptée et une équipe bien formée peuvent
faire des miracles.

\hypertarget{collaboration}{%
\subsubsection{Collaboration}\label{collaboration}}

La grande majorité des conversations devraient se faire dans les
chaînes. Les conversations privées ont leur utilité, vous vous en
servirez. Il est parfois nécessaire d'avoir des discussions plus
confidentielles et de parler rapidement à quelqu'un sur un sujet
éphémère. Toutefois, par soucis de transparence et d'inclusion, toute
discussion à propos d'un projet devrait se faire dans sa chaîne. Si vous
jugez qu'un membre d'une chaîne ne devrait pas lire ce que vous avez à
dire sur le projet, c'est qu'il ne devrait pas faire partie de la
chaîne. Par rapport aux membres, trouvez le bon équilibre par rapport à
qui devrait être dans quelle chaîne. L'objectif n'est pas d'exclure et
de cacher du contenu, vous voulez une équipe transparente. Vous voulez
que vos membres restent bien informés de l'avancement des projets sans
les submerger d'information qui ne leur est pas utile. C'est à vous de
trouver la formule gagnante. Invitez vos partenaires externes dans votre
espace d'équipe. Créez des chaînes spécifiques aux partenaires pour que
les conversations externes soient tout aussi organisées. N'invitez pas
vos partenaires dans vos chaînes privées, question de confidentialité.
Si un partenaire n'a pas l'habitude d'utiliser Slack ou l'outil que vous
utilisez, proposez-lui de vous y joindre quand même. Moins vous utilisez
les outils des autres, plus vous gardez centralisées vos communications
et évitez de jongler avec plusieurs plateformes.

\hypertarget{optimisation-personnelle}{%
\subsubsection{Optimisation
personnelle}\label{optimisation-personnelle}}

Une fois que la structure d'équipe est définie et que vos membres et vos
partenaires sont à l'aise avec l'utilisation de la plateforme, il est
temps d'organiser la structure de votre Slack personnel. Créez des
sections pour trier les chaînes. La structure d'équipe est essentielle,
mais une fois qu'elle est déterminée, chaque membre n'utilise pas
forcément les chaînes de la même façon. Vous pouvez vous créer une
section de favoris, ou encore différentes sections par rapport aux
différents thèmes pour y faciliter la navigation. Également, ajustez vos
paramètres de notifications. C'est à vous de déterminer quelle chaînes
méritent de produire des alertes, et à quels moments vous souhaitez les
recevoir. Slack a plusieurs applications intégrées qui facilitent la
compatibilité avec vos autres outils. Vous pouvez connecter votre
calendrier, votre Notion et votre GitHub pour recevoir des alertes
pertinentes. Allez explorer ces applications pour déterminer lesquelles
vous conviennent.

Tel que mentionné précédemment, plusieurs logiciels peuvent convenir à
vos besoins. Puisque nous utilisons Slack, voici quelques raisons qui
pourraient vous convaincre d'opter pour cette option ou de vous en
éloigner. Sachez que cette liste n'est pas du tout exhaustive, mais
reflète simplement quelques-unes de nos observations par rapport à notre
outil de travail.

\begin{itemize}
\item
  Avantages

  L'utilisation de Slack est très intuitive. Nous l'utilisons
  régulièrement dans des cours, et les étudiants apprennent rapidement à
  l'utiliser. La distinction entre les chaînes publiques accessibles à
  tous les membres d'un espace d'équipe et les chaînes privées est
  claire et simple d'utilisation. Slack offre aussi une fonction de
  recherche, qui vous permet de retrouver des messages à travers les
  chaînes. L'intégration de applications qui font le pont avec d'autres
  outils est fort appréciée. Enfin, Slack est utilisé partout dans le
  monde par des équipes de toutes les tailles et dans tous les domaines.
  C'est un outil très présent en recherche académique qui facilite la
  collaboration et la multidisciplinarité. Les chances sont élevées que
  vos partenaires utilisent déjà l'outil, ou au minimum en aient déjà
  entendu parlé.
\item
  Inconvénients

  Si vous avez l'habitude d'utiliser les outils d'une suite, comme
  celles de Microsoft ou de Google, il est possible que vous trouviez
  l'intégration de ces outils à Slack moins pratique que si vous
  utilisiez les plateformes proposées par ces compagnies. Également,
  gardez en tête que la version gratuite de Slack a plusieurs
  limitations. Elle implique notamment un limite de temps par rapport à
  l'archivage des messages, que vous ne pourrez pas retracer après 90
  jours. Les coûts pour utiliser Slack à son plein potentiel peuvent
  être élevés, mais puisque ce genre d'outils est de plus en plus
  répandu, il est fort possible que son utilisation soit financée par
  votre département.
\end{itemize}

\hypertarget{logiciel-de-gestion-de-versions-duxe9centralisuxe9}{%
\section{Logiciel de gestion de versions
décentralisé}\label{logiciel-de-gestion-de-versions-duxe9centralisuxe9}}

Lorsque l'on aborde le domaine de la recherche scientifique en sciences
sociales numériques, la collaboration et la gestion efficace du code
deviennent des éléments cruciaux pour progresser dans ses projets. Dans
cette optique, les outils de gestion de versions décentralisés ont pris
une place prépondérante. Parmi eux, Git et GitHub se démarquent tant par
leur popularité que par leur efficacité.

\hypertarget{pourquoi-choisir-git-et-github}{%
\subsection{Pourquoi choisir Git et
GitHub?}\label{pourquoi-choisir-git-et-github}}

\hypertarget{avantages-1}{%
\subsubsection{Avantages}\label{avantages-1}}

Git, développé par Linus Torvalds en 2005, s'est imposé comme le système
de gestion de versions décentralisé de référence. Sa principale force
réside dans sa capacité à suivre l'évolution d'un projet en enregistrant
les modifications apportées au code source. Chaque modification est
enregistrée sous forme de dépôts (\emph{commits}), avec un message
explicatif, permettant aux collaborateurs de comprendre facilement les
évolutions du projet.

GitHub, lancé en 2008, est une plateforme qui utilise Git comme base
pour l'entreposage et la gestion de projets. C'est une vitrine virtuelle
où les développeurs peuvent héberger leurs dépôts Git et collaborer de
manière transparente. L'aspect social de GitHub, avec ses
fonctionnalités de suivi des projets, de gestion des problèmes et de
demandes de fusion, en fait un lieu de choix pour les projets en code
source ouvert et collaboratifs.

En sciences sociales numériques, où le partage et la collaboration sont
essentiels, Git et GitHub offrent plusieurs avantages majeurs. Tout
d'abord, ils permettent de suivre les modifications apportées au code,
ce qui facilite la reproductibilité des résultats. Les chercheurs
peuvent revenir à n'importe quelle version précédente du code, ce qui
est particulièrement utile pour corriger des erreurs ou analyser
l'impact de différentes approches.

De plus, Git et GitHub favorisent le travail collaboratif. Plusieurs
chercheurs peuvent travailler sur le même projet simultanément, chacun
dans sa branche de développement. Une fois les modifications effectuées,
il est possible de fusionner les branches pour intégrer les changements.
Cette approche évite les conflits majeurs et facilite la répartition des
tâches au sein de l'équipe.

Enfin, l'aspect de code source ouvert de GitHub permet aux chercheurs en
sciences sociales numériques de partager leurs codes avec la communauté
académique et de bénéficier des contributions d'autres chercheurs. Cela
favorise un environnement de partage des connaissances et de
collaboration fructueuse.

\hypertarget{inconvuxe9nients}{%
\subsubsection{Inconvénients}\label{inconvuxe9nients}}

Cependant, Git et GitHub ne sont pas sans leurs défis. La courbe
d'apprentissage peut être raide pour les débutants, car ces outils
impliquent des concepts spécifiques tels que les branches, les conflits
de fusion et les requêtes de tirage. De plus, bien que GitHub offre un
niveau de gratuité pour les projets en code source ouvert, des frais
peuvent être appliqués pour des fonctionnalités avancées ou pour des
projets privés.

\hypertarget{comment-les-utiliser-efficacement-en-paralluxe8le-uxe0-dropbox-etc.}{%
\subsection{Comment les utiliser efficacement (en parallèle à Dropbox,
etc.)}\label{comment-les-utiliser-efficacement-en-paralluxe8le-uxe0-dropbox-etc.}}

Pour utiliser Git et GitHub efficacement dans un contexte de recherche
en sciences sociales numériques, il est recommandé de suivre quelques
bonnes pratiques. Tout d'abord, il est important de structurer son dépôt
Git de manière logique, en organisant les fichiers et les dossiers de
manière cohérente. Les messages de commit doivent être descriptifs et
clairs, pour permettre à tous les collaborateurs de comprendre les
changements effectués.

Il est également conseillé de travailler sur des branches distinctes
pour chaque fonctionnalité ou modification majeure. Cela facilite la
gestion des changements et minimise les conflits lors de la fusion. Les
chercheurs devraient également consulter régulièrement les projets et
les problèmes sur GitHub pour encourager une communication ouverte et
résoudre rapidement les problèmes.

L'utilisation de Git et de GitHub peut être complémentaire à d'autres
outils d'entreposage, tels que Dropbox ou Google Drive. Ces derniers
peuvent être utilisés pour entreposer des fichiers non liés au code,
tels que des données brutes non sensibles ou des documents de recherche,
tandis que Git et GitHub gèrent le code source et ses évolutions.

Bien qu'il existe plusieurs alternatives à l'utilisation combinée de Git
et de GitHub sur le marché, ces deux plateformes liées continuent de
dominer le domaine de la gestion de versions décentralisée. Parmi les
alternatives notables, on peut citer Mercurial, Bitbucket, GitLab et
SourceForge. Chacun de ces outils offre des fonctionnalités similaires à
celles de Git et GitHub, mais il est important de comprendre pourquoi
Git et GitHub restent les choix privilégiés pour les chercheurs en
sciences sociales numériques.

\hypertarget{pourquoi-prioriser-git-et-github-pour-les-chercheurs-en-sciences-sociales}{%
\subsection{Pourquoi prioriser Git et GitHub pour les chercheurs en
sciences
sociales}\label{pourquoi-prioriser-git-et-github-pour-les-chercheurs-en-sciences-sociales}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Intégration et adoption répandue}~: Git est devenu un standard
  de facto dans l'industrie du développement logiciel. Sa popularité et
  son adoption répandue signifient que de nombreuses ressources
  d'apprentissage, des tutoriels et des forums de support sont
  disponibles en ligne, ce qui facilite l'utilisation de cet outil pour
  les chercheurs en sciences sociales débutants. GitHub, en tant que
  plateforme principale de gestion des versions, bénéficie également
  d'une grande base d'utilisateurs et d'une communauté active, ce qui
  encourage la collaboration et le partage des connaissances.
\item
  \emph{Facilité de collaboration}~: Git et GitHub sont conçus pour
  faciliter la collaboration entre les individus et les équipes. Les
  chercheurs en sciences sociales travaillent souvent ensemble sur des
  projets de recherche, et la capacité de suivre les modifications, de
  gérer les conflits et de fusionner les contributions devient
  essentielle. L'interface conviviale de GitHub, avec des
  fonctionnalités telles que les demandes de fusion et les commentaires
  en ligne, simplifie grandement la collaboration.
\item
  \emph{Visibilité et partage}~: GitHub brille par sa fonctionnalité de
  projet open source, qui permet aux chercheurs en sciences sociales de
  partager leurs travaux avec la communauté mondiale. Les projets en
  code source ouvert sont visibles et accessibles à tous, favorisant
  ainsi la collaboration et l'examen par les pairs. Cela peut être
  particulièrement bénéfique pour les chercheurs souhaitant contribuer à
  des initiatives académiques et collaborer à des projets
  interdisciplinaires.
\item
  \emph{Suivi des versions et recherche reproductible}~: Les chercheurs
  en sciences sociales doivent s'assurer que leurs travaux sont
  reproductibles et vérifiables. Git permet de suivre les versions du
  code, ce qui signifie que les chercheurs peuvent retrouver facilement
  des versions antérieures pour reproduire des analyses spécifiques ou
  corriger des erreurs. Cette fonctionnalité est cruciale pour maintenir
  l'intégrité des résultats de recherche.
\item
  \emph{Infrastructure et sécurité}~: GitHub offre une infrastructure
  robuste pour l'entreposage sécurisé des dépôts Git. Les chercheurs
  peuvent être assurés que leurs travaux sont sauvegardés et protégés
  contre les pertes de données accidentelles. De plus, les contrôles
  d'accès et les autorisations granulaires de GitHub permettent aux
  chercheurs de contrôler qui peut accéder et contribuer à leurs
  projets.
\end{enumerate}

En somme, Git et GitHub offrent aux chercheurs en sciences sociales
numériques un moyen puissant de gérer leur code, de collaborer
efficacement et de contribuer à la communauté académique grâce à l'open
source. Bien que leur apprentissage puisse représenter un défi initial,
les avantages qu'ils apportent en termes de suivi des versions, de
collaboration et de partage des connaissances en font des outils
essentiels dans l'arsenal de tout chercheur moderne.

\hypertarget{pratiques-uxe0-uxe9viter-sur-github-pour-les-chercheurs-en-sciences-sociales}{%
\subsection{Pratiques à éviter sur GitHub pour les chercheurs en
sciences
sociales}\label{pratiques-uxe0-uxe9viter-sur-github-pour-les-chercheurs-en-sciences-sociales}}

Lorsque les chercheurs en sciences sociales utilisent GitHub pour
partager leur code, collaborer sur des projets et contribuer à la
communauté académique, il est essentiel de connaître les pratiques à
éviter. En effet, certaines erreurs peuvent compromettre la sécurité, la
confidentialité et l'efficacité de la recherche. Voici quelques éléments
à éviter~:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Entreposer des informations sensibles}~: Évitez d'entreposer des
  données sensibles ou confidentielles sur GitHub. Cela inclut les
  données de sondages, les informations personnelles identifiables et
  tout autre contenu pouvant porter atteinte à la vie privée des
  individus. Assurez-vous de supprimer ou de masquer soigneusement ces
  informations avant de les télécharger sur la plateforme.
\item
  \emph{Inclure des mots de passe et clés d'accès}~: Ne jamais inclure
  de mots de passe, de clés d'accès ou d'informations d'identification
  dans votre code source. Cela peut compromettre la sécurité de vos
  systèmes et de vos données. Utilisez plutôt des méthodes sécurisées
  pour gérer ces informations, telles que les variables d'environnement
  ou les fichiers de configuration externes.
\item
  \emph{Entreposer des fichiers lourds}~: Évitez d'entreposer des
  fichiers volumineux sur GitHub, notamment des fichiers binaires, des
  données brutes massives ou des ensembles de données volumineux. Ces
  fichiers peuvent ralentir les opérations de clonage et de fusion, ce
  qui affecte la performance globale du dépôt. Utilisez plutôt des
  services d'entreposage dédiés pour ces fichiers et fournissez des
  liens vers ces ressources dans votre dépôt.
\item
  \emph{Inclure des identifiants personnels}~: Évitez de publier vos
  propres identifiants personnels, tels que des numéros de sécurité
  sociale, des numéros de carte de crédit ou d'autres informations
  confidentielles. Ces informations pourraient être exploitées à des
  fins malveillantes si elles tombent entre de mauvaises mains.
\item
  \emph{Ignorer les pratiques de branches et de fusion}~: Évitez de
  fusionner directement du code dans la branche principale
  (habituellement appelée \emph{main} ou \emph{master}). Utilisez plutôt
  des branches distinctes pour les fonctionnalités et les corrections,
  et suivez les pratiques de fusion pour intégrer proprement les
  changements. Ignorer ces pratiques peut entraîner des conflits et une
  perte de trace des modifications.
\item
  \emph{Ignorer les commentaires des collaborateurs}~: Lorsque vous
  travaillez avec d'autres chercheurs, ne négligez pas les commentaires
  et les suggestions qu'ils fournissent. Les retours d'expérience et les
  idées des autres peuvent contribuer à améliorer la qualité de votre
  code et de vos analyses.
\item
  \emph{Ne pas documenter}~: Évitez de ne pas documenter votre code. Une
  documentation claire et détaillée est essentielle pour permettre à
  d'autres chercheurs de comprendre vos méthodes et vos résultats.
  Utilisez des commentaires explicatifs et fournissez des explications
  sur la manière d'exécuter votre code.
\end{enumerate}

En suivant ces conseils et en évitant ces erreurs courantes, les
chercheurs en sciences sociales peuvent garantir la sécurité, la qualité
et l'efficacité de leurs projets sur GitHub. La responsabilité de
préserver la confidentialité des données et de créer un environnement de
travail collaboratif et respectueux repose sur les épaules de chaque
contributeur.

\hypertarget{exemple-dutilisation-de-git-et-de-github-pour-un-chercheur-en-sciences-sociales}{%
\subsection{Exemple d'utilisation de Git et de GitHub pour un chercheur
en sciences
sociales}\label{exemple-dutilisation-de-git-et-de-github-pour-un-chercheur-en-sciences-sociales}}

Dans le contexte de la recherche en sciences sociales numériques, la
gestion efficace du code, la collaboration transparente et la
préservation des données sensibles sont des impératifs. Imaginons que
vous êtes un jeune chercheur en sciences sociales qui étudie l'impact
des médias sur l'opinion publique. Vous utilisez le langage de
programmation R pour analyser des données de médias et des données de
sondage. Bien que vous travailliez seul, vous souhaitez rendre votre
travail accessible à votre équipe pour validation et permettre à vos
collègues de contribuer aux améliorations. Voici comment vous pouvez
utiliser Git et GitHub pour gérer votre projet de manière structurée et
collaborative.

\hypertarget{uxe9tape-1-cruxe9ation-dun-ruxe9pertoire-local-et-initialisation-de-git}{%
\subsubsection{Étape 1~: Création d'un répertoire local et
initialisation de
Git}\label{uxe9tape-1-cruxe9ation-dun-ruxe9pertoire-local-et-initialisation-de-git}}

Ouvrez votre terminal et naviguez vers le dossier où vous souhaitez
enregistrer votre projet.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ chemin/vers/votre/dossier}
\end{Highlighting}
\end{Shaded}

Créez un nouveau répertoire pour votre projet et accédez-y.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ mon\_projet}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ mon\_projet}
\end{Highlighting}
\end{Shaded}

Initialisez Git dans ce répertoire.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ init}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-2-ajout-de-votre-code-et-de-vos-fichiers}{%
\subsubsection{Étape 2~: Ajout de votre code et de vos
fichiers}\label{uxe9tape-2-ajout-de-votre-code-et-de-vos-fichiers}}

Ajoutez vos fichiers R contenant le code pour l'analyse des médias et
des sondages dans le répertoire. Par exemple, vous pouvez avoir des
fichiers \emph{analyse\_medias.R} et \emph{analyse\_sondages.R}.

Utilisez la commande \texttt{git\ status} pour vérifier l'état de vos
fichiers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ status}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-3-ajout-validation-et-commit-de-vos-modifications}{%
\subsubsection{Étape 3~: Ajout, validation et commit de vos
modifications}\label{uxe9tape-3-ajout-validation-et-commit-de-vos-modifications}}

Ajoutez vos fichiers pour qu'ils soient prêts à être validés.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ add }\AttributeTok{{-}A}
\end{Highlighting}
\end{Shaded}

Validez vos modifications avec un message descriptif.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Ajout du code d\textquotesingle{}analyse des médias et des sondages"}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-4-cruxe9ation-du-ruxe9pertoire-sur-github-et-du-lien-avec-votre-ruxe9pertoire-local}{%
\subsubsection{Étape 4~: Création du répertoire sur GitHub et du lien
avec votre répertoire
local}\label{uxe9tape-4-cruxe9ation-du-ruxe9pertoire-sur-github-et-du-lien-avec-votre-ruxe9pertoire-local}}

Allez sur GitHub et connectez-vous à votre compte. Créez un nouveau
répertoire vide avec le nom \emph{mon\_projet}.

De retour dans votre terminal, ajoutez le lien GitHub à votre répertoire
local.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ remote add origin https://github.com/votre{-}utilisateur/mon\_projet.git}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-5-push-de-votre-travail-sur-github}{%
\subsubsection{Étape 5~: Push de votre travail sur
GitHub}\label{uxe9tape-5-push-de-votre-travail-sur-github}}

Envoyez vos commits locaux vers GitHub.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ push }\AttributeTok{{-}u}\NormalTok{ origin master}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-6-collaboration-avec-vos-colluxe8gues}{%
\subsubsection{Étape 6~: Collaboration avec vos
collègues}\label{uxe9tape-6-collaboration-avec-vos-colluxe8gues}}

Si vos collègues souhaitent contribuer à votre projet, ils peuvent
\emph{forker} votre répertoire sur GitHub, ce qui créera une copie dans
leur propre compte.

Lorsqu'ils ont fait des modifications dans leur copie, ils peuvent
soumettre une \emph{pull request} pour vous demander de fusionner leurs
modifications dans votre répertoire principal.

\hypertarget{uxe9tape-7-pull-des-modifications-de-vos-colluxe8gues}{%
\subsubsection{Étape 7~: Pull des modifications de vos
collègues}\label{uxe9tape-7-pull-des-modifications-de-vos-colluxe8gues}}

Lorsque vos collègues ont soumis des modifications et vous ont demandé
de les fusionner, vous pouvez mettre à jour votre répertoire local avec
leurs changements.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ pull origin master}
\end{Highlighting}
\end{Shaded}

\hypertarget{uxe9tape-8-ruxe9puxe9ter-le-processus}{%
\subsubsection{Étape 8~: Répéter le
processus}\label{uxe9tape-8-ruxe9puxe9ter-le-processus}}

Répétez les étapes 2 à 7 au fur et à mesure que vous développez votre
projet, ajoutez du code, effectuez des analyses et collaborez avec vos
collègues. Assurez-vous de valider et de pousser régulièrement vos
modifications pour maintenir le dépôt à jour.

\hypertarget{github-desktop}{%
\subsection{GitHub Desktop}\label{github-desktop}}

Alors que le terminal reste une approche fondamentale pour maîtriser Git
et GitHub, il existe des outils conviviaux tels que GitHub Desktop qui
offrent une alternative intuitive. Cet outil simplifie le processus de
gestion de versions décentralisée, en particulier pour ceux qui
souhaitent commencer par une approche visuelle. Cependant, comprendre
son fonctionnement et équilibrer les avantages et les inconvénients est
essentiel.

\begin{figure}

{\centering \includegraphics{images/chapitre8_gitdesktop.png}

}

\caption{image}

\end{figure}

GitHub Desktop fournit une vue claire de vos dépôts, de vos
modifications, de vos branches et de vos demandes de fusion. Il élimine
la nécessité de mémoriser les commandes en ligne de terminal, ce qui
peut être un défi pour certains chercheurs. L'application simplifie
également la résolution des conflits lors de la fusion des branches.

Toutefois, en utilisant GitHub Desktop, il est possible de perdre la
compréhension des commandes Git en ligne de commande, ce qui pourrait
devenir un inconvénient si vous devez travailler dans un environnement
sans interface visuelle. De plus, GitHub Desktop est spécifiquement
conçu pour interagir avec GitHub. Si vous devez travailler avec d'autres
plateformes de gestion de versions, cela pourrait poser des problèmes.

La décision entre l'utilisation du terminal et de GitHub Desktop dépend
de vos préférences et de vos besoins. Pour les chercheurs qui débutent,
GitHub Desktop offre une transition en douceur vers les concepts de
gestion de versions. Cependant, il est important de ne pas se limiter à
une interface visuelle. Comprendre les commandes Git en ligne de
commande reste essentiel pour résoudre des problèmes complexes, gérer
des projets avancés et collaborer avec d'autres chercheurs qui utilisent
des approches basées sur le terminal.

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

En utilisant Git et GitHub de manière stratégique, vous pouvez gérer
efficacement votre projet de recherche en sciences sociales, collaborer
avec vos collègues et rendre votre travail accessible tout en préservant
la confidentialité des données sensibles. Ce processus contribue à un
environnement de recherche collaboratif et structuré, essentiel pour
mener à bien vos analyses sur l'impact des médias sur l'opinion
publique.

\hypertarget{outils-dentreposage-des-donnuxe9es}{%
\section{Outils d'entreposage des
données}\label{outils-dentreposage-des-donnuxe9es}}

L'entreposage des données occupe une place cruciale dans la recherche en
sciences sociales numériques. La manière dont vous entreposez et gérez
vos données peut avoir un impact significatif sur la sécurité, la
confidentialité et la reproductibilité de votre travail. Dans cette
section, nous allons aborder différents aspects de l'entreposage de
données, des outils disponibles et de l'importance d'une gestion
efficace de vos fichiers.

\hypertarget{entreposage-de-donnuxe9es-non-sensibles}{%
\subsection{Entreposage de données non
sensibles}\label{entreposage-de-donnuxe9es-non-sensibles}}

Au fil du temps, de nombreux outils d'entreposage ont émergé pour
répondre aux besoins variés des chercheurs en sciences sociales. Des
solutions populaires incluent Dropbox, Google Drive, OneDrive et Amazon
S3 d'AWS. L'histoire de ces outils témoigne de l'évolution des besoins
d'entreposage et de collaboration.

Lorsqu'il s'agit d'entreposer vos données de recherche, la règle d'or
est de ne jamais perdre d'informations précieuses. Cette préoccupation
prend toute son importance lorsqu'un chercheur en sciences sociales,
seul ou en équipe restreinte, se lance dans un projet. Pour répondre à
ce besoin, les services d'entreposage cloud tels que Dropbox, Google
Drive et OneDrive se révèlent indispensables. Voici quelques avantages
d'un entreposage sur le cloud pour la recherche~:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Sauvegarde automatique}~: Les solutions cloud sauvegardent
  automatiquement vos fichiers, garantissant que vous ne perdrez jamais
  vos données en cas de panne d'ordinateur ou d'accident.
\item
  \emph{Accessibilité universelle}~: Vous pouvez accéder à vos fichiers
  à partir de n'importe quel appareil avec une connexion Internet, ce
  qui favorise la flexibilité dans la gestion de vos projets.
\item
  \emph{Partage facilité}~: Les services cloud permettent de partager
  facilement des fichiers et des dossiers avec des collègues, même en
  dehors de votre équipe de recherche. Cela favorise la collaboration et
  la communication.
\end{enumerate}

Il est important de noter que le choix d'un service cloud dépend de vos
besoins et de vos préférences. Considérez des facteurs tels que la
capacité d'entreposage, les fonctionnalités de partage, la convivialité
et la compatibilité avec vos outils de recherche existants.

Dropbox est connu pour sa simplicité d'utilisation et sa convivialité.
Il peut être un choix approprié pour entreposer des fichiers non
sensibles, partager des documents avec des collègues et faciliter la
collaboration.

Pour utiliser Dropbox efficacement, organisez vos fichiers en
arborescence logique. Créez des dossiers spécifiques pour chaque projet
et partagez-les avec les membres de votre équipe. Pour éviter de pousser
des fichiers sensibles sur GitHub, ajoutez le nom de dossier à exclure
dans un fichier \emph{.gitignore}.

\begin{figure}

{\centering \includegraphics{images/chapitre8_dropbox.png}

}

\caption{image1}

\end{figure}

Dropbox offre un suivi automatique des modifications, ce qui vous permet
de remonter dans le temps pour restaurer des versions antérieures de vos
fichiers. Cela garantit l'intégrité de vos données et vous permet de
revenir à des versions précédentes si nécessaire. De plus, l'archivage
de dossiers et de projets complets peut aider à conserver une vue
chronologique de votre travail au fil du temps.

Il est également crucial de considérer la taille de vos données. Si vous
traitez des fichiers volumineux tels que des images, des vidéos ou des
ensembles de données massifs, il peut être judicieux d'utiliser un
service cloud pour entreposer ces fichiers et les partager avec vos
collaborateurs, plutôt que de les pousser sur des plateformes de gestion
de versions comme GitHub.

Pour les données sensibles, les services cloud tels que Dropbox et
Google Drive peuvent ne pas être suffisamment sécurisés. C'est là que
des solutions comme AWS entrent en jeu. Cependant, il est important de
noter que l'utilisation d'AWS peut s'avérer complexe, en particulier
pour un jeune chercheur travaillant en solo ou en petite équipe.

\hypertarget{entreposage-de-donnuxe9es-sensibles}{%
\subsection{Entreposage de données
sensibles}\label{entreposage-de-donnuxe9es-sensibles}}

Lorsqu'il s'agit d'entreposer des données sensibles, tels que des
données de sondage comportant des informations personnelles
identifiables, la sécurité et la confidentialité sont essentielles.
Comme abordé précédemment, GitHub n'est pas adapté à l'entreposage de
telles données en raison de ses caractéristiques publiques et de son
orientation vers le code source ouvert. Une solution courante est
d'utiliser des services de cloud sécurisés, tels qu'AWS, qui offrent des
mesures de sécurité robustes pour protéger vos données sensibles.

AWS regroupe un ensemble de services \emph{cloud} proposés par Amazon.
Il offre une vaste gamme de services, allant de l'entreposage et de la
gestion des données à la computation et à l'analyse avancée. AWS est
conçu pour offrir une infrastructure hautement évolutive et sécurisée,
ce qui en fait un choix attrayant pour les chercheurs qui gèrent des
données sensibles. L'outil présente de multiples avantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Sécurité robuste}~: AWS met l'accent sur la sécurité, avec des
  fonctionnalités telles que le chiffrement des données en transit et au
  repos, la gestion des accès basée sur les rôles et la conformité à des
  normes de sécurité strictes.
\item
  \emph{Scalabilité}~: AWS permet de faire évoluer vos ressources en
  fonction des besoins, garantissant des performances optimales même
  lorsque vos projets de recherche croissent en taille et en complexité.
\item
  \emph{Flexibilité}~: AWS propose une variété de services adaptés à
  différentes utilisations, allant de l'entreposage de données au calcul
  intensif pour l'analyse avancée.
\item
  \emph{Collaboration simplifiée}~: Bien que le coût d'entrée soit
  généralement bas, la possibilité de partager des ressources avec des
  collègues et de travailler en équipe rend AWS adapté à la
  collaboration.
\end{enumerate}

AWS n'est pas le seul service cloud disponible. Microsoft Azure et
Google Cloud Platform (GCP) sont des concurrents majeurs offrant des
fonctionnalités similaires. Lorsque vous choisissez un fournisseur,
prenez en compte les coûts, la convivialité et les fonctionnalités
offertes. Le coût d'utilisation d'AWS peut varier en fonction des
services utilisés, de la quantité de données entreposées et de la
capacité de calcul requise. Lorsque vous travaillez seul, le coût peut
sembler élevé par rapport à l'utilisation de solutions gratuites telles
que Dropbox. Cependant, en équipe, la répartition des coûts peut rendre
AWS plus abordable.

\hypertarget{exemple-dutilisation-daws-pour-entreposer-et-accuxe9der-uxe0-des-donnuxe9es-de-sondages-dans-rstudio}{%
\subsubsection{Exemple d'utilisation d'AWS pour entreposer et accéder à
des données de sondages dans
RStudio}\label{exemple-dutilisation-daws-pour-entreposer-et-accuxe9der-uxe0-des-donnuxe9es-de-sondages-dans-rstudio}}

Imaginez un jeune chercheur en sciences sociales qui travaille sur une
analyse comparative de données de sondages recueillies sur plusieurs
décennies. Pour maintenir la sécurité des données sensibles et faciliter
l'accès pour les analyses dans RStudio, il décide d'utiliser AWS pour
l'entreposage et la gestion de ses données.

\hypertarget{uxe9tape-1-cruxe9ation-dun-compte-aws-et-configuration}{%
\paragraph{Étape 1~: Création d'un compte AWS et
configuration}\label{uxe9tape-1-cruxe9ation-dun-compte-aws-et-configuration}}

Le chercheur crée un compte AWS et configure ses paramètres de sécurité,
y compris la configuration de l'authentification à deux facteurs pour
renforcer la sécurité de son compte.

\hypertarget{uxe9tape-2-cruxe9ation-dun-espace-dentreposage-s3}{%
\paragraph{Étape 2~: Création d'un espace d'entreposage
S3}\label{uxe9tape-2-cruxe9ation-dun-espace-dentreposage-s3}}

Le chercheur crée un compartiment Amazon S3 (Simple Storage Service)
pour entreposer ses données de sondage. Il choisit une région AWS et
définit les paramètres de sécurité appropriés, tels que le chiffrement
des données.

\hypertarget{uxe9tape-3-transfert-des-donnuxe9es-vers-amazon-s3}{%
\paragraph{Étape 3~: Transfert des données vers Amazon
S3}\label{uxe9tape-3-transfert-des-donnuxe9es-vers-amazon-s3}}

Le chercheur transfère les données de sondage dans son compartiment
Amazon S3 à l'aide de l'interface en ligne AWS ou d'outils
d'importation.

\hypertarget{uxe9tape-4-configuration-des-autorisations}{%
\paragraph{Étape 4~: Configuration des
autorisations}\label{uxe9tape-4-configuration-des-autorisations}}

Pour sécuriser davantage les données, le chercheur configure les
autorisations d'accès aux données dans Amazon S3. Il attribue des rôles
et des politiques d'accès spécifiques aux utilisateurs, garantissant que
seules les personnes autorisées peuvent accéder aux données.

\hypertarget{uxe9tape-5-configuration-daccuxe8s-dans-rstudio}{%
\paragraph{Étape 5~: Configuration d'accès dans
RStudio}\label{uxe9tape-5-configuration-daccuxe8s-dans-rstudio}}

Le chercheur installe le package \emph{aws.s3} dans RStudio pour accéder
à ses données entreposées dans Amazon S3. Il configure également les
informations d'identification AWS dans son environnement RStudio.

\hypertarget{uxe9tape-6-accuxe8s-et-analyse-des-donnuxe9es-dans-rstudio}{%
\paragraph{Étape 6~: Accès et analyse des données dans
RStudio}\label{uxe9tape-6-accuxe8s-et-analyse-des-donnuxe9es-dans-rstudio}}

À l'aide du package \emph{aws.s3}, le chercheur peut maintenant accéder
à ses données directement dans RStudio par quelques lignes de code. Il
peut charger les données dans des structures de données R et effectuer
des analyses statistiques, des visualisations et des croisements.

\hypertarget{uxe9tape-7-suxe9curituxe9-et-conservation-des-donnuxe9es}{%
\paragraph{Étape 7~: Sécurité et conservation des
données}\label{uxe9tape-7-suxe9curituxe9-et-conservation-des-donnuxe9es}}

Après avoir effectué ses analyses, le chercheur peut choisir de
conserver les données de sondage dans Amazon S3 en utilisant les
politiques de conservation appropriées. Il peut également archiver des
copies de sauvegarde pour garantir l'intégrité des données à long terme.

Dropbox se concentre principalement sur l'entreposage et la
collaboration de fichiers, alors que AWS offre une gamme de services
\emph{cloud}, y compris l'entreposage sécurisé de données sensibles et
la mise en place d'infrastructures évolutives. GitHub, d'autre part, se
concentre sur la gestion de versions et la collaboration de code source.
Chaque outil a son propre domaine d'expertise et peut être utilisé de
manière complémentaire pour différents aspects de la recherche.

\hypertarget{conclusion-2}{%
\subsection{Conclusion}\label{conclusion-2}}

L'entreposage des données est une étape cruciale dans la recherche en
sciences sociales numériques. Choisissez des outils adaptés à la
sensibilité des données, privilégiez des services sécurisés comme AWS
pour les données sensibles, et utilisez Dropbox pour la collaboration et
l'entreposage de fichiers non sensibles. Une gestion efficace des
versions, de la structure des dossiers et de la sécurité garantira
l'intégrité de vos données et facilitera la collaboration tout au long
de vos projets de recherche.

\bookmarksetup{startatroot}

\hypertarget{sec-chap4}{%
\chapter{Gestion de la littérature}\label{sec-chap4}}

\hypertarget{straight-up-from-jedro-arnaud}{%
\section{Straight up from Jedro
Arnaud}\label{straight-up-from-jedro-arnaud}}

Comme mentionné précédemment, les outils numériques de données mas-
sives facilitent le travail des personnes chercheuses lors de la récolte
de données dans le cadre d'analyses empiriques. Cependant, la révolution
technologique offre également des outils pouvant être utiles lors
d'autres étapes du cycle de la recherche. Il s'agit notamment du cas de
la revue de littérature, alors que de nombreux outils offrent aux
personnes chercheuses des ressources permettant d'élaborer un cadre
théorique exhaustif par le biais de données massives sur la littérature
scientifique. L'outil Covidence, géré par une compagnie sans but
lucratif, en est un exemple particulière- ment prisé du monde académique
lors de l'entreprise de revues de littérature. La plateforme en ligne
Covidence est utilisée pour faciliter les revues systématiques de
littérature. Cette dernière permet de réduire drastiquement le temps
d'accomplissement du travail en plus de le rendre plus simple et plus
intuitif. L'outil a été développé pour mieux gérer et organiser
l'évaluation de quantité importante d'études scientifiques. L'exécution
d'une revue de littérature sur Covidence se fait par le biais d'un
double codage. C'est- à-dire que l'évaluation des études se fait
manuellement par deux codeurs

3.6 Covidence : outil de récolte d'articles scientifiques travaillant de
manière autonome et qui mettront en commun leurs résultats à la fin de
l'exercice. L'outil est reconnu pour ses trois étapes précises : « Title
and abstract screening », « Full text review » et « Extraction ».
Covidence permet d'importer des données massives provenant de base de
données bibliographiques. En effet, l'outil lance des requêtes auprès de
multiples bibliothèques, ce qui offre l'accès à des milliers d'études
sur le champ étudié par les personnes chercheuses. Ces requêtes sont
adap- tées aux besoins spécifiques de la personne chercheuse voulant
explorer en profondeur un domaine de la littérature scientifique. La
première étape, soit le « Title and abstract screening », consiste en la
révision des titres et des résumés des articles récoltés. Pour rendre le
travail davantage eﬀicace, il est nécessaire d'inclure des critères
précis pour analyser les titres et résumés d'articles. En se servant du
jugement et des critères qui étaient recherchés, les individus doivent
éliminer ou accepter selon la pertinence de l'article quant à la
littérature étudiée. Cette partie est souvent longue, puisque la
littérature existante est souvent massive. Il est donc important pour
les personnes chercheuses de se rencontrer à maintes reprises pour
discuter des conflits de jugement et pour trouver des compromis. En
outre, cette étape, plutôt longue, s'avère très utile et motivante,
puisqu'il est possible de développer un jugement critique davantage
raﬀiné et de s'instruire dans une littérature continuellement plus
précise. Une fois avoir complété la revue des titres et des résumés, il
faut entamer le « Full text review » qui, comme l'indique le nom,
consiste à la révision complète des textes sélectionnés. Cette étape
demande d'analyser chaque texte, puis de voter « oui », « non » ou «
peut-être » quant à la conservation du texte dans la revue de
littérature. Le vote permet donc soit d'exclure l'article, de le retenir
ou de l'envoyer à la prochaine étape. D'un autre côté, les conflits
rendent le travail beaucoup plus long, puisque les codeurs.euses ont un
texte entier à argumenter. Ainsi, cette partie du travail, bien qu'elle
comporte beaucoup moins de documents, est assez longue et exigeante. La
dernière étape, soit celle de l'extraction, consiste à recueillir toute
donnée étant utile à l'étude de la littérature désignée. Cette étape est
deman- dante, car les chercheur.euse.s doivent se conformer à une grille
de codi- fication prédéfinie. Le but est qu'un consensus entre les
codeurs émerge de ce processus. L'extraction permet de faire ressortir
les théories, les méthodologies et les conclusions présentent dans les
études retenues. Une fois les étapes de la revue systématique terminées,
Covidence facilite l'exportation des résultats de l'extraction sous
forme de tableaux, de graphiques et de rapports pour la méta-analyse ou
pour la rédaction d'articles scientifiques. De nombreuses universités
offrent un accès à Covidence par le biais de licences, et l'outil est
particulièrement utile et bien construit. Toutefois, il existe d'autres
alternatives à Covidence. Le choix de l'outil dépend des coûts de même
que des besoins spécifiques des per- sonnes chercheuses. Les plateformes
DistillerSR, Archie et Rayyan sont notamment largement utilisées par les
personnes chercheuses.

\bookmarksetup{startatroot}

\hypertarget{sec-chap6}{%
\chapter{La gestion des références}\label{sec-chap6}}

\hypertarget{pourquoi-citer}{%
\section{Pourquoi citer ?}\label{pourquoi-citer}}

La citation des sources est une pratique incontournable dans le monde
académique, essentielle à la préservation de la crédibilité académique
et au maintien des normes éthiques. Elle sert de fondement à la
contextualisation de nos recherches, nous permettant de situer nos
travaux au sein d'un cadre scientifique établi et reconnu. Ce processus
de contextualisation facilite non seulement la compréhension de
l'évolution des connaissances dans un domaine donné, mais contribue
également à la création d'une base de connaissances solide et dynamique,
sur laquelle d'autres travaux peuvent être bâtis (Zaid, Shamsudin, and
Habil 2017). La référenciation rigoureuse des travaux antérieurs
garantit la reproductibilité des expériences et des analyses, un pilier
central de la méthodologie scientifique. En fournissant des détails
précis sur les méthodes et résultats, nous ouvrons la voie à la
validation et à l'éventuelle réfutation de nos travaux, renforçant ainsi
l'intégrité de la recherche (Hughes 2013). De plus, la citation adéquate
des sources est une marque de respect envers les contributions des
autres chercheurs, assurant une juste attribution du mérite. Cela
reconnaît l'importance de chaque découverte et idée dans l'avancement de
la science, tout en prévenant le plagiat, une faute grave dans la
recherche académique (Racz and Marković 2018).Enfin, une référenciation
minutieuse aide à éviter les biais, en exposant clairement les
fondements sur lesquels se base notre recherche. Cela permet une
évaluation critique des sources et des perspectives, encourageant une
approche plus équilibrée et nuancée dans l'analyse scientifique (Kostoff
and Cummings 2013).En résumé, la citation des sources est un acte
fondamental qui englobe et adresse de multiples aspects cruciaux de la
recherche académique : de la crédibilité et la contextualisation à la
reproductibilité, de la création d'une base de connaissances solide à
l'attribution correcte du mérite, tout en combattant le plagiat et en
minimisant les biais. C'est dans ce contexte que des outils tels que
Zotero prennent toute leur importance, en facilitant la gestion
rigoureuse des références et en soutenant les chercheurs dans leur quête
de rigueur et d'excellence académiques.

\hypertarget{uxe0-quoi-sert-un-logiciel-de-gestion-bibliographique}{%
\section{À quoi sert un logiciel de gestion bibliographique
?}\label{uxe0-quoi-sert-un-logiciel-de-gestion-bibliographique}}

Un outil de référence bibliographique est un logiciel conçu pour aider
les scientifiques à gérer et à organiser leurs références
bibliographiques de manière efficace. Ces outils s'avèrent
particulièrement utiles lors de la rédaction d'articles de recherche, de
thèses, de mémoires ou d'autres documents académiques. Voici
quelques-unes des fonctions principales d'un tel outil :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Collecte de références : Les outils de référence bibliographique
  permettent aux personnes utilisatrices de collecter et d'importer des
  références bibliographiques à partir de bases de données, de
  catalogues de bibliothèques, de sites Web ou d'autres sources.
  Certains outils offrent même la possibilité d'extraire automatiquement
  les métadonnées à partir de documents PDF.
\item
  Organisation et classement : Les références collectées peuvent être
  organisées en différentes catégories et dossiers. Cela facilite la
  recherche ultérieure et permet de garder une vue d'ensemble claire de
  la bibliographie.
\item
  Citation et génération de bibliographies : L'un des avantages majeurs
  des outils de référence est leur capacité à générer automatiquement
  des citations et des bibliographies conformes à différents styles de
  citation (APA, MLA, Chicago, etc.). Ce processus permet de gagner
  énormément de temps en formatage. Les personnes utilisatrices peuvent
  insérer des références directement dans leurs documents sans avoir à
  se soucier des détails de formatage.
\item
  Collaboration : Certains outils offrent la possibilité de collaborer
  en ligne, ce qui donne l'occasion à plusieurs personnes de travailler
  sur une bibliographie commune. Cela peut être utile pour les projets
  de groupe ou de recherche partagée comme c'est le cas dans une chaire
  de recherche. En plus d'utiliser un même logiciel, l'utilisation d'un
  outil de référencement contribue à économiser du temps par la
  centralisation des données sur un même interface.
\item
  Recherche et exploration : De nombreux outils de référence
  bibliographique offrent des fonctionnalités de recherche avancée qui
  facilitent la découverte de nouvelles références liées à un sujet
  spécifique.
\item
  Synchronisation et sauvegarde : Les références et les bibliographies
  peuvent être synchronisées sur plusieurs appareils, ce offre la
  possibilité aux personnes utilisatrices d'accéder à leurs références
  où qu'elles soient. Les sauvegardes régulières assurent que les
  données ne soient pas perdues en cas de problème technique.
\item
  Suivi de lecture : Certains outils permettent aux personnes
  utilisatrices de suivre les articles et les documents qu'elles ont
  lus, ce qui est particulièrement utile pour garder une trace de la
  littérature pertinente.
\item
  Importation et exportation : Les outils de référence bibliographique
  autorisent généralement l'importation et l'exportation des références
  dans différents formats, ce qui facilite le transfert de données.
\end{enumerate}

En résumé, un outil de référence bibliographique simplifie grandement le
processus de gestion des références bibliographiques, de formatage ainsi
que de création de bibliographies. De plus, ces outils offrent la
flexibilité de changer de style de citation instantanément, facilitant
l'adaptation aux exigences variées des revues scientifiques et
permettant aux chercheurs de se consacrer à l'essence de leurs
recherches sans se préoccuper des contraintes formelles et des détails
de formatage. D'ailleurs, il existe plusieurs outils de référence
bibliographique, dont : Endnote, Zotero et Mendeley.

Chaque logiciel offre des caractéristiques uniques tout en partageant
des objectifs communs fondamentaux. Ils visent principalement à
optimiser l'efficacité et la collaboration en centralisant les
références, une commodité indéniable pour les chercheuses et chercheurs
et les équipes académiques. Le choix d'un logiciel adapté aux besoins
spécifiques des personnes l'utilisants dépend de plusieurs facteurs,
notamment la nécessité de partager les résultats de recherche et de
collaborer sur des projets communs. Lorsque la collaboration est au cœur
d'un projet, il est judicieux que tous les membres de l'équipe adoptent
le même outil pour faciliter l'échange d'informations et la cohésion du
groupe.

Zotero, EndNote et Mendeley, bien qu'ils partagent des principes de base
similaires, se distinguent par des fonctionnalités spécifiques qui
peuvent mieux s'aligner sur les préférences et exigences individuelles.
La sélection d'un logiciel doit donc être guidée par une évaluation
attentive de ses capacités à répondre aux besoins de l'utilisateur, tout
en considérant des aspects cruciaux tels que le partage, la
collaboration et la facilité d'utilisation.

Il est essentiel de souligner l'importance de la préférence personnelle
dans ce choix. L'interface utilisateur, la facilité d'intégration dans
les flux de travail existants, et la compatibilité avec d'autres outils
numériques sont des critères qui influent grandement sur l'expérience
utilisateur et, par conséquent, sur la productivité. En fin de compte,
l'outil idéal est celui qui non seulement facilite la gestion des
références mais s'intègre de manière transparente dans le quotidien
académique de l'utilisateur, lui permettant ainsi de se concentrer
pleinement sur la substance de ses recherches.

\hypertarget{pourquoi-zotero}{%
\section{Pourquoi Zotero?}\label{pourquoi-zotero}}

L'avantage de Zotero réside dans sa gratuité et son accessibilité libre.
Son code est ouvert et son dépôt GitHub compte plus de 13 000
contributions. Il propose une large gamme de fonctionnalités ainsi que
la possibilité d'ajouter des extensions, complétant ainsi son usage.
Zotero est puissant tout en restant facile à utiliser. Il est disponible
sur plusieurs plateformes (Windows, Mac, Linux, iOS, Android),
favorisant ainsi la collaboration entre tous les membres d'une équipe de
recherche utilisant des plateformes diverses. Il est possible de
synchroniser sa bibliothèque Zotero sur plusieurs appareils, soit en
utilisant le service cloud payant de Zotero, soit en configurant son
propre espace de stockage cloud. Zotero s'intègre parfaitement dans un
projet de recherche utilisant LaTeX ou Quarto, car il permet de générer
des fichiers .bib à partir des bibliothèques et de les maintenir à jour
automatiquement. Il s'intègre également aux logiciels de traitement de
texte tels que LibreOffice et Microsoft Office. Il est possible de
générer des bibliographies et des citations dans plus de 9 000 styles de
citation différents, ce qui le rend adaptable à tous les besoins.

Un autre grand avantage de Zotero réside dans la centralisation des
sources bibliographiques et de leurs fichiers associés. Il est possible
d'ajouter des PDF à Zotero et de les synchroniser au sein de groupes de
travail, facilitant ainsi le partage de documents avec les autres
membres de l'équipe de recherche. Plus besoin de recourir à des dossiers
partagés ou d'envoyer des documents par courriel ou via des plateformes
de partage de fichiers : tout est centralisé dans Zotero. Cette
centralisation offre la possibilité d'effectuer des recherches par
mot-clé (type ctrl+f) à travers l'ensemble des sources d'une
bibliothèque. Vous rédigez une conclusion sur les radis finlandais et
souhaitez discuter des enjeux internationaux liés à leur agriculture en
citant une source consultée il y a trois ans ? Avec Zotero, retrouver
cette source ne prend que quelques secondes grâce à une simple
recherche.

L'inconvénient de Zotero réside dans sa difficulté à gérer d'immenses
bibliothèques contenant plusieurs milliers de fichiers, nécessitant
parfois l'achat d'espace de stockage supplémentaire. De plus, bien que
performant, le logiciel n'est pas exempt de défauts. Il arrive qu'il
faille compléter manuellement des informations non détectées
automatiquement par le connecteur intégré.

Zotero est souvent utilisé en combinaison avec BibLaTeX via l'extension
Better BibTeX pour exporter et actualiser automatiquement des
bibliographies au format .bib. BibLaTeX, une extension moderne pour
gérer les bibliographies dans LaTeX et Quarto, s'utilise couramment avec
Biber, un outil de traitement bibliographique avancé compatible avec
BibLaTeX. Biber propose des fonctionnalités telles que le tri poussé, la
gestion de multiples bibliographies et le traitement de divers formats
de données bibliographiques. BibLaTeX, prenant en charge de nombreuses
langues, est idéal pour la rédaction de documents destinés à un public
international. L'exportation de bibliothèques Zotero sous forme de
fichiers .bib pour leur utilisation avec BibLaTeX est simplifiée grâce à
Better BibTeX, qui assure la mise à jour automatique de ces fichiers. Il
est recommandé de maintenir dans votre fichier .bib uniquement les
références utilisées, organisées par ordre alphabétique, afin de
faciliter la collaboration et le partage des ressources.

\hypertarget{biblatex}{%
\section{BibLaTeX}\label{biblatex}}

BibLaTeX est une extension destinée au traitement des bibliographies
dans LaTeX et Quarto, généralement associée à Biber, un programme conçu
pour le traitement des données bibliographiques spécifiquement pour
BibLaTeX. Biber propose des fonctionnalités avancées comme le tri
poussé, la gestion de multiples bibliographies et la capacité de traiter
des sources bibliographiques dans divers formats. Grâce à sa prise en
charge étendue des langues, BibLaTeX est particulièrement adapté à la
rédaction d'articles ou de livres pour un public international. Bien que
BibLaTeX soit avant tout un package pour LaTeX, il est possible
d'exporter des bibliothèques depuis des outils tels que Zotero sous
forme de fichiers .bib, qui peuvent ensuite être exploités avec
BibLaTeX. L'extension Better BibTeX permet de maintenir automatiquement
à jour vos fichiers .bib à partir de Zotero. Il est recommandé de
conserver uniquement les références utilisées dans votre fichier et de
les organiser par ordre alphabétique, facilitant ainsi la coopération et
le partage des sources.

Les inconvénients potentiels de BibLaTeX comprennent une courbe
d'apprentissage plus accentuée pour ceux habitués à BibTeX, ainsi que le
besoin de mises à jour régulières pour assurer la compatibilité avec les
versions les plus récentes de LaTeX. En outre, certains éditeurs
académiques ou revues possèdent leurs propres styles de citation et
peuvent ne pas accepter les soumissions réalisées avec BibLaTeX, même si
cette réticence tend à diminuer.

En conclusion, pour ceux qui cherchent à maximiser la flexibilité et la
puissance de leurs outils de gestion de bibliographie dans LaTeX,
BibLaTeX, en tandem avec Biber, offre une solution moderne et robuste.

\hypertarget{installation-et-configuration-de-zotero}{%
\section{Installation et configuration de
Zotero}\label{installation-et-configuration-de-zotero}}

Dans cette section, vous serez amené notamment à installer Zotero ainsi
que Better BibTeX. Better BibTeX est une extension de Zotero servant à
générer et à maintenir à jour des fichiers .bib compatibles avec
BibLaTeX, à partir de Zotero.

\hypertarget{zotero}{%
\subsection{Zotero}\label{zotero}}

\begin{itemize}
\item
  Installer \href{https://www.zotero.org/download/}{Zotero}
\item
  Installer \href{https://www.zotero.org/download/}{Zotero Connector}
\item
  Une fois Zotero installé, vous avez l'option de créer un
  \href{https://www.zotero.org/user/register/}{compte Zotero}.
  L'identifiant que vous utiliserez sera celui que vous partagerez à vos
  collaborateurs pour créer et joindre des groupes.
\end{itemize}

\hypertarget{better-bibtex}{%
\subsubsection{Better Bibtex}\label{better-bibtex}}

\begin{itemize}
\tightlist
\item
  La prochaine étape sera d'installer
  \href{https://retorque.re/zotero-better-bibtex/installation/}{Better
  BibTex}. Pour ce faire, allez dans l'onglet tools \textgreater{}
  Add-ons ensuite cliquez sur l'icone de paramètre et faites Install
  Add-on From File. Sélectionnez le fichier .xpi que vous avez
  téléchargé.
\end{itemize}

\emph{IMPORTANT}

\begin{itemize}
\item
  Une fois le module complémentaire installé, accédez aux paramètres de
  Better BibTeX en allant dans l'onglet Zotero \textgreater{}
  Préférences \textgreater{} Onglet Better BibTeX \textgreater{} Ouvrir
  les préférences de Better BibTeX.
\item
  Il est important, lors de la collaboration, de s'assurer d'avoir les
  mêmes clés de citation que vos collègues. Better BibTeX peut s'assurer
  que vos clés respectent un format standard.
\item
  Voici une suggestion de format de clé de citation : il s'agit
  simplement du nom de l'auteur et de l'année de publication à deux
  chiffres. Pour l'utiliser, collez ceci dans la section Format de clé
  de citation :
  \texttt{authEtal2.fold.lower.replace(find=".",replace=\_)\ +\ len\ +\ shortyear\ \textbar{}\ veryshorttitle\ +\ shortyear}
\item
  Afin de vous assurer d'avoir les mêmes clés de citation, vous pouvez
  faire un clic droit sur vos références, aller dans les options de
  Better BibTeX et cliquer sur ``Actualiser les clés de citation''
\end{itemize}

\hypertarget{guxe9nuxe9ration-du-fichier-.bib}{%
\subsubsection{Génération du fichier
.bib}\label{guxe9nuxe9ration-du-fichier-.bib}}

Dans Zotero, vous devriez maintenant voir le groupe Zotero de votre
équipe dans les Group Libraries.

\includegraphics[width=1.77083in,height=\textheight]{images/chapitre6_zotero.png}

\emph{Il est important de comprendre que tout changement que vous faites
dans Zotero sera automatiquement synchronisé avec le groupe de votre
équipe de travail. Ainsi, si vous supprimez une référence, elle sera
supprimée pour tout le monde!}

Clic-droit sur la collection livre-outils \textgreater{} Export
Collection choisissez le format Better BibLaTex et cochez la case
{[}x{]} Keep updated. Faites OK et sauvegardez le fichier dans le
dossier .git du projet livre-outils. Ce dossier sera constamment mis à
jour avec les changements que vous faites dans Zotero et sera
synchronisé avec le projet Github quand vous ferez vos pull requests.

\includegraphics[width=3.125in,height=\textheight]{images/chapitre6_biblatex.png}

\hypertarget{utilisation-de-zotero-lors-de-luxe9criture}{%
\subsubsection{Utilisation de Zotero lors de
l'écriture}\label{utilisation-de-zotero-lors-de-luxe9criture}}

Lors de l'écriture, vous n'avez qu'a écrire @ dans votre éditeur pour
faire sortir la palette de référencement.

\hypertarget{ajouter-des-ruxe9fuxe9rences-uxe0-zotero}{%
\subsubsection{Ajouter des références à
Zotero}\label{ajouter-des-ruxe9fuxe9rences-uxe0-zotero}}

Il y a différentes façons d'ajouter des références à Zotero :

\begin{itemize}
\tightlist
\item
  Glisser-déposer à partir de votre bibliothèque personnelle.
\item
  Glisser-déposer les PDF que vous avez sur votre ordinateur dans la
  collection Livres-Outils. Zotero va essayer de trouver les métadonnées
  automatiquement.
\item
  Si cela ne réussit pas, vous pourrez ajouter la référence en cliquant
  sur la baguette magique en haut à gauche du symbole ``+'' vert.
  L'outil de la baguette magique est utile si vous possédez le DOI ou
  l'ISBN de l'article/livre que vous devez ajouter. Dans les rares cas
  où Zotero ne trouve rien concernant votre référence, vous pourrez
  remplir les différents champs manuellement.
\item
  Utiliser le connecteur dans votre navigateur. Zotero tentera également
  de télécharger l'article directement et de l'inclure dans la
  collection appropriée.
\end{itemize}

\hypertarget{conclusion-3}{%
\section{Conclusion}\label{conclusion-3}}

Pour conclure, l'adoption de Zotero comme outil de gestion
bibliographique se révèle être un choix judicieux pour tout chercheur
soucieux de l'efficacité et de la rigueur dans le processus de
documentation scientifique. Au-delà de la simple facilitation du travail
de recherche en équipe, Zotero se distingue par sa capacité à optimiser
la gestion des citations et des bibliographies, permettant ainsi une
économie de temps considérable et une réduction des risques d'erreurs.
Sa fonctionnalité de centralisation des sources et de leurs fichiers
associés offre un avantage notable en termes d'organisation et d'accès
rapide à l'information, cruciale dans le cadre de recherches
approfondies ou pluridisciplinaires. L'intégration de Zotero dans les
environnements académiques, même en dehors des contextes de recherche,
comme l'enregistrement des lectures pour des cours ou des séminaires,
prépare efficacement les utilisateurs à des pratiques de recherche plus
poussées et renforce la culture de la gestion rigoureuse des références.
Cette initiation précoce est d'autant plus pertinente que Zotero se
prête à une variété de styles de citation, répondant ainsi aux exigences
diverses des publications académiques. Il est important de souligner que
la maîtrise de Zotero, bien que facilitée par de nombreux tutoriels et
ressources en ligne, représente un investissement en temps qui se trouve
largement compensé par les bénéfices en termes d'efficacité et de
qualité du travail de recherche. En outre, l'accès gratuit et le
caractère open-source de Zotero témoignent de son engagement en faveur
d'une diffusion élargie du savoir et d'une collaboration scientifique
ouverte.

\hypertarget{revue-systuxe9matique}{%
\subsection{Revue Systématique}\label{revue-systuxe9matique}}

une revue systématique est une synthèse méthodique et exhaustive de la
littérature centrée sur une question de recherche spécifique. Elle vise
à recenser l'ensemble des travaux académiques, publiés ou non, relatifs
à cette question Kibbee (2023) \textbf{kibbee23}. Comme mentionné
précédemment, une telle revue requiert un investissement conséquent en
temps et en effort, mais, lorsqu'elle est bien menée, elle présente un
grand potentiel scientifique.

La méthode a été choisie selon le processus décisionnel suggéré par
\textbackslash textcite{[}16{]} (\textbf{cloutier23?}). Elle répond aux
critères suivants: l'objectif est de recenser toutes les données
relatives au sujet, l'équipe de recherche compte plus de deux personnes,
le calendrier du projet prévoit plus de 12 mois de recherche, et
l'analyse porte sur une unique question de recherche.

La revue systématique sera menée en suivant les 24 étapes décrites par
(\textbf{muka\_etal20?}).

\begin{enumerate}

  \item Définir la question de recherche. 
  
  Dans le cadre de ce projet, la question de recherche est: \og{} Quels sont les éléments constitutifs d'une recherche en science politique? \fg{}.

  \item Établir l'équipe. 
  
    L'équipe sera composée de collègues universitaires souhaitant participer au projet et ayant une expérience dans le domaine d'étude.
  
  \item Définir la stratégie pour l'obtention des références. 
  
    Plusieurs bases de données seront sélectionnées pour effectuer la recherche, dont celle de la bibliothèque de l'université, Web of Science, Google Scholar, JSTOR, etc. L'assistance d'un bibliothécaire sera sollicitée pour optimiser la stratégie de recherche et affiner l'utilisation des mots-clés, comme suggéré par \textcite{cloutier23} et \textcite{dufour23}.

  \item Définir les critères d'inclusion et d'exclusion.
  
    Les critères de sélection permettront de circonscrire le nombre de références utilisées pour la revue. Ils pourront concerner un domaine d'étude, un choix de langues de publication ou une période donnée. Dans ce projet, les critères d'inclusion et d'exclusion seront à définir avec l'équipe.

  \item Définir le formulaire d'extraction des données.

    L'objectif de la revue systématique est d'extraire les données des textes analysés. Dans ce projet, il s'agit de construire une base de données composée d'éléments pertinents pour la recherche. Ainsi, les données extraites seront des métadonnées, telles que les sections, leur longueur, leurs titres, leur ordre, les méthodes utilisées, les définitions, les références, etc. Tous ces éléments sont pertinents pour structurer une recherche en science politique.

  \item Écrire le protocole de recherche servant à guider le processus.
  
    Le protocole de recherche est essentiel pour guider l'équipe tout au long du processus de filtrage. Avant de commencer la revue, \textcite[4]{muka_etal20} recommandent de consulter différents experts afin de valider le protocole, s'assurant ainsi de sa complétude et de l'absence d'éléments manquants.

  \item Appliquer la stratégie de recherche à diverses bases de données.

    Il faudra mettre en œuvre la stratégie de recherche sur les bases de données choisies et l'ajuster pour chaque base. Il sera important de bien documenter cette étape car elle servira à la preuve de concept d'automatisation de la revue systématique.

  \item Rassembler les références et les résumés.

    Les références seront rassemblées et intégrées dans une bibliothèque de groupe Zotero, un logiciel libre permettant de gérer les références bibliographiques.

  \item Éliminer les doublons.
  
    Zotero offre la possibilité d'éliminer les doublons automatiquement.

  \item Filtrer les résultats à partir des titres et résumés.

    À cette étape, deux membres de l'équipe de recherche effectueront le filtrage des résultats. Ils élimineront les références non pertinentes à la question de recherche en se basant sur le titre et le résumé. Les références non écartées par les deux évaluateurs seront conservées pour l'étape suivante.

  \item Comparer les résultats des codeurs.

    Il faudra conserver les références acceptées par au moins deux codeurs, écarter celles jugées non pertinentes par au moins deux d'entre eux et organiser une discussion concernant les références acceptées par un seul codeur pour juger de leur pertinence.

  \item Télécharger les textes entiers et appliquer les critères d'inclusion et d'exclusion
  
    Les références retenues seront téléchargées. Les textes difficiles d'accès seront recherchés manuellement. L'assistance d'un bibliothécaire pourrait être nécessaire à cette étape.

  \item Contacter les experts du champs et s'informer à propos des éléments manquants ou non-publiés

    Pour s'assurer qu'aucun élément pertinent n'a été omis, il est recommandé de consulter des experts du domaine.~\textcite[6]{muka_etal20} suggèrent de contacter chaque auteur des articles retenus à l'étape 12.

  \item Rechercher manuellement des références additionelles
  
    Il s'agira de réaliser une recherche boulle de neige (\textit{snowballing}) à partir de tous les articles retenus aux étapes 12 et 13. Cette méthode consiste à compiler chaque référence citée dans les articles sélectionnés et chaque article qui cite les articles retenus. Des outils tels qu'Elsevier Scopus peuvent faciliter cette démarche.

  \item Sélectionner les références à inclure et déssiner le diagramme de flux
  
    Les articles choisis lors des étapes 12, 13 et 14 constitueront ceux inclus dans la revue systématique. Un diagramme de flux sera réalisé pour documenter le processus de sélection: combien d'articles ont été examinés, combien ont été retenus, à quelle étape ont-ils été écartés, etc.

  \item Extraire les données des articles
  
    Les données des articles seront extraites. Il est crucial que chaque membre de l'équipe utilise le même formulaire d'extraction pour assurer la comparabilité des données.

  \item Évaluer la qualité des articles et les risques de biais
  
    L'échelle de notation Newcastle-Ottawa de \textcite{wells_etal00} sera utilisée pour évaluer la qualité des articles. Cette échelle fréquemment utilisée en médecine peut être utile en science politique. Elle permet de noter les articles sur la sélection de leurs cas, la comparabilité de leurs données, et la pertinence de leurs résultats. Chaque catégorie est notée sur 5 points, soit un total de 15 points. Les articles ayant obtenu plus de 9 points sont jugés de bonne qualité avec un faible risque de biais. Un score de 6 ou moins indique un risque de biais élevé, ce qui peut remettre en question la pertinence d'inclure l'article dans la revue.

  \item Préparer la base de donnée pour l'analyse.
  
    Les données de chaque article seront compilées dans une base de données, laquelle servira pour l'analyse des résultats. Le langage de programmation R pour la gestion de cette base de donnée sera privilégié pour sa facilité d'utilisation et sa popularité en science politique.

  \item Construire une synthèse descriptive des données.
  
    Une synthèse descriptive des données sera élaborée. Cela permettra de se familiariser avec elles, de les nettoyer et de les comprendre. Cette synthèse aidera à identifier les tendances et les éléments constitutifs de la recherche en science politique.

  \item Décider si une méta-analyse est appropriée.
  
    Selon \textcite[7]{muka_etal20}, les étapes 1 à 19 d'une revue systématique sont identiques à celles d'une méta-analyse. C'est à l'étape 20 que l'on décidera de la pertinence d'une méta-analyse. Si les données s'avèrent trop hétérogènes, leur comparaison et leur analyse pourraient s'avérer impossibles. Dans ce cas, une méta-analyse ne serait pas judicieuse. Dans le cadre de ce projet, les données récoltés seront probablement propices à une méta-analyse puisqu'il s'agira majoritairement de méta-données.

  \item Explorer l'hétérogénéité.
  
    Il s'agira d'examiner les différences entre les articles pour déterminer s'il est possible d'harmoniser les données de chaque étude, optimisant ainsi leur comparaison et leur analyse.

  \item Évaluer les biais.
  
    Il est essentiel de reconnaître et de traiter les biais. Un biais de publication serait particulièrement susceptible d'affecter une revue systématique en science politique.

  \item Évaluer la qualité des preuves et la confiance dans les résultats
  
    La qualité de la base de données sera évaluée pour déterminer si les données sont suffisamment fiables pour une méta-analyse ou une revue systématique. Si la qualité s'avère insuffisante, il pourrait être nécessaire de reprendre le processus depuis la première étape.

  \item Rédiger le rapport de la revue systématique
  
    Ce rapport final documentera le processus de peaufinage des GML utilisés lors de la deuxième étape. Les données collectées seront essentielles à la réalisation du projet.

\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{les-outils-de-collecte-de-donnuxe9es}{%
\chapter{Les outils de collecte de
données}\label{les-outils-de-collecte-de-donnuxe9es}}

La révolution numérique engendrée par l'émergence du Big Data représente
un important défi pour le monde des sciences sociales (Manovich, 2011;
Burrows et Savage, 2014). Elle constitue également une opportunité de
recherche enrichissante et innovante permettant une compréhension plus
accrue des phénomènes sociaux étudiés par la communauté scientifique
(Connelly et al., 2016). Cette meilleure compréhension est permise,
entre autres, par l'accès à des données massives concernant les trois
acteurs clés de la société démocratique: les citoyens, les médias et les
décideurs (Schroeder, 2014; Kramer, 2014). Si l'accès à ces données
représente un défi éthique et théorique, tel qu'explicité lors des
chapitres précédents, elle représente également un défi technique pour
les chercheurs.euses voulant exploiter le potentiel et les opportunités
offertes par les données massives (Burrows et Savage, 2014). Le chapitre
qui suit vise à offrir un portrait de certains outils de collecte de
données pouvant être exploités par les chercheurs.euses en sciences
sociales visant à tirer profit de la révolution numérique. À travers ce
chapitre, il sera question d'outils permettant de collecter des données
de sondages, des données médiatiques, de même qu'une panoplie de données
textuelles par le biais d'extracteurs. Ce chapitre offre donc un tour
d'horizon de certains outils de collecte de données à la disposition de
la communauté scientifique souhaitant entamer des recherches en sciences
sociales numériques.

\hypertarget{le-big-data-et-les-diffuxe9rents-acteurs-de-la-sociuxe9tuxe9}{%
\section{\texorpdfstring{\textbf{Le Big Data et les différents acteurs
de la
société~:}}{Le Big Data et les différents acteurs de la société~:}}\label{le-big-data-et-les-diffuxe9rents-acteurs-de-la-sociuxe9tuxe9}}

Le champ d'étude de la science politique repose sur l'étude de trois
types d'acteurs distincts ayant un impact sur la condition
socio-économique et politique d'une société~: les décideurs, les médias
et les citoyens. La recherche sur les décideurs comprend entre autres
l'analyse des politiques publiques, des partis politiques, de stratégies
électorales ou encore l'analyse de discours de politiciens ou
d'organisations. L'étude des médias repose largement sur le rôle des
médias dans la formation des priorités et des jugements des citoyens
quant aux enjeux politiques, de même que sur leur capacité d'influencer
l'agenda des politiciens. En ce qui concerne les citoyens, le champ
d'étude de l'opinion publique se consacre à l'analyse des comportements
et des attitudes politiques des individus. De plus, de nombreuses
recherches visant à comprendre le rôle des citoyens dans une société
démocratique portent sur l'influence de la société civile de même que
sur l'effet des mouvements sociaux.

Chacun de ces champs de recherches se voit confronté à une panoplie de
défis théoriques et techniques en lien avec l'émergence des données
massives. La révolution technologique permet une étude plus approfondie
des phénomènes auxquels sont confrontés les différents acteurs de la
société démocratique, en raison de l'importante quantité de données
accessible aux chercheurs.euses. Toutefois, la collecte de données
permettant de mener à terme de telles études peut s'avérer complexe.
Pour chacun des trois acteurs démocratiques énumérés précédemment, les
sections suivantes énumèrent et expliquent les capacités techniques
d'outils permettant aux chercheurs.euses d'accéder à des données
massives. Bien que d'autres outils existent et offrent des résultats
satisfaisants, les méthodes suivantes sont particulièrement pertinentes
dans une optique d'étude des sciences sociales numériques en raison de
leur capacités techniques de même que par la relative simplicité de leur
utilisation.

\hypertarget{section}{%
\section{}\label{section}}

\hypertarget{plateformes-de-sondages-et-collecte-de-donnuxe9es}{%
\section{Plateformes de sondages et collecte de
données}\label{plateformes-de-sondages-et-collecte-de-donnuxe9es}}

Malgré certaines différences méthodologiques, toute recherche doit
analyser et interpréter des données fiables et de qualité afin d'émettre
des résultats (Nayak \& K. A., 2019). Notamment lorsqu'il est question
d'étudier les citoyens et l'opinion publique, il est nécessaire
d'accumuler suffisamment de données auprès d'un échantillon assez grand
afin d'inférer des conclusions sur la population.

\hfill\break
Quelques méthodes sont couramment utilisées pour la collecte de données
sont le sondage, le panel, l'enquête, ou le questionnaire. Cette section
se concentre sur le sondage. Ils peuvent être manuels ou électroniques,
et dans le second cas, peuvent être administrés par ordinateur, par
courriel ou via le web (Nayak \& K. A., 2019). La différence majeure
entre les méthodes manuelles et les méthodes numériques réside dans le
fait que les premières impliquent un contact direct entre le chercheur
et le répondant, tandis que dans le cas des secondes le contact est
indirect (Evans \& Mathur, 2018). L'arrivée des données massives et des
outils numériques offre une panoplie de nouvelles opportunités de
collecte de données pour la communauté scientifique. Lorsqu'exécutée
manuellement, la collecte de données et la réalisation de sondages
peuvent devenir des tâches lourdement fastidieuses, et de facto,
demander énormément de ressources pour mener une recherche à grande
échelle. C'est pourquoi les technologies du numérique peuvent faciliter
cet aspect de la recherche en fournissant des plateformes de sondages et
de collecte de données. De plus, les sondages représentaient en 2016
environ 20\% du chiffre d'affaires de l'industrie globale du marketing
(Evans \& Mathur, 2018). Ces chiffres montrent la pertinence de
l'acquisition de compétences nécessaires à la formation de sondages,
tant dans le monde académique que professionnel. Le numérique permet
donc de créer un questionnaire, de cibler une population et de la
contacter, d'entreposer les données des répondants pour ainsi les
visualiser, le tout à un coût réduit et plus rapidement que s'il avait
été conduit manuellement (Nayak \& K. A., 2019). Ainsi, les sondages en
ligne ont une portée internationale, permettent le suivi de la ligne du
temps, offrent des options qui contraignent le répondant à répondre à
certaines questions et permettent d'utiliser des arbres de logique
avancés que les sondages manuels ne permettent pas.

\hypertarget{les-principales-plateformes-web.}{%
\subsection{Les principales plateformes
web.}\label{les-principales-plateformes-web.}}

\hfill\break
Il existe un large éventail de plateformes de sondages et de collecte de
données qui peuvent être utiles dans un contexte académique. Cet ouvrage
se limite à cinq d'entre elles: Qualtrics, REDCap, SurveyMonkey, Google
Forms et Typeform. Cependant, il n'est pas déconseillé de se renseigner
sur les autres plateformes disponibles en fonction de ses besoins et de
ses ressources. Voici une liste non-exhaustive: LimeSurvey, Zoho Survey,
Qualaroo, Formstack, Wufoo, Checkbox Survey, SmartSurvey,
QuickTapSurvey, SoGoSurvey, Snap Surveys, AskNicely, Opinio, Alchemer,
Cognito Forms, Feedbackify.

\hypertarget{qualtrics-httpswww.qualtrics.com}{%
\subsubsection{Qualtrics
(https://www.qualtrics.com/)}\label{qualtrics-httpswww.qualtrics.com}}

\hfill\break
Cette plateforme est une des plus reconnues et utilisées à
l'international, tant dans le milieu académique que dans le secteur
privé. En plus d'offrir des outils de collecte de données et de
sondages, Qualtrics est utilisé dans le marketing et dans la gestion de
l'expérience client. Il est donc pertinent de se familiariser avec cet
outil, car il offre des compétences pratiques pour la recherche, mais
également pour obtenir des opportunités de carrière. Qualtrics offre
plusieurs services pratiques pour la collecte de données, avec des
options flexibles pour la programmation et l'administration des
sondages. Par exemple, Qualtrics s'adapte à différents formats en
fonction de l'appareil du répondant (Evans \& Mathur, 2018).

\hypertarget{redcap-httpswww.project-redcap.org}{%
\subsubsection{REDCap
(https://www.project-redcap.org/)}\label{redcap-httpswww.project-redcap.org}}

\hfill\break
Research Electronic Data Capture (REDCap) permet de construire et de
gérer des sondages ainsi que des bases de données. Pour accéder aux
services de cette application, il est nécessaire d'être un partenaire du
REDCap Consortium ou membre d'une organisation qui en fait partie.
Seules les organisations à but non lucratif peuvent adhérer au
Consortium. Les données et les sondages qui y sont produits peuvent être
partagés et utilisés par différents chercheurs issus de diverses
institutions. L'exportation vers différents types de fichiers (Excel,
PDF, SPSS, SAS, Stata, R) est possible. Ce qui distingue REDCap des
autres applications est sa compatibilité avec les dossiers médicaux, sa
sécurité pour les données sensibles, ainsi que son approche académique à
la collecte de données par sondage.

\hypertarget{surveymonkey-httpswww.surveymonkey.com}{%
\subsubsection{SurveyMonkey
(https://www.surveymonkey.com/)}\label{surveymonkey-httpswww.surveymonkey.com}}

\hfill\break
SurveyMonkey se distingue des autres applications en permettant de
construire et gérer des sondages/formulaires à l'aide d'une interface
conviviale sans toutefois perdre de ses fonctionnalités. En plus d'avoir
recours aux nouvelles technologies de l'I.A. pour aider à construire des
sondages adaptés à vos besoins, cette application propose plusieurs
centaines de modèles personnalisables élaborés par des experts dans le
domaine. SurveyMonkey permet également l'analyse des données et la
création de rapports directement sur l'application, en plus de permettre
l'exportation vers d'autres types de programmes. Les forfaits varient en
gamme de tarifs, allant du gratuit avec des fonctionnalités restreintes,
jusqu'aux options payantes destinées aux particuliers et aux
entreprises.

\hypertarget{google-forms-httpsdocs.google.com}{%
\subsubsection{Google Forms
(https://docs.google.com/)}\label{google-forms-httpsdocs.google.com}}

\hfill\break
Cette application se distingue par sa simplicité et son accessibilité,
en grande partie grâce à l'omniprésence de google tant dans le monde
académique que dans la vie courante. Google Forms est inclus dans le
forfait de base du Google Workspace, ce qui le rend largement compatible
avec les autres applications de Google, en plus d'être disponible
gratuitement. Bien que ses fonctionnalités soient moins avancées que
celles de ses compétiteurs, Google Forms peut convenir pour des sondages
plus simples et rapides grâce à son interface conviviale, à sa fonction
d'analyse de données directement sur la plateforme, ainsi qu'à ses
modèles préfabriqués.

\hypertarget{typeform-httpswww.typeform.com}{%
\subsubsection{TypeForm
(https://www.typeform.com/)}\label{typeform-httpswww.typeform.com}}

\hfill\break
Si votre objectif est de produire des formulaires avec une esthétique
attrayante, moderne et interactive, TypeForm est la plateforme idéale.
Elle permet de se concentrer sur l'expérience de l'utilisateur et de
l'impliquer dans le sondage grâce à son aspect visuel. Cette plateforme
dispose d'une option gratuite, ainsi que plusieurs forfaits payants.
Typeform est également compatible avec plusieurs applications de gestion
du flux de travail (Zapier, Google Sheets, Slack, etc).

\hypertarget{les-limites-des-sondages-en-ligne}{%
\subsection{Les limites des sondages en
ligne}\label{les-limites-des-sondages-en-ligne}}

\hfill\break
Néanmoins, les sondages en ligne comportent des défis, notamment en ce
qui concerne l'échantillonnage, les taux de réponse et les
caractéristiques des non-répondants. Il est également nécessaire de se
méfier des enjeux éthiques et de confidentialité (Nayak \& K. A., 2019).
Comme la généralisation est essentielle pour conférer une valeur
scientifique à ses résultats de recherche, les sondages en ligne ont
leurs limites. En effet, il est crucial de connaitre la population cible
pour effectuer des inférences valides, et l'échantillonnage doit reposer
sur des caractéristiques précises. Même si des informations
démographiques peuvent être collectées et des quotas utilisés, il n'est
toutefois pas réellement possible de confirmer les informations sur le
répondant (Andrade, 2020). Les sondages traditionnels où l'on retrouve
un contact direct sont plus susceptibles de permettre de brosser un
portrait plus complet du répondant (Evans \& Mathur, 2018). Les
répondants avec des biais peuvent également plus facilement répondre aux
sondages en ligne et limiter la généralisation (Andrade, 2020). Les
sondages en ligne sont également souvent perçus comme des pourriels, ont
généralement de faibles taux de réponse, sont impersonnels et peuvent
avoir des instructions peu claires. Ils ont également leurs lots
d'enjeux de confidentialité (Evans \& Mathur, 2018).

\hfill\break
Conseils méthodologiques à la réalisation d'un sondage numérique\\
(Evans \& Mathur, 2018)\\
L'article de Evans et Mathur (2018) est une revue de littérature
observant l'évolution des sondages numériques depuis la parution de leur
dernier article sur le sujet en 2005. À travers cet article, les auteurs
offrent des conseils méthodologiques en fonction de leur analyse de
contenu de la littérature scientifique. Les conseils d'Evans et Mathur
(2018) sont résumés ci-dessous. Afin d'obtenir plus de détails,
n'hésitez pas à vous référer directement à cet article. De plus, bien
qu'il s'agisse d'un article crédible et largement documenté, il est
toujours pertinent de consulter des sources spécifiques à vos besoins.

\hfill\break
1.Définir le but du sondage avant la méthodologie. Lorsque possible,
inclure des hypothèses testables et des méthodes basées sur des
fondations théoriques.\\
2.Choisir le type de sondage.\\
3.Décider des méthodes d'échantillonnage, des quotas et des échéances.\\
4.Déterminer le responsable de la construction du sondage.\\
5.Soyez transparent en divulguant le but du sondage, la façon dont les
données seront utilisées ainsi que l'auteur du sondage.\\
6.Les questions et les catégories de réponses doivent être élaborées de
manière objective et dans une perspective de convivialité.\\
7.Les sondages doivent être assez légers pour favoriser un taux de
réponse positif, mais assez complet pour avoir l'information
nécessaire.\\
8.Ils doivent également être attrayant afin de favoriser leur complétion
par le répondant.\\
9.S'assurer de l'anonymat du répondant.\\
10.Il faut régulièrement procéder à des tests afin de corriger les
faiblesses du questionnaire.\\
11.Déterminer qui administre le sondage, qui collecte l'information, et
qui analyse les données.\\
12.Établir un échéancier pour les différentes étapes de l'étude.\\
13.Suite à la collecte de données, entreposer les données brutes dans un
fichier électronique.\\
14.Utiliser les méthodes appropriées (qualitatif ou quantitatif), et
analyser les données selon les buts de l'étude.\\
15.Dans le cas d'une recherche académique, il est important d'avoir une
section dédiée aux limites de l'étude.\\
16.Conserver l'anonymat des répondants lors de l'analyse et de la
publication.\\
17.Agir sur les résultats. Rien ne sert de conduire un sondage qui ne
contribue pas à la croissance du savoir ou n'apporte pas de changement
stratégique ou organisationnel.\\
18.Toujours se plier à un code d'éthique rigide.

Il s'agit donc ici d'un court résumé des plateformes de sondages et de
la collecte de données en ligne, tentant de couvrir l'essentiel de cet
outil afin de vous aider lors de votre parcours académique, ou
simplement comme aide-mémoire pour la réalisation d'un sondage. Les
outils énumérés précédemment permettent une étude approfondie de
phénomènes concernant les citoyens. Bien sûr, il n'est pas possible de
couvrir l'entièreté de cet outil très complexe et ayant évolué dans le
temps. Cette section ne sert donc que de point de départ si vous vous
intéressez à l'élaboration d'un sondage numérique. Il vous est donc
recommandé de vous renseigner davantage avec d'autres ressources afin de
compléter ce qui est indiqué dans cet ouvrage.

\hypertarget{factiva-outils-de-ruxe9colte-de-donnuxe9es-muxe9diatiques}{%
\section{\texorpdfstring{\textbf{Factiva~: outils de récolte de données
médiatiques}}{Factiva~: outils de récolte de données médiatiques}}\label{factiva-outils-de-ruxe9colte-de-donnuxe9es-muxe9diatiques}}

L'émergence de nouvelles technologies de même que la fragmentation
médiatique, causée notamment par l'apparition de chaînes de nouvelles en
continu, ébranlent considérablement les écosystèmes médiatiques
occidentaux (Chadwick, 2017). Un récent courant de recherche se penche
sur le rôle des médias relativement aux comportements des individus dans
une perspective de fragmentation médiatique. Ces changements de
dynamique médiatiques permettent aux individus de choisir leurs sources
d'information. Cette fragmentation aurait conséquemment pour effet de
contribuer à la formation de chambres d'écho. Ainsi, les études sur les
effets des médias visent à comparer les agendas de différentes
organisations médiatiques de même que de comprendre le cadrage de la
nouvelle qu'ils offrent aux citoyens. Pour effectuer de telles études
comparées, l'accès à des données médiatiques est essentiel. L'arrivée de
données massives permet de nouvelles avenues de recherche pour les
chercheurs.euses en sciences sociales en raison de l'importante quantité
de données accessibles aux chercheurs.euses, ce qui permet une
compréhension accrue des réalités médiatiques modernes.

L'outil Factiva offre un accès à l'ensemble des articles d'une panoplie
de médias provenant d'une vaste sélection de pays. Le moteur de
recherche est opéré par Dow Jones et offre également l'accès à des
documents d'entreprises. En revanche, l'accès qu'il offre aux contenus
médiatiques est particulièrement pertinent pour la communauté
scientifique en communication et en sciences sociales. Il offre l'accès
à plus de 15~000 sources médiatiques provenant de 120 pays. Il permet de
télécharger une quantité illimitée de documents RTF, un format de
fichier de texte, pouvant contenir jusqu'à 100 articles chacun. En
outre, ils peuvent être sélectionnés automatiquement en cochant le
bouton proposant de sélectionner les 100 articles de la page de
résultat. Chaque page de résultat contient 100 articles à la fois.
Enfin, Factiva permet également de filtrer les doublons.

Additionnellement, cet outil permet également de lancer une requête de
recherche par mots-clés et par date qui permet, par exemple, de récolter
les articles médiatiques concernant un sujet précis dans une ligne de
temps déterminée. De manière plus précise, Factiva permet de filtrer la
recherche d'articles par source, par date, par auteur, par sociétés, par
sujet, par secteur économique, par région et par langue. Disons qu'un.e
chercheur.euse désire comparer la couverture médiatique d'une élection
donnée. Il peut, par le biais de Factiva, sélectionner tous les articles
contenant le mot «~élection~» dans une sélection de médias, et ce,
durant la période de l'élection. Les mots clés sélectionnés peuvent être
adaptés aux désirs de l'utilisateur.ice de manière à inclure des mots
qui peuvent être mis ensemble ou à un maximum d'intervalle de mot.
L'utilisation des signes «~and~» et «~or~», aussi connus sous le nom
d'opérateurs booléens, permettent d'ajouter un mot dans la requête de
recherche. En ajoutant near5, l'on peut spécifier qu'il doit y avoir un
maximum de 5 mots entre les deux mots recherchés. L'on peut également
mettre certains signes à la fin de mots, ce qui permet de préciser le
champ de recherche. Par exemple, dans une étude récoltant des articles
sur les immigrants, le mot immigrant pourrait être écrit de la manière
suivante~: immigra*. Ainsi, tous les mots débutant par ce suffixe
seraient inclus de la recherche d'article, ce qui comprend donc~:
immigrant, immigration, immigrants, immigrante, etc. La Figure 1 est une
capture d'écran de l'interface de recherche de Factiva. Ainsi, en
ajoutant un opérateur booléen, l'on peut préciser un champ de recherche.
L'utilisateur.rice pourrait, par exemple, rechercher des articles sur
les immigrants syriens, et rajoutant les opérateurs ``and'' ou encore
``or'', de même que le mot « syri* », l'étoile étant rajoutée pour
inclure le plus de mots possible.

\begin{figure}

{\centering \includegraphics{images/chapitre3_factiva.png}

}

\caption{image3\_1}

\end{figure}

Ainsi, Factiva permet d'avoir accès facilement à des données utiles pour
de l'analyse textuelle d'articles médiatiques. Comme les textes
deviennent accessibles rapidement et simplement aux chercheurs.euses,
cet outil optimise considérablement l'analyse de contenu par thèmes ou
par ton.

Cependant, ce ne sont pas tous les médias qui sont accessibles sur
Factiva. Dans l'optique ou un média recherché n'est pas trouvable sur
Factiva, le logiciel Eureka représente une bonne alternative. Eureka se
concentre principalement sur les médias francophones (autant au Québec
qu'en Europe). La structure d'Eureka est similaire à celle de Factiva.
En effet, Eureka permet de filtrer des articles médiatiques par requête
de recherche adaptée à la source, la date ou encore l'auteur. Toutefois,
les requêtes de recherche doivent être formulées d'une manière quelque
peu différente. Elles doivent donc être adaptées au fonctionnement
d'Eureka. Les articles doivent être sélectionnés à la main, et peuvent
être téléchargés dans un document PDF pouvant contenir un maximum de 50
articles à la fois. La Figure 2 contient l'interface de recherche
d'Eureka.

\begin{figure}

{\centering \includegraphics{images/chapitre3_eureka.png}

}

\caption{image3\_2}

\end{figure}

Il existe aussi une panoplie d'outils permettant un accès à des données
médiatiques. Quoique Factiva soit intuitive et que de nombreuses
universités possèdent des licences permettant d'exploiter la plateforme,
plusieurs alternatives s'offrent à la communauté scientifique. NexisUni,
qui comprend entre autres l'outil LexisNexis Academic particulièrement
prisé par le champ d'études de communication aux États-Unis, représente
une excellente alternative. C'est également le cas de NewsBank qui
permet lui aussi un accès à un vaste répertoire d'articles médiatiques.
Les chercheurs.euses peuvent choisir la plateforme qui leur convient le
mieux, en prenant en compte notamment l'accès qui peut leur être fourni
par l'institution universitaire les employant.

En somme, la révolution numérique permet un accès sans précédent aux
données médiatiques, ce qui permet des analyses approfondies du rôle des
médias traditionnels dans une société démocratique.

\hypertarget{les-extracteurs-avoir-accuxe8s-uxe0-des-donnuxe9es-massives-via-du-code.}{%
\section{\texorpdfstring{\textbf{Les extracteurs~: avoir accès à des
données massives via du
code.}}{Les extracteurs~: avoir accès à des données massives via du code.}}\label{les-extracteurs-avoir-accuxe8s-uxe0-des-donnuxe9es-massives-via-du-code.}}

Chacun des acteurs démocratiques énumérés précédemment peut également
être étudié par le biais d'extracteurs qui offrent un accès à des
données numériques massives. Les extracteurs de données numériques sont
des infrastructures de code permettant d'extraire des données brutes
d'une source définie. La section suivante explique comment les
extracteurs peuvent être utiles dans un contexte de recherche en
sciences sociales numériques.

Les données en lien avec les décideurs sont souvent accessibles sur des
sites gouvernementaux. Toutefois, certaines identifications peuvent être
nécessaires et l'accès peut être compliqué, particulièrement dans une
perspective de données massives. C'est dans cette optique que les
extracteurs de données numériques peuvent être utiles. Un code peut
extraire de manière automatisée les débats des parlements, les
communiqués de presse des gouvernants, les plateformes électorales des
partis politiques, ce qui offre un accès inégalé aux chercheurs.euses
aux données de décideurs. Dans une autre optique, des extracteurs
peuvent également offrir l'accès aux données provenant de médias
socionumériques comme Twitter (maintenant X) ou Facebook . Un extracteur
peut, par exemple, être en mesure de répertorier l'ensemble des Tweets
de journalistes, de politiciens ou encore de citoyens de manière
automatisée, offrant un accès inégalé aux chercheurs.euses à des données
massives exclusives. L'élaboration d'extracteurs est toutefois facilitée
par l'existence d'API (Application programming interface) sur les
plateformes exploitées. L'API d'un site ou d'une application permet à un
tierce parti d'avoir accès à du code expliquant le fonctionnement de la
plateforme étudiée, ce qui en facilite l'extraction de données. Par
exemple, Twitter possédait avant les changements de directions récents
un API qui facilitait l'élaboration d'un extracteur. En contrepartie,
Facebook ne possède pas d'API, ce qui rend l'accès à ses données
beaucoup plus complexe. Un extracteur peut également offrir l'accès à
des données médiatiques, en codant un accès à des fils RSS ou encore aux
HTML des médias extraits.

L'élaboration d'un extracteur est toutefois une tâche complexe qui
requiert un certain nombre de connaissances en lien avec les langages de
programmation. Les chapitres 4 et 5 du présent ouvrage offrent justement
un survol du langage fonctionnel R, qui est utilisé par de nombreux
développeurs lors de l'écriture d'extracteurs. R est également reconnu
pour ces fonctionnalités statistiques qui sont, elles aussi, abordées
ultérieurement dans ce livre.

\hypertarget{covidence-outil-de-ruxe9colte-darticles-scientifiques}{%
\section{\texorpdfstring{\textbf{Covidence~: outil de récolte d'articles
scientifiques}}{Covidence~: outil de récolte d'articles scientifiques}}\label{covidence-outil-de-ruxe9colte-darticles-scientifiques}}

Comme mentionné précédemment, les outils numériques de données massives
facilitent le travail des personnes chercheuses lors de la récolte de
données dans le cadre d'analyses empiriques. Cependant, la révolution
technologique offre également des outils pouvant être utiles lors
d'autres étapes du cycle de la recherche. Il s'agit notamment du cas de
la revue de littérature, alors que de nombreux outils offrent aux
personnes chercheuses des ressources permettant d'élaborer un cadre
théorique exhaustif par le biais de données massives sur la littérature
scientifique. L'outil Covidence, géré par une compagnie sans but
lucratif, en est un exemple particulièrement prisé du monde académique
lors de l'entreprise de revues de littérature.

La plateforme en ligne Covidence est utilisée pour faciliter les revues
systématiques de littérature. Cette dernière permet de réduire
drastiquement le temps d'accomplissement du travail en plus de le rendre
plus simple et plus intuitif. L'outil a été développé pour mieux gérer
et organiser l'évaluation de quantité importante d'études scientifiques.
L'exécution d'une revue de littérature sur Covidence se fait par le
biais d'un double codage. C'est-à-dire que l'évaluation des études se
fait manuellement par deux codeurs travaillant de manière autonome et
qui mettront en commun leurs résultats à la fin de l'exercice. L'outil
est reconnu pour ses trois étapes précises : « Title and abstract
screening », « Full text review » et « Extraction ». Covidence permet
d'importer des données massives provenant de base de données
bibliographiques. En effet, l'outil lance des requêtes auprès de
multiples bibliothèques, ce qui offre l'accès à des milliers d'études
sur le champ étudié par les personnes chercheuses. Ces requêtes sont
adaptées aux besoins spécifiques de la personne chercheuse voulant
explorer en profondeur un domaine de la littérature scientifique.

La première étape, soit le «~Title and abstract screening », consiste en
la révision des titres et des résumés des articles récoltés. Pour rendre
le travail davantage efficace, il est nécessaire d'inclure des critères
précis pour analyser les titres et résumés d'articles. En se servant du
jugement et des critères qui étaient recherchés, les individus doivent
éliminer ou accepter selon la pertinence de l'article quant à la
littérature étudiée. Cette partie est souvent longue, puisque la
littérature existante est souvent massive. Il est donc important pour
les personnes chercheuses de se rencontrer à maintes reprises pour
discuter des conflits de jugement et pour trouver des compromis. En
outre, cette étape, plutôt longue, s'avère très utile et motivante,
puisqu'il est possible de développer un jugement critique davantage
raffiné et de s'instruire dans une littérature continuellement plus
précise.

Une fois avoir complété la revue des titres et des résumés, il faut
entamer le « Full text review » qui, comme l'indique le nom, consiste à
la révision complète des textes sélectionnés. Cette étape demande
d'analyser chaque texte, puis de voter « oui », « non » ou « peut-être »
quant à la conservation du texte dans la revue de littérature. Le vote
permet donc soit d'exclure l'article, de le retenir ou de l'envoyer à la
prochaine étape. D'un autre côté, les conflits rendent le travail
beaucoup plus long, puisque les codeurs.euses ont un texte entier à
argumenter. Ainsi, cette partie du travail, bien qu'elle comporte
beaucoup moins de documents, est assez longue et exigeante.

La dernière étape, soit celle de l'extraction, consiste à recueillir
toute donnée étant utile à l'étude de la littérature désignée. Cette
étape est demandante, car les chercheur.euse.s doivent se conformer à
une grille de codification prédéfinie. Le but est qu'un consensus entre
les codeurs émerge de ce processus. L'extraction permet de faire
ressortir les théories, les méthodologies et les conclusions présentent
dans les études retenues.

Une fois les étapes de la revue systématique terminées, Covidence
facilite l'exportation des résultats de l'extraction sous forme de
tableaux, de graphiques et de rapports pour la méta-analyse ou pour la
rédaction d'articles scientifiques. De nombreuses universités offrent un
accès à Covidence par le biais de licences, et l'outil est
particulièrement utile et bien construit. Toutefois, il existe d'autres
alternatives à Covidence. Le choix de l'outil dépend des coûts de même
que des besoins spécifiques des personnes chercheuses. Les plateformes
DistillerSR, Archie et Rayyan sont notamment largement utilisées par les
personnes chercheuses.

\hypertarget{conclusion-et-discussion}{%
\section{Conclusion et discussion:}\label{conclusion-et-discussion}}

Le précédent chapitre portait sur les différents outils de collecte de
données massives mis à la disposition des chercheur.euse.s s'intéressant
au champ des sciences sociales numériques. Les outils relevés se
démarquent par leur capacité d'accorder l'accès à des données permettant
d'étudier les trois principaux acteurs de la société démocratique, soit:
les citoyens, les décideurs et les médias. Comme mentionné à plusieurs
reprises lors du chapitre, le but de ce dernier n'est pas d'offrir une
liste complète des outils disponibles. Toutefois, les outils énumérés
ont été sélectionnés en raison de leur intuitivité, leur relative
simplicité d'accès de même que leurs capacités techniques considérées
par les auteurs comme étant particulièrement pertinentes dans une
optique de recherche en sciences sociales numérique. Ainsi, ce chapitre
démontre que la possibilité d'effectuer des recherches en sciences
sociales numériques par le biais de données massives est plus que jamais
accessible à la communauté scientifique, particulièrement en ce qui a
trait à la collecte de données permettant de tels travaux. Une fois les
données collectées, le travail d'analyse représente un défi technique
supplémentaire se dressant devant les personnes chercheuses. Les
chapitres suivants visent à familiariser les chercheurs.euses à des
outils méthodologiques permettant l'analyse et la visualisation de
données massives au sein des sciences sociales.

Bibliographie:

Schroeder, R. (2014). Big data and the brave new world of social media
research.~\emph{Big Data \& Society},~\emph{1}(2), 2053951714563194.

Chadwick, A. (2017). The hybrid media system: Politics and power. Oxford
University Press.

Connelly, R., Playford, C. J., Gayle, V., \& Dibben, C. (2016). The role
of administrative data in the big data revolution in social science
research.~\emph{Social science research},~\emph{59}, 1-12.

Manovich, L. (2011). Trending: The promises and the challenges of big
social data.~\emph{Debates in the digital humanities},~\emph{2}(1),
460-475.

Burrows, R., \& Savage, M. (2014). After the crisis? Big Data and the
methodological challenges of empirical sociology.~\emph{Big data \&
society},~\emph{1}(1), 2053951714540280.

Kramer, A. D., Guillory, J. E., \& Hancock, J. T. (2014). Experimental
evidence of massive-scale emotional contagion through social
networks.~\emph{Proceedings of the National academy of Sciences of the
United States of America},~\emph{111}(24), 8788.

Andrade, C. (2020). The Limitations of Online Surveys. Indian Journal of
Psychological Medicine, 42(6), 575-576.
https://doi.org/10.1177/0253717620957496

\hfill\break
Evans, J. R., \& Mathur, A. (2018). The value of online surveys: A look
back and a look ahead. Internet Research, 28(4), 854-887.
https://doi.org/10.1108/IntR-03-2018-0089

\hfill\break
Nayak, M., \& K A, N. (2019). Strengths and Weakness of Online Surveys.
24, 31-38. https://doi.org/10.9790/0837-2405053138

\bookmarksetup{startatroot}

\hypertarget{une-image-vaut-mille-mots}{%
\chapter{Une image vaut mille mots}\label{une-image-vaut-mille-mots}}

Camille Tremblay-Antoine\footnote{Université Laval} Nadjim
Fréchet\footnote{Université de Montréal}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

Une fois les données collectées, nettoyées, traitées et analysées, une
partie centrale du travail d'un scientifique de données est de faire
parler les résultats de ses tests empiriques. Il s'agit alors de trouver
la meilleure manière de rendre l'information digeste pour les experts et
initiés de votre discipline académique ou pour le grand public. La
visualisation graphique des données est donc centrale afin de vulgariser
les résultats d'une recherche empirique.

L'objectif de ce chapitre est d'apprendre aux codeurs débutants les
rudiments de la visualisation graphique en R. Ce chapitre présentera
plus particulièrement les packages R \emph{ggplot2} et \emph{dplyr}
eux-mêmes téléchargeable à partir du package \emph{tidyverse}. Si
\emph{dplyr} permet de préparer les données avant leur visualisation,
\emph{ggplot2} est un package dédié à la production de graphiques. Ce
chapitre présente sa grammaire avec une série d'exemples (Wickham 2009;
Wickham, Çetinkaya-Rundel, and Grolemund 2023).

Ce chapitre est plus technique que théorique et permet aux codeurs
débutants d'en apprendre davantage sur la manière de construire des
graphiques en R avec des données concrètes. La question centrale qui
devrait vous guider lorsque vous créez des visualisations est la
suivante: \textbf{Comment opimiser l'intelligibilité des données?}
L'objectif d'un graphique n'est pas seulement d'illustrer les données.
Un bon graphique devrait permettre de vulgariser une information ou de
mettre en saillance un aspect particulier des données. L'objectif
communicationnel devrait toujours être gardé en tête. Les graphiques en
exemple dans ce chapitre sont construits avec les données de l'Étude
Électorale Canadienne de 2019 qui sont facilement téléchargeables sur
leur site\footnote{http://www.ces-eec.ca/}.

La première section de ce chapitre expose les options et packages
également disponibles pour la construction de graphiques en R. La
deuxième section de ce chapitre compare les avantages et inconvénients
de l'utilisation de \emph{ggplot2} par rapport aux autres packages de
visualisation de données qui auront été présentés. La troisième section
de ce chapitre montre des exemples de graphiques construits avec la
grammaire de \emph{ggplot2} en utilisant les données de l'Étude
électorale canadienne de 2019. Les codes employés pour produire les
graphiques en exemple sont disponibles dans l'annexe de ce livre. Ces
codes reproductibles permettront aux codeurs débutants d'adapter ces
derniers pour leurs propres projets.

\hypertarget{ruxe9flexion-thuxe9orique}{%
\section{Réflexion théorique}\label{ruxe9flexion-thuxe9orique}}

\hypertarget{les-options-disponibles}{%
\subsection{Les options disponibles}\label{les-options-disponibles}}

De nombreux \emph{packages} ont été développés dans le langage R dans le
but de visualiser des données graphiquement, il devient donc facile de
s'y perdre. Heureusement, les options qui s'offrent à nous se précisent
lorsque l'on s'intéresse à ce qui est le plus utilisé dans la communauté
des codeurs de ce langage de programmation. Les \emph{packages} les plus
utilisés représentent des outils qui ont été substantiellement validés
et améliorés par leurs développeurs, mais aussi par une importante
communauté de codeurs en ligne et de chercheurs universitaires. Trois de
ces options sont présentées dans ce chapitre: les graphiques du
\emph{Base R}, le \emph{package} \emph{Lattice} et le \emph{package}
\emph{ggplot2}. Les avantages et inconvénients respectifs de ces trois
approches pour la création de graphiques sont explicités dans les
sections suivantes.

\hypertarget{avantages-et-inconvuxe9nients-de-base-r}{%
\subsubsection{Avantages et inconvénients de Base
R}\label{avantages-et-inconvuxe9nients-de-base-r}}

Le \emph{Base R} est le langage de base de R et il permet de faire de
nombreuses manipulations statistiques sans avoir à installer de
\emph{packages} au préalable. Le \emph{Base R} permet notamment de
produire des graphiques rapidement. Cela peut être utile pour visualiser
la distribution d'une variable ou pour regarder la relation entre deux
d'entre elles, par exemple. Pour produire un graphique avec le langage
de base R, il suffit de faire appel à la fonction \emph{plot()}. Avec la
fonction \emph{plot()}, le codeur peut visualiser la distribution d'une
variable seule en spécifiant l'axe des \emph{x} dans cette dernière. Le
codeur peut également visualiser la relation entre deux variables en
spécifiant à l'intérieur de la fonction celles qui composeront les axes
des \emph{x} et des \emph{y} du graphique. Les fonctions
\emph{barplot(), hist()} ou \emph{boxplot()} disponibles dans le
\emph{Base R} permettent de spécifier le style de graphique souhaité,
qu'on veuille représenter nos données sous forme de diagramme à barre,
d'histogramme ou de diagramme en boîtes (Kabacoff 2022, 119--32).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Exemple de graphique avec la fonction barplot() du BaseR}

\FunctionTok{barplot}\NormalTok{(y,}\AttributeTok{names.arg=}\NormalTok{x,}
 \AttributeTok{main=}\StringTok{"Figure 1 {-} Proportion (\%) de répondants par province}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
 \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
 \AttributeTok{sub=}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Source: Étude Électorale Canadienne de 2019                                                "}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Alors qu'un peu tout peut être fait avec le \emph{Base R}, ce langage
demeure élémentaire; il est difficile d'innover dans la visualisation ou
même de produire des graphiques plus sophistiqués. Le \emph{Base R} peut
sembler plus simple pour l'exploration de données ou pour produire des
graphiques de base rapidement, mais ce langage devient rapidement
complexe lorsqu'on cherche à améliorer l'esthétique de son graphique ou
à visualiser des relations entre plusieurs variables, ce que
\emph{lattice} et \emph{ggplot2} permettent plus facilement(Wickham
2009, 3--4).

\hypertarget{avantages-et-inconvuxe9nients-de-lattice}{%
\subsubsection{\texorpdfstring{Avantages et inconvénients de
\emph{lattice}}{Avantages et inconvénients de lattice}}\label{avantages-et-inconvuxe9nients-de-lattice}}

Développé par Deepayan Sarkar, \emph{lattice} cherche à faciliter la
visualisation de graphique en facettes. Plus précisément, ce
\emph{package} vise à améliorer les graphiques du \emph{Base R} en
fournissant de meilleures options de graphisme par défaut pour
visualiser des relations multivariées. Ce \emph{package} est donc
intéressant pour les chercheurs et les codeurs voulant présenter
graphiquement la relation entre plus de deux variables (Kabacoff 2022,
373--77; Sarkar 2008, 2023). Pour produire un graphique de base avec
\emph{Lattice}, le \emph{package lattice} doit préalablement être
installé dans la bibliothèque de \emph{packages} du codeur et chargé
dans sa session au début de son code (voir annexe). Par la suite, le
codeur doit spécifier le type de graphique souhaité avec la fonction
appropriée\footnote{Plusieurs options disponibles comme des histogrammes
  avec la fonction \emph{histogram()} ou des graphiques de densité avec
  la fonction \emph{densityplot()}.}. Une fois la fonction choisie, il
doit spécifier par une formule les variables x et y ainsi que la
troisième variable à contrôler et à visualiser en facettes
(\emph{graph\_type(formula \textbar{} variable en facettes, data=)}).

Si la Figure 1 produite à partir du \emph{Base R} nous permet de
visualiser le pourcentage de répondants par province dans l'Étude
Électorale Canadienne de 2019, le \emph{package lattice} nous permet de
visualiser facilement ce même pourcentage de répondants en tenant compte
du positionnement idéologique des Canadiens par province sur l'échelle
gauche-droite, comme l'illustre la Figure 2 (0 étant la gauche et 10 la
droite).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Exemple de graphique avec la fonction histogram() du package lattice}

\FunctionTok{histogram}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{gaucheDroite }\SpecialCharTok{|}\NormalTok{ province, }\AttributeTok{data =}\NormalTok{ GraphiqueLattice, }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }
  \AttributeTok{by =} \DecValTok{1}\NormalTok{), }
  \AttributeTok{main =} \StringTok{"Figure 2 {-} Distribution des Canadiens}\SpecialCharTok{\textbackslash{}n}\StringTok{ par province sur l\textquotesingle{}échelle gauche{-}droite}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Idéologie gauche{-}droite"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"Pourcentage (\%)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
  \AttributeTok{col  =} \StringTok{"blue"}\NormalTok{,}
  \AttributeTok{sub=}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Source: Étude Électorale Canadienne de 2019                                                "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Cependant, le \emph{package lattice} a pour désavantage d'avoir un
modèle formel (une grammaire de graphique) moins compréhensible et
intuitif que celui de \emph{ggplot2} lorsque vient le temps d'améliorer
l'esthétisme des graphiques. De plus, sa plus faible popularité fait en
sorte que ce \emph{package} demeure moins développé par la communauté de
codeurs de R que ne l'est \emph{ggplot2}. Nous examinons plus en détail
la grammaire de graphique de ce dernier \emph{package} ainsi que ses
avantages et inconvénients dans la prochaine section (Kabacoff 2022,
373--77 et 390; Wickham 2009, 6).

\hypertarget{avantages-et-inconvuxe9nients-de-ggplot2}{%
\subsubsection{Avantages et inconvénients de
ggplot2}\label{avantages-et-inconvuxe9nients-de-ggplot2}}

Développé principalement par Hadley Wickham, \emph{ggplot2} est un
\emph{package R} faisant partie de la collection de \emph{packages} de
\emph{tidyverse}. Ainsi, \emph{Ggplot2} peut être utilisé avec les
autres \emph{packages} centraux de \emph{tidyverse} ce qui limite de
potentiels conflits entre les fonctions de \emph{packages} qui puissent
être incompatibles avec \emph{ggplot2}. Par exemple, le \emph{package
dplyr} de \emph{tidyverse} est très utile pour analyser, organiser et
préparer vos données à visualiser avec \emph{ggplot2} (Wickham et al.
2019; Wickham, Çetinkaya-Rundel, and Grolemund 2023, 30).

Le principal avantage de \emph{ggplot2} reste sa grammaire qui permet à
l'utilisateur de rendre ses graphiques beaucoup plus visuellement
attrayants en facilitant la personnalisation esthétique. Ceci permet de
pousser l'esthétisme de vos graphiques à un très haut niveau par rapport
aux autres \emph{packages} de visualisation graphique disponibles en R.
Les graphiques \emph{ggplot2} se construisent couche par couche, soit
par l'ajout des différents éléments du graphique au fur et à mesure dans
le code du graphique à construire.

La première couche des graphiques \emph{ggplot} est généralement celle
des données et des variables à visualiser. Elle contient plusieurs
éléments fondamentaux qui sont essentiels à chaque graphique. Le premier
élément est la spécification de l'utilisation du \emph{package ggplot2}
qui se fait simplement en appelant la fonction \emph{ggplot2()}. Dans
cette fonction, il faut ensuite mentionner quelle est la base de données
(data=) ainsi que la fonction qui sera utilisée pour positionner les
données (aes(). Le positionnement le plus courant est de positionner des
données \emph{x} par rapport à des données \emph{y}, ce qui se fait de
la sorte: aes(x=, y=).

La deuxième couche des graphiques \emph{ggplot2} est celle du
\emph{geom}, qui spécifie le type de graphique souhaité. Les types de
graphiques les plus couramment utilisés avec \emph{ggplot2} sont les
nuages de points (\emph{geom\_point()}), les diagrammes de lignes de
tendances ou de séries chronologiques (\emph{geom\_line()}), les courbes
de densité (\emph{geom\_density()}) ainsi que les graphiques à bandes
(\emph{geom\_bar()}). Mais les possibilités sont infinies (ou presque!)
avec ggplot2 et bien plus de types de graphiques existent.

Les autres couches des graphiques \emph{ggplot2} dépendent souvent du
codeur et des étapes de construction de son graphique\footnote{Les
  étapes (couches) d'un graphique \emph{ggplot2} ne sont pas
  nécessairement dans le même ordre d'un graphique à un autre.} (Wickham
2009, 77 et 89-93). Le reste de ce chapitre présente la grammaire de
\emph{ggplot2} avec un exemple de construction de graphique à bande
présenté couche par couche.

\hypertarget{ruxe9flexion-muxe9thodologique}{%
\section{Réflexion
méthodologique}\label{ruxe9flexion-muxe9thodologique}}

\hypertarget{comment-utiliser-ggplot2}{%
\subsection{Comment utiliser ggplot2}\label{comment-utiliser-ggplot2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Première couche de l\textquotesingle{}exemple de graphique}
\CommentTok{\# ggplot2 (base de données, variables et geom)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{GraphiqueExemple, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{province, }\AttributeTok{y=}\NormalTok{prop)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Tel que mentionné dans le dernier paragraphe, la première étape est de
spécifier la base de données et les variables qu'on souhaite visualiser.
Vous vous souviendrez qu'au début de la section, nous avons mentionné la
collection \emph{tidyverse}, et plus spécifiquement le \emph{package
dplyr} qui y est compris. Ce dernier a été utilisé pour
nettoyer/calculer la proportion de répondants par province au préalable,
ce qui nous permet de positionner directement la variable \emph{prop}
dans l'axe y.

\hypertarget{exemples-et-fonctionnalituxe9s}{%
\subsection{Exemples et
fonctionnalités}\label{exemples-et-fonctionnalituxe9s}}

\hypertarget{trucs-et-astuces}{%
\section{Trucs et astuces}\label{trucs-et-astuces}}

\hypertarget{pour-aller-plus-loin}{%
\section{Pour aller plus loin}\label{pour-aller-plus-loin}}

\clearpage

\hypertarget{ruxe9fuxe9rences}{%
\section{Références}\label{ruxe9fuxe9rences}}

\bookmarksetup{startatroot}

\hypertarget{sec-chap7}{%
\chapter{Baliser les sciences sociales~: langages et
pratiques}\label{sec-chap7}}

Lorsque vous lisez un article scientifique, une page Web ou un
curriculum vitæ professionnel, vous vous doutez peut-être que le texte
n'est pas toujours produit à l'aide d'un logiciel de traitement de texte
comme Microsoft Word, Apple Pages ou LibreOffice Writer. La mise en page
complexe réglée au millimètre près, la qualité des figures et des
tableaux, l'utilisation de gabarits professionnels, le style des
références ou encore la présence d'éléments interactifs sont difficiles
et parfois impossibles à reproduire à l'aide d'un logiciel de traitement
de texte régulier. L'ajout d'extraits de code, de tableaux de régression
ou encore de figures de haute qualité graphique, ainsi que leur
personnalisation, nécessitent une interface particulière.

Pour ces raisons et plusieurs autres, les chercheurs en sciences
sociales font souvent appel aux langages de balisage, ou \emph{markup
languages}. Ceux-ci permettent de produire des documents et pages Web
sans les limitations des logiciels de traitement de texte. Le présent
livre, par exemple, est écrit à l'aide du langage de balisage Markdown
avec l'aide du système de publication Quarto. D'entrée de jeu, vous vous
demandez peut-être quelle est l'utilité d'apprendre ces langages alors
que les logiciels de traitement de texte sont nombreux, simples
d'approche et en amélioration constante. Ce chapitre n'a pas pour
objectif de décourager l'utilisation de ces logiciels, qui sont utiles
et même souvent essentiels pour la production rapide de documents ainsi
que pour des tâches de suivi des modifications et de travail avec des
équipes multidisciplinaires. Le chapitre tentera plutôt de montrer que
de savoir utiliser les langages de balisage est un atout pour les
équipes de recherche qui souhaitent élaborer des documents complexes et
professionnels. Il s'agira de répondre, tour à tour, aux trois grandes
questions suivantes~: \emph{Qu'est-ce qu'un langage de balisage? Quand
et pourquoi utiliser un langage de balisage? Comment utiliser un langage
de balisage?} L'accent sera mis sur Quarto ainsi que sur les langages
Markdown et \LaTeX, bien que d'autres langages soient aussi abordés.

\hypertarget{quest-ce-quun-langage-de-balisage}{%
\section{Qu'est-ce qu'un langage de
balisage?}\label{quest-ce-quun-langage-de-balisage}}

Un langage de balisage constitue un ensemble de commandes qui peuvent
être entremêlées à du texte afin de produire une action informatique.
Chaque langage contient son propre ensemble de commandes cohérentes et
complémentaires. De manière plus formelle, ces commandes sont nommées
\emph{balises} (\emph{tags} en anglais) et inscrites par le chercheur ou
la chercheuse au travers du texte. Les balises constituent une manière
de communiquer avec le logiciel utilisé dans un langage qu'il peut
comprendre. Par exemple, une balise permet d'indiquer au logiciel que
vous désirez qu'une section du texte soit écrite en caractères gras, en
italique, à double interligne ou encore que vous souhaitez positionner
une image d'une certaine manière au travers du texte. Cette interaction
est rendue possible par la standardisation des langages de balisage~:
chaque balise correspond à une action précise, peu importe le logiciel
utilisé, la langue dans laquelle le texte est rédigé, le type
d'ordinateur utilisé, etc. Dans votre document source, les balises sont
entremêlées au contenu de votre document. Au moment de compiler ce
dernier, les balises produisent les actions informatisées qu'elles
commandent et laissent comme document final le contenu mis en page tel
que vous l'avez défini via les balises utilisées. La compilation est le
processus par lequel un document écrit en langage de balisage est
transformé en fichier textuel, en format PDF dans le cas de \LaTeX~par
exemple. La Figure~\ref{fig-vscode} montre un exemple d'utilisation du
langage de balisage Markdown dans un fichier Quarto sur la plateforme
Visual Studio Code. L'écran à droite de l'image montre le fichier PDF
résultant du formatage réalisé dans la partie centrale de l'écran. Les
balises utilisées sont décrites plus tard dans ce chapitre.

\begin{figure}

{\centering \includegraphics[width=4.8in,height=\textheight]{images/chapitre5_vscode.png}

}

\caption{\label{fig-vscode}Exemple d'utilisation du langage de balisage
Markdown dans un fichier Quarto sur la plateforme VS Code
\newline \textit{Source}~: Auteurs du présent chapitre.}

\end{figure}

Le premier langage de balisage, le Generalized Markup Language (GML), a
été inventé en 1969 par les chercheurs Charles F. Goldfarb, Ed Mosher et
Ray Lorie pour la compagnie IBM. Goldfarb et ses collègues devaient
intégrer trois applications créées avec des langages différents et avec
une logique différente pour les besoins d'un bureau de droit. Même après
avoir créé un programme qui permettait aux trois applications
d'interagir, ces langages demeuraient différents et avaient chacun leur
propre fonctionnement. Le développement de GML a permis de résoudre ce
problème en standardisant et en structurant le langage~: les mêmes
commandes étaient utilisées pour accomplir les mêmes tâches dans chaque
programme (Goldfarb 1996). GML a été amélioré durant les décennies
suivantes et a été suivi par d'autres langages de balisage, dont
\LaTeX~(1985), \textsc{Bib}\TeX~(1988), HTML (1993), XML (1998),
Markdown (2004) et \texttt{R} Markdown (2012) (Encyclopaedia Britannica
2023; Hameed 2023; Markdown Guide 2023; World Wide Web Consortium (W3C)
1998; Xie 2023).

Les langages de balisage permettent d'effectuer différentes tâches.
HTML, qui est sans doute le plus connu des langages de balisage, permet
de formater des sites Web. XML, quant à lui, permet de structurer de
larges volumes de données. \LaTeX~permet pour sa part de formater du
texte et de créer des documents en format PDF. Markdown permet également
de créer des documents en format PDF, mais aussi en format HTML ou DOCX
--- format utilisé pour les documents Word ---, contrairement à \LaTeX.
\texttt{R} Markdown permet d'ajouter des extraits de code \texttt{R} à
un fichier en langage Markdown. Enfin, depuis 2022, le système de
publication scientifique et technique multilingue Quarto permet de créer
des documents qui intègrent des extraits de code \texttt{R}, \LaTeX,
Python, Julia ou JavaScript, créés dans différents types
d'environnements, à un fichier en langage Markdown (Allaire 2022).
\LaTeX, Markdown, \texttt{R} Markdown et Quarto permettent aussi
d'intégrer les références bibliographiques du système de traitement de
références \textsc{Bib}\TeX. Les langages de balisage communiquent ainsi
souvent les uns avec les autres au sein d'un même fichier. Le chapitre 6
explique la manière de citer les références en langage
\textsc{Bib}\TeX~par le biais de Zotero et de Better \textsc{Bib}\TeX.

Les balises constituent une manière de donner manuellement des commandes
au logiciel que vous utilisez. Si vous utilisez Microsoft Word, vous
avez accès à une panoplie de boutons qui vous permettent de formater
votre texte. Les balises exercent les mêmes fonctions de formatage pour
les fichiers produits en \LaTeX~ou en Markdown, mais doivent être
ajoutées à l'écrit par l'utilisateur. Lorsque vous appuyez sur un bouton
ou utilisez une commande comme \texttt{Ctrl-G} ou \texttt{Cmd-I} dans
Word, en réalité, cette commande ajoute des balises au travers de votre
texte, mais rend celles-ci invisibles dans l'interface que vous
utilisez. Cela permet d'avoir un texte élégant et facile à lire, mais
comporte aussi plusieurs inconvénients. Le principal inconvénient est de
limiter le pouvoir que vous avez sur le formatage de votre texte. En
effet, si les boutons à votre disposition ne vous permettent pas de
réaliser une opération, celle-ci sera éternellement impossible à
réaliser pour vous. A contrario, les langages de balisage permettent un
contrôle presque infini sur les opérations que vous souhaitez réaliser.
Incidemment, dans la mesure où vous utilisez le langage approprié pour
la tâche que vous souhaitez accomplir, vous devriez être capable de
donner exactement la commande nécessaire à votre logiciel. Les langages
de balisage, bien qu'ils aient un coût d'apprentissage qui peut s'avérer
important et que l'interface de travail soit moins intuitive qu'un
document Word, vous offrent une plus grande flexibilité.

Afin d'utiliser un langage de balisage, il est impératif que le logiciel
que vous utilisez puisse prendre en compte ce langage. Un logiciel
permet rarement d'utiliser n'importe quel langage. Par exemple, le
logiciel \TeX{}Shop permet seulement d'utiliser le langage \LaTeX. Il
est aussi impératif de bien utiliser le langage de balisage. En effet,
comme pour les langages de programmation, les langages de balisage ne
peuvent pas déduire ce que vous souhaitez leur faire comprendre. Si vous
souhaitez mettre du texte en gras, vous devez utiliser les bonnes
balises. La moindre erreur peut être coûteuse, puisqu'une erreur dans la
balise que vous utilisez risque de produire une commande
incompréhensible et un message d'erreur, le logiciel ne réussissant pas
à associer votre balise mal inscrite à une action informatisée.
Conséquemment, il est impératif de bien vérifier les balises utilisées
afin d'éviter toute erreur qui empêcherait votre document d'être
compilé, c'est-à-dire d'être traduit dans son format final\footnote{Les
  logiciels permettent plus ou moins efficacement d'identifier les
  balises problématiques. Certains ne produisent qu'un message d'erreur
  sans donner d'indication sur la source du problème, alors que d'autres
  ciblent très spécifiquement la ligne de syntaxe où se situe la balise
  problématique.}. Chaque caractère dans une balise est important et il
y a rarement plus d'une seule manière de commander une action. Par
exemple, en \LaTeX, il n'y a qu'une seule manière de mettre du texte en
gras. Il faut précisément utiliser cette commande:
\texttt{\textbackslash{}textbf\{\}}. Le positionnement des balises est
lui aussi critique~: il délimite la portion de texte à laquelle doit
être appliquée l'action commandée par la balise.

Il est important de distinguer les langages de balisage des langages de
programmation, qui sont abordés plus en détail dans le chapitre 4. En
effet, ceux-ci sont similaires à certains égards, mais ont des vocations
différentes. Les deux s'appuient sur un langage informatisé, mais les
langages et leurs objectifs diffèrent. Un langage de programmation
définit des processus informatisés alors qu'un langage de balisage
permet d'encoder du contenu de manière à ce que celui-ci soit lisible
tant pour l'humain que pour son ordinateur.

Dans le contexte de la recherche en sciences sociales, la programmation
est généralement utilisée afin de récolter, d'analyser et de présenter
visuellement des données. Une fois cartes, tableaux et graphiques
produits, ceux-ci peuvent être enregistrés --- par exemple en format PDF
ou PNG --- et inclus au sein d'un document qui sera formaté en utilisant
un langage de balisage. En \texttt{R} Markdown et en Quarto, des
extraits de langage de programmation peuvent être inclus dans des
sections bien délimitées de documents écrits en langage de balisage.
Plus généralement, le langage de programmation contribue à l'analyse
alors que le langage de balisage est essentiellement utile afin de
présenter les travaux de recherche, que ce soit dans un document écrit
ou sur un site Web. C'est principalement de cette manière que sont
utilisés les langages de programmation et de balisage dans le cadre de
la recherche en sciences sociales.

\hypertarget{comment-utiliser-un-langage-de-balisage}{%
\section{Comment utiliser un langage de
balisage?}\label{comment-utiliser-un-langage-de-balisage}}

En pratique, comment utilise-t-on Markdown, \LaTeX~et \textsc{Bib}\TeX?
D'emblée, \LaTeX~a une syntaxe particulière qui demande un certain temps
d'adaptation. Pour écrire une phrase simple comme celle-ci, la phrase
peut être écrite telle quelle. Par contre, pour mettre un \textbf{mot}
en caractères gras, il faut utiliser la balise suivante:
\texttt{\textbackslash{}textbf\{mot\}}. Pour mettre le
\textcolor{red}{mot} en rouge, la balise est
\texttt{\textbackslash{}textcolor\{red\}\{mot\}}. Pour le mettre en
italique et en note de bas de page\footnote{\emph{mot}}, les balises
\texttt{\textbackslash{}footnote\{\textbackslash{}emph\{mot\}\}} peuvent
être utilisées. Ainsi, des balises peuvent contenir d'autres balises. En
langage \LaTeX, une balise commence toujours par une barre oblique
inversée. Par la suite, le nom de la fonction --- \emph{emph},
\emph{textbf}, \emph{textcolor}, etc. --- est appelé. Enfin,
généralement, le mot à formater est placé entre accolades
(\texttt{\{\}}).

Chaque document \LaTeX{} commence par un préambule. Celui-ci présente
des informations telles que la taille des caractères, le type de
document, le format de mise en page, la police de caractères,
l'utilisation d'en-têtes et de pieds de page, ainsi que l'utilisation de
\emph{packages} \LaTeX~permettant différentes fonctionnalités de
personnalisation du document. Il n'est pas nécessaire ni souhaitable
d'apprendre l'ensemble des fonctions et des \emph{packages} \LaTeX~qui
existent. Au contraire, il est souvent mieux de commencer par un gabarit
de document qui convient au type de document que vous voulez créer et
ensuite de rechercher en anglais sur Stack Overflow la manière d'ajouter
des éléments de formatage que vous ne connaissez pas, par exemple en
recherchant \texttt{highlight\ latex\ text}.

Markdown fonctionne de manière similaire à \LaTeX, mais se démarque par
sa plus grande flexibilité et sa syntaxe beaucoup plus légère. Par
contre, il nécessite parfois l'utilisation de balises \LaTeX~afin de
réaliser certaines tâches, comme changer la couleur du texte. Tout
document Markdown débute avec un court bloc de syntaxe \texttt{YAML}
(acronyme de \texttt{Yet\ Another\ Markup\ Language}) qui définit les
paramètres généraux du document. Voici un bloc \texttt{YAML} typique
pour un document Quarto~:

\begin{verbatim}
---
title: "Baliser les sciences sociales"
subtitle: "Langages et pratiques"
date: today
author:
  - Alexandre Fortier-Chouinard^[University of Toronto]
  - Maxime Blanchard^[McGill University]
  - Étienne Proulx^[McGill University]
format: pdf
toc: true
date-format: "MMMM D, YYYY"
bibliography: references.bib
---
\end{verbatim}

Outre le titre, le sous-titre et le nom des auteurs, on trouve aussi
dans l'en-tête YAML la présence d'une table des matières (\texttt{toc}),
la date et son format, le format du document compilé --- dans ce cas-ci,
PDF --- ainsi que le chemin d'arborescence afin d'accéder au document
\textsc{Bib}\TeX~où sont enregistrées les références utilisées. Il est
aussi possible d'y définir la taille de la police de caractères ou
encore le gabarit Word servant à définir le format d'un document DOCX à
produire. De manière particulièrement importante, c'est l'endroit où
sont chargés les \emph{packages} \LaTeX~qui seront utilisés. En effet,
la majorité des \emph{packages} et fonctions \LaTeX~sont utilisables
dans Markdown, alors que l'inverse n'est pas vrai. Il est donc possible
de personnaliser un document Markdown en utilisant des \emph{packages}
ayant été créés pour \LaTeX.

La syntaxe à utiliser au travers du texte est somme toute plutôt simple.
Pour mettre un ou plusieurs \textbf{mots en gras}, il suffit de les
entourer de deux astérisques (\texttt{**mots\ en\ gras**}); pour les
mettre \emph{en italique}, il faut les encadrer d'une seule astérisque
(\texttt{*en\ italique*}). Pour définir un titre de section ou de
sous-section, il suffit de mettre des \texttt{\#} devant le titre en
question. Plus vous ajoutez de \texttt{\#}, plus le titre sera petit et
plus il sera considéré à un niveau hiérarchique inférieur dans la
structure du texte. La syntaxe Markdown est donc plus légère que celle
de \LaTeX, dans le but d'en rendre la lecture plus simple pour les
utilisateurs et utilisatrices.

Bien que des gabarits Markdown soient disponibles, ceux-ci sont plus
rares. Ils se trouvent pour la plupart sur GitHub et sont rendus
disponibles par leur créateur. Cela étant dit, leur personnalisation
peut s'avérer plutôt complexe. En somme, Markdown est particulièrement
pratique pour les documents ne nécessitant pas de respecter un gabarit
précis et requérant simplement un document d'allure simple et
professionnelle.

Pour sa part, \textsc{Bib}\TeX~a une syntaxe relativement simple.
D'emblée, les références \textsc{Bib}\TeX~pour des articles et ouvrages
scientifiques sont disponibles sur Google Scholar. Toutefois, pour citer
des sites Web ou des articles de médias, la référence doit être écrite à
la main selon un format précis. Une bibliographie sur
\textsc{Bib}\TeX~peut ressembler à ceci~:

\begin{verbatim}
@book{darwin03,
  address = {London},
  author = {Darwin, Charles},
  publisher = {John Murray},
  title = {{On the Origin of Species by Means of Natural Selection
or the Preservation of Favoured Races in the Struggle for Life}},
  year = {1859}
}

@article{goldfarb96,
  title={The Roots of SGML: A Personal Recollection},
  author={Goldfarb, Charles F},
  journal={Technical communication},
  volume={46},
  number={1},
  pages={75},
  year={1999},
  publisher={Society for Technical Communication}
}
\end{verbatim}

Un fichier \textsc{Bib}\TeX~ne contient rien de plus qu'une série de
publications commençant chacune par la balise \texttt{@} suivie du type
d'article --- \emph{article}, \emph{book} pour un livre,
\emph{incollection} pour un chapitre de livre, \emph{inproceedings} pour
une présentation dans une conférence, \emph{unpublished} pour un article
non publié et \emph{online} pour un site Web sont parmi les plus connus
--- et des informations sur la publication mises entre accolades. La
première information entre accolades est le code de la référence, par
exemple \texttt{goldfarb96}. Dans le fichier \LaTeX, l'auteur doit
écrire \texttt{\textbackslash{}cite\{goldfarb96\}} pour voir dans le
document PDF compilé Goldfarb (1996); le lien est automatiquement
cliquable et renvoie à la notice bibliographique correspondante. L'ordre
des publications dans le document \textsc{Bib}\TeX~a peu d'importance,
puisque \LaTeX~réordonne par défaut la bibliographie en ordre
alphabétique.

\hypertarget{environnements-duxe9dition-et-de-compilation}{%
\subsection{Environnements d'édition et de
compilation}\label{environnements-duxe9dition-et-de-compilation}}

Contrairement à Microsoft Word et Apple Pages, il existe plusieurs
options d'environnements d'édition et de compilation spécifiques à
chaque langage. Ces environnements sont des plateformes et des logiciels
conçus pour faciliter l'édition, la mise en forme et la compilation de
documents dans des langages de balisage tels que \LaTeX~et Markdown. Ils
permettent également de rendre plus efficace et conviviale la production
de documents tout en fournissant des fonctionnalités spécifiques aux
besoins de chaque langage. Il existe une grande diversité
d'environnements d'édition et de compilation, et le choix est libre pour
la chercheuse ou le chercheur de trouver celui qui convient le mieux à
ses besoins ou aux besoins de son groupe de recherche. Les trois options
discutées ici sont parmi les plus utilisées par les chercheurs en
sciences sociales et peuvent être regroupées en deux catégories~: les
logiciels de bureau et les éditeurs en ligne.

D'abord, il existe plusieurs logiciels de bureau qui offrent un
environnement d'édition et/ou de compilation pour les langages de
balisage. Ces logiciels fournissent les programmes principaux, les
extensions essentielles et des outils complémentaires de compilation et
de visualisation afin de permettre la production de documents écrits en
langages de balisage. Le logiciel RStudio, également abordé dans le
chapitre 4, permet de produire des documents avec différents langages de
balisage et programmation, ainsi que de naviguer entre eux, à partir
d'une même fenêtre. Il suffit d'installer certains \emph{packages}
contenant les fichiers nécessaires à l'utilisation des langages de
balisage. Par exemple, il est possible de produire des documents en
\LaTeX~en utilisant le code suivant dans la console pour installer le
\emph{package} nécessaire à l'utilisation de la distribution
\LaTeX~Tiny\TeX~: \texttt{install.packages("tinytex")}. Suivant le même
principe, il est possible de produire des documents en \texttt{R}
Markdown sur RStudio en installant le \emph{package} suivant~:
\texttt{install.packages("rmarkdown")}. Pour Quarto, le téléchargement
se fait en ligne, directement à partir du site Web de Quarto (2023).

Pour l'écriture en \LaTeX, il est également nécessaire d'installer l'une
des nombreuses distributions en ligne afin de pouvoir compiler ces
documents dans un environnement local. Il existe des distributions
telles que Mac\TeX~pour Mac, Mik\TeX~pour Windows et plusieurs autres
(Just 2013). Ces distributions se distinguent par les différents
\emph{packages} avec lesquelles elles sont compatibles.

Un autre environnement régulièrement utilisé pour travailler en langage
de balisage est le logiciel de bureau VS Code. VS Code prend en compte
un plus grand nombre de langages de programmation et est utilisé par les
programmeurs de tous domaines, tandis qu'RStudio est surtout utile pour
les chercheurs en sciences sociales qui travaillent surtout en
\texttt{R}.

Lorsque vient le temps de collaborer à plusieurs sur un document écrit
en Markdown ou en \LaTeX, les logiciels de bureau évoqués précédemment
nécessitent l'utilisation de GitHub et de Git. L'utilisation de ces
éditeurs peut présenter un défi supplémentaire pour les équipes de
recherche non initiées. Il existe ainsi des éditeurs en ligne qui
permettent de collaborer en temps réel sans passer par Git et GitHub, de
manière similaire à Google Docs\footnote{VS Code possède également une
  extension, Live Share, qui permet de travailler en temps réel sur un
  même document.}. Le plus connu de ces logiciels est Overleaf, qui
permet de produire des documents en langage \LaTeX. Puisqu'Overleaf
permet d'avoir accès à ses documents \LaTeX~à partir de n'importe quel
navigateur, il n'y a pas de dépendance à un logiciel local sur un
ordinateur, ce qui constitue un avantage important. La contrepartie de
cet avantage est qu'en utilisant Overleaf, l'équipe de recherche est
dépendante d'une connexion à Internet. En utilisant le package
\LaTeX~\texttt{rmarkdown}, Overleaf peut également inclure du code
Markdown. Cependant, Overleaf ne permet pas de créer des documents en
format DOCX ou HTML, ce qui constitue une limite de l'application.
Overleaf comporte un compteur de mots intégré, ce qui n'est pas le cas
des autres logiciels et environnements présentés plus haut.

\hypertarget{quand-et-pourquoi-utiliser-un-langage-de-balisage}{%
\section{Quand et pourquoi utiliser un langage de
balisage?}\label{quand-et-pourquoi-utiliser-un-langage-de-balisage}}

La plupart des langages de balisage permettent de remplir l'une des deux
fonctions suivantes, qui sont particulièrement importantes dans le
contexte de la recherche en sciences sociales~: produire des documents
écrits et formater des pages Web. Dans les deux cas, ces actions peuvent
être réalisées à partir de logiciels simples, mais ces logiciels ont des
limites importantes auxquelles les langages de balisage apportent des
solutions\footnote{Les langages de balisage permettent également de
  créer des pages Web. Bien que les pages Web puissent être créées à
  partir de sites Web comme WordPress, le langage HTML permet de
  produire des résultats plus \emph{personnalisables}, plus
  \emph{automatisables} et avec une plus \emph{grande qualité
  graphique}. Cette question n'est pas abordée en détail dans ce
  chapitre.}.

Pour l'écriture de documents très simples comme une liste d'épicerie ou
des notes rapides pendant une conférence, les logiciels de traitement de
texte sont tout à fait convenables~: ils sont simples et rapides à
utiliser, un formatage professionnel du document n'est pas de mise.
Utiliser un langage de balisage pour des tâches de base peut en effet
rendre la tâche inutilement longue et complexe. Toutefois, plus la
complexité d'un document augmente, plus il devient difficile d'obtenir
un résultat satisfaisant en utilisant un logiciel de traitement de texte
tel que Word, Pages ou Writer. A contrario, \LaTeX~permet de produire
des documents de tous les niveaux de complexité, tel que démontré sur la
Figure~\ref{fig-latex-vs-word}. Quant à Markdown, sa courbe
d'apprentissage se situerait logiquement entre celles de \LaTeX~et de
Word, puisque ses balises sont simplifiées. Plus généralement, utiliser
un langage de balisage comme \LaTeX~ou Markdown\footnote{Les avantages
  et désavantages de Markdown cités dans cette section s'appliquent
  également à Quarto et à \texttt{R} Markdown, puisque ces derniers font
  appel au langage Markdown.} comporte plusieurs avantages par rapport
aux logiciels de traitement de texte traditionnels. Ces avantages font
tous appel à un mélange de quatre concepts principaux~: automatisation,
personnalisation, flexibilité et qualité graphique.

\begin{figure}

{\centering \includegraphics[width=4.18in,height=\textheight]{images/chapitre5_word-vs-latex.png}

}

\caption{\label{fig-latex-vs-word}Utilité relative de Word et de
\LaTeX~selon la complexité et la taille du document
\newline \textit{Source}~: Yannick Dufresne (2015).}

\end{figure}

\hypertarget{avantages-2}{%
\subsection{Avantages}\label{avantages-2}}

\hypertarget{ruxe9fuxe9rencement}{%
\subsubsection{Référencement}\label{ruxe9fuxe9rencement}}

Premièrement, \LaTeX~et Markdown permettent d'intégrer une bibliographie
\emph{automatique} et professionnelle en utilisant \textsc{Bib}\TeX.
Cette bibliographie peut être adaptée très facilement en différents
styles bibliographiques reconnus ou en un style bibliographique
\emph{personnalisé} à partir d'un des nombreux gabarits professionnels
disponibles. Avec \textsc{Bib}\TeX, il n'y a pas à vérifier si le titre
de l'article est toujours en italique, si le numéro de volume est
toujours entre parenthèses ou si le nom de famille des deuxièmes auteurs
est toujours avant ou après le prénom puisque toutes ces opérations sont
effectuées de manière \emph{automatique}. \textsc{Bib}\TeX~comprend
également les différences entre les types de sources --- articles
scientifiques, livres, sites Internet, etc. --- et ajuste leur
présentation en conséquence. De plus, si une des sources que vous citez
n'est pas incluse dans la bibliographie, une erreur s'affiche, vous
permettant d'identifier le problème plutôt que de vous retrouver avec
une référence manquante. À l'inverse, si une source est retirée du
texte, elle disparait \emph{automatiquement} de la bibliographie dans le
document final mais demeure présente dans le fichier où se trouvent les
références bibliographiques. Cela évite les aller-retour pour vérifier
que chaque source de la bibliographie se trouve au moins une fois dans
le texte et que chaque source dans le texte est citée en bibliographie.
Grâce aux balises, en cliquant sur les références incluses dans le
document, vous vous retrouverez immédiatement plus loin dans le
document, à l'endroit où se trouve l'entrée bibliographique associée.
Les références \textsc{Bib}\TeX~pour articles scientifiques peuvent être
copiées-collées à partir de Google Scholar. \textsc{Bib}\TeX~rend donc
extrêmement simple et efficace l'utilisation des références
bibliographiques grâce à sa capacité à \emph{personnaliser} et
\emph{automatiser} leur présentation\footnote{L'utilisation d'un
  logiciel de traitement de texte, en particulier lorsque combiné avec
  une extension Zotero, peut également produire un résultat automatisé
  et personnalisé à un certain degré. Cependant, au moment où ce
  chapitre est écrit, le retrait d'une citation du texte principal en
  Word avec Zotero n'enlève pas immédiatement cette citation de la
  bibliographie, et l'ajout de liens vers la bibliographie doit se faire
  source par source, ce qui prend un temps important et peut occasionner
  des erreurs humaines.}.

\hypertarget{figures-et-tableaux}{%
\subsubsection{Figures et tableaux}\label{figures-et-tableaux}}

L'intégration de figures et de tableaux dans le texte est aussi rendue
très simple et professionnelle grâce à \LaTeX~et à Markdown. La taille
de la figure ou du tableau, son positionnement et son intégration par
rapport au texte environnant peuvent être réglés avec précision.
Cependant, l'ajout de texte avant ou après la figure ou le tableau ne
produira pas des résultats inattendus tels qu'une demi-page vide avant
un graphique ou un titre de tableau complètement en bas d'une page. En
définissant des paramètres pour l'ensemble du texte, la chercheuse ou le
chercheur peut \emph{personnaliser} entièrement la présentation des
figures et des tableaux. De plus, la \emph{qualité des figures et des
tableaux} ne diminue pas lors de leur intégration~: les figures restent
aussi belles qu'elles l'étaient originalement, ce qui n'est pas toujours
le cas dans les logiciels de traitement de texte. Les figures et les
tableaux sont aussi numérotés \emph{automatiquement}, ce qui veut dire
que vous n'aurez jamais à vous préoccuper de modifier les numéros si
l'ordre des figures et tableaux est modifié dans le texte. Grâce aux
balises, en cliquant sur le numéro associé à la figure ou au tableau
dans le texte, le document se retrouve automatiquement à l'endroit où se
trouve la figure ou le tableau. De plus, les figures peuvent être
intégrées en format PDF, ce qui permet au lecteur de copier-coller ou de
surligner de l'information se trouvant sur le graphique directement,
incluant les titres des axes et les annotations.

Surtout, l'intégration de graphiques produits par \texttt{R} au texte en
langage de balisage est simplifiée et \emph{automatisée}. En effet, même
lorsque les données ou le code pour produire un graphique changent,
\texttt{R} resauvegarde le fichier dans le même chemin d'arborescence
(\emph{path}) particulier que vous avez indiqué, par exemple
\texttt{C:/Users/Jean/Dropbox/projet1/graphs/Figure1.pdf}. Le langage de
balisage peut ensuite indiquer le même chemin d'arborescence, de sorte
qu'il n'est pas nécessaire de recopier-coller la figure à l'intérieur du
document chaque fois que des changements y sont apportés; la figure est
mise à jour \emph{automatiquement}.

L'intégration de figures et de tableaux est particulièrement simple et
\emph{flexible} avec Quarto. Contrairement à \LaTeX, qui nécessite la
production de tableaux et de figures dans un document en langage de
programmation (comme \texttt{R}), Quarto permet de créer une figure
grâce à du code \texttt{R} \emph{et} d'intégrer celle-ci au texte dans
un même document. Cela se fait grâce à l'intégration de blocs de code
\texttt{R} (\emph{code chunks}) dans le document. Le code est produit
dans le bloc de code et la figure ou le tableau qui en résulte apparait
à la fois dans le document Quarto, où des balises supplémentaires
permettent d'adapter le formatage, et sur le document fini. Cependant,
certains \emph{packages} R permettent de créer des tableaux de
régression de grande qualité en format \LaTeX. Les tableaux de
régression en format Markdown sont pour l'instant plus difficiles à
produire au-delà d'un certain niveau de complexité, en raison des
limitations du langage Markdown. Il demeure possible de produire des
tableaux en langage \LaTeX~dans un fichier Markdown ou Quarto.

\hypertarget{uxe9quations}{%
\subsubsection{Équations}\label{uxe9quations}}

\LaTeX~permet également d'ajouter des équations mathématiques poussées.
En effet, il existe des balises pour chaque symbole mathématique, et
celles-ci peuvent être agencées de manière à former des équations
cohérentes. Ces équations peuvent être intégrées au sein même d'une
phrase ou être mises de l'avant dans un paragraphe à part centré.

\hypertarget{table-des-matiuxe8res-et-mise-en-page}{%
\subsubsection{Table des matières et mise en
page}\label{table-des-matiuxe8res-et-mise-en-page}}

Markdown et \LaTeX~permettent aussi la gestion \emph{automatisée} de la
table des matières, et les références aux pages appropriées à partir de
la table des matières se mettent à jour en continu. La table des
matières prend en compte l'architecture du texte choisie manuellement
par le chercheur, qui est définie par des balises définissant différents
niveaux hiérarchiques de sections, sous-sections ou chapitres. Des
manières \emph{automatiques} de référencer les figures et les tableaux
dans des sections distinctes de la table des matières sont également
offertes, encore une fois \emph{personnalisables} au goût du chercheur.

Bien que la mise en page de documents produits via Markdown et
\LaTeX~puisse être définie entièrement manuellement par les personnes
plus expérimentées, les novices apprécieront les nombreux gabarits
(\emph{templates}) qui permettent de gérer \emph{automatiquement} la
mise en page clés en main. Les gabarits permettent de rendre l'apparence
d'un document plus esthétique et uniforme et peuvent être utilisés tels
quels ou servir de point de départ pour un chercheur ou une chercheuse
souhaitant y apporter certaines modifications sans toutefois partir
d'une feuille blanche. La majorité des personnes qui utilisent ces
langages, même les plus expérimentées, utilisent ces gabarits comme base
lorsqu'elles rédigent un document. Ceux-ci constituent une mine d'or
puisqu'ils rendent accessible le code Markdown ou \LaTeX~ayant servi à
la conception du gabarit, permettant à la chercheuse ou au chercheur de
comprendre comment est obtenu le résultat que lui offre le gabarit.
Incidemment, il est possible d'identifier les sections de code
produisant certains éléments de mise en page --- positionnement des
numéros de page, positionnement du nom des auteurs en début de document,
etc. --- et les modifier ou s'en inspirer afin de modifier d'autres
gabarits. L'utilisation de ces gabarits peut s'avérer complexe au
départ, mais il s'agit d'une complexité qui s'avère ultimement
extrêmement productive puisqu'elle vous permettra de devenir autonome et
d'ajuster les gabarits à votre convenance afin de produire exactement le
résultat désiré en termes de mise en page. En comparaison, les logiciels
de traitement de texte rendent souvent très ardue la mise en page
uniforme d'un document, puisque cet élément ne peut pas être
\emph{automatisé}. La liste des gabarits disponibles est extrêmement
large, et ceux-ci ont une variété de fonctions. En effet, une variété de
gabarits professionnels et de haute \emph{qualité graphique} sont
offerts gratuitement en ligne pour des articles, des livres, des
rapports, des \emph{curriculum vitæs} Figure~\ref{fig-cv} ou encore des
feuilles de temps pour des contrats rémunérés Figure~\ref{fig-invoice}.

\begin{figure}

{\centering \includegraphics[width=2.02in,height=\textheight]{images/chapitre5_CVtemp.png}

}

\caption{\label{fig-cv}Exemple de gabarit \LaTeX~de curriculum vitae
\newline \textit{@liantze23}~: .}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1.85in,height=\textheight]{images/chapitre5_TStemp.png}

}

\caption{\label{fig-invoice}Exemple de gabarit \LaTeX~de feuille de
temps \newline \textit{@roux13}~: .}

\end{figure}

Les Figure~\ref{fig-cv} et Figure~\ref{fig-invoice} ne sont que quelques
exemples des milliers de gabarits de documents \LaTeX~disponible en
ligne. Plusieurs d'entre eux peuvent être téléchargés à partir du site
Web d'(\textbf{overleaf23?}). Vous pouvez y naviguer et voir quel
gabarit convient le mieux à vos besoins. Certaines manières plutôt
spécifiques de formater le texte sont présentement disponibles avec
\LaTeX~ou Markdown bien que non disponibles en Word, ce qui constitue
une autre preuve de leur grande \emph{flexibilité} et capacité de
\emph{personnalisation}. Bien qu'il soit rare que nous ayons absolument
besoin de personnaliser le texte ainsi, ces possibilités peuvent
s'avérer utiles lorsque vous rédigez un texte qui doit se conformer en
tout point à un gabarit spécifique. En effet, certaines revues
scientifiques, maisons d'édition et universités, dans le cadre de la
rédaction d'articles, de mémoires et de thèses par exemple, imposent ce
type de gabarit inflexible et parfois plutôt capricieux.

\hypertarget{compatibilituxe9-entre-types-de-documents}{%
\subsubsection{Compatibilité entre types de
documents}\label{compatibilituxe9-entre-types-de-documents}}

Un autre avantage non négligeable de Markdown --- qui le distingue à cet
égard de \LaTeX~--- est la \emph{flexibilité} des formats de documents
qui peuvent être produits. En effet, Pandoc Markdown, une extension du
langage Markdown de base, permet d'intégrer dans un seul document
plusieurs langages de balisage différents tels que Markdown, \LaTeX~et
HTML. Quarto utilise Pandoc Markdown et est également habilité à
travailler avec des extraits de code \texttt{R} ou Python. Ceci permet
donc à l'utilisateur ou à l'utilisatrice de bénéficier des
fonctionnalités de différents langages dans un seul document, rendant
ainsi possible une variété de \emph{personnalisations} qui ne seraient
pas possibles autrement. Qui plus est, puisque Markdown permet de créer
des fichiers Word réguliers, PDF professionnels et HTML à partir d'un
même document, vous pouvez choisir à votre convenance et à tout moment
de quelle manière sera compilé le document rédigé. Cette possibilité de
créer des documents Word est particulièrement pratique dans le cadre de
collaboration avec des chercheuses et chercheurs n'utilisant pas les
langages de balisage ainsi que lors de l'envoi de manuscrits à des
revues scientifiques, puisque certaines d'entre elles exigent de
recevoir ceux-ci sous forme de document Word.

\hypertarget{popularituxe9}{%
\subsubsection{Popularité}\label{popularituxe9}}

La popularité de certains langages de balisage dans le monde de la
recherche confère un avantage considérable à celles et ceux qui savent
les utiliser. La maîtrise de ces langages offre aux chercheurs et
chercheuses une polyvalence lorsqu'ils doivent collaborer avec diverses
équipes de recherche utilisant différentes méthodes de travail. Par
exemple, depuis sa création \LaTeX~a été adopté largement par le milieu
de la publication de travaux scientifiques (Gaudeul 2007). À titre
indicatif, le logiciel Overleaf était utilisé en 2022 par 11 millions
d'utilisateurs et utilisatrices dans 189 pays autour du globe. Plus de
2000 compagnies et 6800 universtiés utilisent Overleaf pour écrire en
\LaTeX~({``Wow! {Ten} Million Users!''} n.d.). La popularité des
langages de balisages en fait donc un outil difficile à contourner pour
une personne qui voudrait poursuivre une carrière en recherche
académique. L'avantage ci-bas sur la gestion des embûches illustre
également comment la popularité permet une meilleure gestion de
celles-ci.

\hypertarget{gestion-des-embuches}{%
\subsubsection{Gestion des embuches}\label{gestion-des-embuches}}

Bien que l'apprentissage de \LaTeX~et de Markdown puisse être parsemé de
nombreuses embuches, ces deux langages bénéficient d'une communauté
d'utilisateurs et d'utilisatrices en ligne sur laquelle il est possible
de s'appuyer afin de résoudre tout problème rencontré. Ces individus ---
particulièrement les plus expérimentés --- sont nombreux à partager leur
expérience à leurs collègues rencontrant des problèmes afin de
contribuer à régler ceux-ci. Cette communauté est présente sur une
multitude de sites Web, bien que le point de rencontre principal soit le
forum Stack Overflow (2023), qui est également utilisé pour régler des
problèmes de programmation et est abordé plus en détail dans le chapitre
4. Une simple recherche sur Google d'un problème rencontré avec
\LaTeX~ou Markdown vous offrira des liens vers des échanges pertinents
ayant eu lieu sur Stack Overflow ou encore vers de la documentation
technique. Vous pourrez donc filtrer les résultats et observer les
nombreuses solutions envisageables à votre problème afin de définir
laquelle est la plus appropriée dans votre situation. Il est important
de noter, toutefois, que cette communauté est nettement plus développée
pour les utilisateurs de \LaTeX~que de Markdown, puisque ce dernier
langage est moins répandu que le premier.

Également, avec l'émergence de l'intelligence artificielle (IA), de
nombreux modèles d'IA génératifs commencent à émerger comme des
ressources d'aides utiles pour les chercheuses et les chercheurs. Au
moment de la rédaction du présent chapitre, le \emph{chatbot} ChatGPT,
développé par OpenAI et basé sur le grand modèle de langage (\emph{large
language model}, LLM) GPT-3.5, est une ressource d'aide en émergence en
ce qui a trait aux langages de balisage. Le corpus de données sur lequel
il a été formé inclut une grande variété de langages et de styles
d'écriture, incluant \LaTeX~et Markdown. Ainsi, il est possible de poser
des questions en langage courant à ce \emph{chatbot} lorsque des
problèmes de balisage sont rencontrés. Celui-ci fournira en réponse le
texte avec les balises adéquates pour régler le problème. Cela
s'applique même pour des problèmes pour lesquels la réponse n'est pas
directement indiquée sur Stack Overflow, lorsque la logique des langages
est comprise par ces modèles basés sur l'IA. ChatGPT est toutefois plus
outillé en \LaTeX~qu'en Markdown ou en Quarto en raison de la plus
grande abondance de ressources en \LaTeX~disponibles en ligne, bien que
ses capacités soient en constante amélioration. Il arrive cependant
régulièrement que les réponses des modèles de langage comme ChatGPT soit
erronées --- tout comme certaines réponses sur Stack Overflow peuvent ne
pas être adaptées à régler un problème similaire vécu sur un autre
ordinateur, avec des paramètres différents. Il demeure donc important de
vérifier les réponses des modèles basés sur l'IA afin de ne pas avoir de
mauvaises surprises lors de la compilation du code. Ainsi, il est utile
de s'appuyer autant sur la communauté d'utilisatrices et d'utilisateurs
de langages de balisage qui échange des ressources en ligne que sur les
modèles de langage basés sur l'IA.

\hypertarget{philosophie-du-code-source-ouvert-et-du-logiciel-libre}{%
\subsection{Philosophie du code source ouvert et du logiciel
libre}\label{philosophie-du-code-source-ouvert-et-du-logiciel-libre}}

L'utilisation des langages de balisage s'inscrit bien dans la
philosophie du logiciel libre. Le langage \LaTeX~est distribué sous la
license \emph{\LaTeX~project public license} (LPPL), alors que Quarto
1.4 est distribué sous la license du Massachusetts Institute of
Technology (MIT). Moyennant le respect de leurs licenses respectives,
ces licenses permettent ainsi aux utilisateurs de \LaTeX~et de Quarto
d'utiliser ces langages comme ils le souhaitent, de redistribuer des
copies, de modifier le fonctionnement de ces langages et d'en
redistribuer des versions améliorées. Bien que \LaTeX~et Quarto ne
soient pas des logiciels, leur license utilisation est cohérente avec la
philosophie du logiciel libre et permet à ces langages d'être utilisés
dans de nombreux logiciels.

D'autre part, \LaTeX~et Quarto sont deux langages à code source ouvert
(\emph{open source}). Ainsi, leur distribution est entièrement gratuite,
il n'y a rien à payer. Leur code source est disponible et les
changements à ce code doivent être indiqués. Les licences LPPL et MIT ne
discriminent pas certains groupes ou personnes, et elles ne restreignent
personne dans l'utilisation pour un domaine d'activité. Ces deux
licences ne sont pas spécifiques pour un produit, ce qui signifie que
ces deux langages peuvent être utilisés dans plusieurs logiciels et
programmes. Elles sont également technologiquement neutres, rendant
ainsi \LaTeX~et Quarto accessibles aux utilisateurs de tous les systèmes
d'exploitation.

\hypertarget{inconvuxe9nients-1}{%
\subsection{Inconvénients}\label{inconvuxe9nients-1}}

Il existe toutefois des désavantages inhérents à l'utilisation des
langages de balisage. L'un des principaux désavantages de Markdown et de
\LaTeX~est le fait qu'ils ne comportent aucun système de suivi des
modifications lors de travaux collaboratifs. Pour réviser un travail
fait en langage de balisage, des commentaires peuvent être ajoutés sur
le fichier sortant --- nécessairement PDF pour un fichier sortant
produit avec \LaTeX. Des commentaires peuvent aussi être faits
directement dans le document \LaTeX~ou Markdown, à l'aide de balises
spécifiques. Ces commentaires n'apparaissent cependant pas dans le
fichier sortant. Le suivi des modifications en \LaTeX~et Markdown
nécessite donc souvent l'utilisation de Git et de GitHub, qui sont
abordés plus en détail dans le chapitre 8. Même avec une plateforme de
gestion des versions comme GitHub, les longs paragraphes ayant fait
l'objet de plusieurs modifications peuvent être longs à comparer par
rapport aux logiciels de traitement de texte, qui permettent de
visualiser les propositions d'ajouts et de retraits de caractères de
manière plus intuitive. Le suivi des modifications en logiciel de
traitement de texte permet également de distinguer les auteurs de
différents commentaires par leurs noms, alors que les ajouts et retraits
itératifs en GitHub peuvent rendre difficile l'identification de
l'auteur d'une modification. Pour ces raisons, et aussi pour faciliter
la mise en page par les éditeurs, certaines revues scientifiques
refusent les fichiers PDF et demandent que les soumissions soient faites
en format DOCX --- ce qui pose problème pour les utilisateurs de
\LaTeX~mais pas ceux de Markdown.

Les langages de balisage comportent également un autre désavantage
important dans certains cas~: l'absence d'un correcteur de fautes de
français complet, en particulier pour corriger les fautes autres que
celles d'orthographe en français. Parmi les principaux endroits
permettant l'édition en langages de balisage, Visual Studio Code (VS
Code) et Overleaf comprennent tous deux une extension LanguageTool, qui
permet la révision orthographique et syntaxique dans plusieurs langues.
VS Code possède également une extension Antidote pour les personnes qui
paient déjà pour ce logiciel. D'autres extensions linguistiques existent
également pour VS Code, de même que pour les logiciels de traitement de
texte comme Word. Cependant, RStudio ne possède qu'un correcteur
\emph{orthographique} de base, disponible en plusieurs langues. Ce
correcteur ne repère pas les erreurs de syntaxe, de grammaire ou de
forme, entre autres. Ces éléments sont pourtant essentiels pour la
rédaction de textes académiques\footnote{Pour les utilisateurs de
  RStudio, il est souvent nécessaire de modifier le texte dans un
  logiciel de traitement de texte externe pour faire une révision
  linguistique complète, puis d'intégrer les corrections en collant le
  texte corrigé dans le document original en Markdown ou \LaTeX. Une
  application de bureau Grammarly peut également être intégrée sur
  RStudio. Cette application repère les erreurs de syntaxe, mais ne
  corrige que l'anglais, et certains soucis de repérage des mots aux
  bons endroits dans le texte en rendent présentement l'utilisation
  difficile.}, d'autant plus que les utilisateurs de \LaTeX~ont tendance
à faire davantage de fautes d'ortographe et de grammaire lors de la
rédaction que les utilisateurs de Word (Knauff and Nejasmic 2014). Nous
décourageons ainsi l'utilisation de RStudio pour l'édition de fichiers
\LaTeX~et Quarto, et nous encourageons fortement les utilisateurs
d'Overleaf et de VS Code d'utiliser des extensions permettant la
correction grammaticale telles que LanguageTool.

Enfin, les langages de balisage, contrairement aux logiciels de
traitement de texte, nécessitent d'être compilés, ce qui implique que
deux fichiers coexistent~: le fichier où le langage de balisage est
utilisé --- format \texttt{.tex} pour \LaTeX, \texttt{.md} pour Markdown
ou encore \texttt{.qmd} pour Quarto --- ainsi que le fichier où le texte
final balisé apparait --- généralement \texttt{.pdf}, \texttt{.docx} ou
\texttt{.html}. La compilation peut prendre un temps variable selon la
complexité du document, mais dure typiquement une quinzaine de secondes.
Le fait de devoir travailler avec deux fichiers en parallèle et de ne
pas voir immédiatement l'effet des balises sur le document final
constitue ainsi un autre désavantage des langages de balisage.

\LaTeX~comporte aussi quelques difficultés techniques particulières qui
peuvent être réglées ou diminuées en travaillent en Markdown.
Premièrement, \LaTeX~est difficile à apprendre. Certaines tâches qui
peuvent sembler simples comme l'ajout d'un tableau peuvent nécessiter de
nombreuses lignes de code. De plus, à la moindre erreur de frappe dans
l'utilisation d'une balise, le code risque de ne pas fonctionner et de
ne pas produire le document PDF souhaité. C'est ce qu'on appelle une
erreur de compilation. Markdown est un langage plus simple à apprendre,
avec des balises plus courtes et intuitives. Il occasionne donc moins
d'erreurs de compilation.

Deuxièmement, \LaTeX~est peu compatible avec les logiciels de traitement
de texte comme Word. Pour transférer un fichier créé à partir d'un
logiciel de traitement de texte vers \LaTeX, les balises doivent être
ajoutées manuellement une par une. À l'inverse, pour transférer un
document \LaTeX~vers un fichier de traitement de texte, le convertisseur
Pandoc peut être utilisé, mais celui-ci ne repère pas toutes les balises
et il est souvent nécessaire de faire des aller-retour entre le fichier
\LaTeX~original et le fichier converti en format DOCX pour s'assurer du
succès de la conversion. Parfois, les balises doivent être retirées une
par une et le formatage doit être refait en utilisant les boutons
fournis sur le logiciel de traitement de texte. Il est aussi possible de
copier le texte directement à partir du fichier PDF produit par
\LaTeX~vers un logiciel de traitement de texte, mais les fins de ligne
sont interprétées par Word, Pages ou Writer comme des retours plutôt que
des espaces, et les accents sont souvent mal copiés et doivent être
réécrits manuellement. Encore une fois, Markdown évite ce problème en
permettant d'écrire un fichier DOCX à partir du langage de balisage. Le
formatage du fichier DOCX demeure un peu compliqué cependant et doit
être fait à partir du modèle d'un autre document DOCX formaté tel que
souhaité. De plus, les fichiers DOCX ne peuvent pas être transformés en
format Markdown. Quarto permet d'écrire un texte en format Markdown et
de produire un fichier DOCX à partir d'un gabarit Word. De plus, pour
les fichiers Word à transformer en format Markdown, les balises plus
simples en Markdown qu'en \LaTeX~rendent la tâche plus simple.

Somme toute, Word n'est pas à antagoniser et demeure très utile pour des
tâches simples. Cependant, dans le monde académique, la production de
fichiers de qualité faisant appel à des graphiques, tableaux et blocs de
code personnalisés de qualité et automatisés est simplifiée en utilisant
des langages de balisage. Il n'est ainsi pas anodin que ces langages
soient adoptés largement dans le monde académique. Pour ces raisons, il
est avantageux pour une personne poursuivant une maitrise en science
sociale ou autre de passer outre la difficulté initiale d'apprentissage
des langages de balisage. L'apprentissage de ces langages permettra de
s'aligner sur les pratiques répandues dans le milieu académique et
d'améliorer davantage la qualité de la production d'écrits
scientifiques.

\hypertarget{conclusion-4}{%
\section{Conclusion}\label{conclusion-4}}

Tout compte fait, les langages de balisage permettent d'effectuer des
tâches que vous ne pourriez pas normalement réaliser en utilisant un
logiciel de traitement de texte classique. Ils facilitent la production
de documents professionnels dans différents formats personnalisés,
produits avec des processus automatisés, avec une grande qualité
graphique. Les langages de balisage demandent un certain temps
d'apprentissage, entre autres pour \LaTeX, mais peuvent ensuite être
utilisés dans différents environnements de travail en ligne comme hors
ligne.

\hypertarget{ruxe9fuxe9rences-1}{%
\section{Références}\label{ruxe9fuxe9rences-1}}

\bookmarksetup{startatroot}

\hypertarget{outils-dintelligence-artificielle}{%
\chapter{Outils d'intelligence
artificielle}\label{outils-dintelligence-artificielle}}

\begin{center}

Jozef Rivest, Laurence-Olivier Foisy, Hubert Cadieux

\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "/Users/jozefrivest/Documents/GitHub/livre-outils"
\end{verbatim}

Ce chapitre vise à initier les lecteurs à ce qu'est l'intelligence
artificielle (IA), aux enjeux qu'elle engendre ainsi qu'à son potentiel
pour les sciences sociales. Nous souhaitons d'abord et avant tout mener
les lecteurs vers la réflexion. Dans ces pages, il y a beaucoup plus de
questions que de réponses. Cet état de fait reflète bien l'état de la
connaissance que nous avons sur l'IA. Nous sommes aussi bien loin d'être
des spécialistes en la matière. Malgré cela, nous pensons qu'initier la
réflexion et la discussion s'impose. Se poser des questions et tenter de
trouver des réponses ne peut que générer des bénéfices. Par conséquent,
si ce chapitre aura permis d'éclairer certains enjeux, ou s'il stimulera
davantage de questionnement, alors nous aurons réussi notre objectif.

Nous débutons par définir et expliquer ce qu'est l'intelligence
artificielle. Bien que ça ne soit pas une tâche facile, nous souhaitons
simplement donner une idée générale de ce qu'il s'agit. Ensuite, nous
souhaitons mettre l'accent sur l'évolution constante de ce champ de
recherche, ainsi que les défis que ça pose. La section suivante se
penche sur certains enjeux éthiques lié à l'utilisation de l'IA,
notamment en ce qui concerne le plagiat. Cela nous mène à se questionner
sur la place du chercheur, aujourd'hui et dans le futur, avec l'arrivé
de ces nouvelles technologies. La dernière section se penchera sur la
place de cette technologie en sciences sociales. Quel usage pourrait-on
en faire, et surtout quels en sont les limites.

\hypertarget{duxe9finition-et-diffuxe9rents-type-dia}{%
\section{Définition et différents type
d'IA}\label{duxe9finition-et-diffuxe9rents-type-dia}}

Qu'est-ce que l'intelligence artificielle (IA)? Est-ce quelque chose
d'homogène, ou s'agit-il plutôt « des intelligences artificielles »?
Dans un premier temps, il est important de préciser que l'intelligence
artificielle est un champ d'études (Devedzic 2022). Par conséquent, il
s'agit d'un ensemble d'objets, relativement vaste et en constante
expansion, qui s'intéressent, à sa façon, à l'intelligence artificielle.
Pour préciser ce propos, prenons l'exemple de la science politique.
Malgré la formulation au singulier, la science politique est un grand
ensemble de différents sous-champs d'études, qui ont chacun leur propre
objet d'intérêt. La philosophie politique, les relations
internationales, la politique comparée et l'étude de l'opinion publique,
par exemple, sont tous des sous-champs qui s'intéressent, à leur façon,
au phénomène politique. Dans le même sens, et compte tenu de cette
pluralité de perspectives, il est important de noter qu'il n'y a pas de
consensus dans la définition de l'IA (Wang 2019 ; König et al. 2022). De
plus, la rapidité du développement de ce champ rend le traçage de
frontières définitionnelles plutôt difficile : comment définir, d'une
manière précise et consensuelle, quelque chose qui évolue constamment
(Devedzic 2022 ; Bertolini 2020, 15)?

Deux définitions de l'IA peuvent tout de même être retenues. La première
vient de John McCarthy (2007, 2) : « Il s'agit de la science et de
l'ingénierie qui consistent à créer des machines
intelligentes\footnote{Intelligence étant définit de la façon suivante :
  « L'intelligence est la partie informatique de la capacité à atteindre
  des objectifs dans le monde. On trouve différents types et degrés
  d'intelligence chez l'homme, chez de nombreux animaux et chez
  certaines machines. » (McCarthy 2007, 2)}, en particulier des
programmes informatiques intelligents. Elle est liée à la tâche
similaire consistant à utiliser des ordinateurs pour comprendre
l'intelligence humaine, mais l'IA ne doit pas se limiter aux méthodes
qui sont biologiquement observables. » {[}Traduction DeepL{]} La seconde
définition provient de la compagnie IBM (2023a) : « Dans sa forme la
plus simple, l'intelligence artificielle est un domaine qui combine
l'informatique et des ensembles de données robustes pour permettre la
résolution de problèmes. Elle englobe également les sous-domaines de
l'apprentissage automatique et de l'apprentissage profond, qui sont
souvent mentionnés en conjonction avec l'intelligence artificielle. Ces
disciplines sont composées d'algorithmes d'IA qui cherchent à créer des
systèmes experts qui font des prédictions ou des classifications basées
sur des données d'entrée. » {[}Traduction DeepL{]} Ces extraits
permettent de comprendre que l'IA consiste à reproduire artificiellement
certaines capacités cognitives humaines, afin de rendre les machines «
intelligentes » en leur donnant la capacité de résoudre des problèmes
par elles-mêmes.

Ces définitions restent toutefois à préciser, notamment dans le champ
d'application de l'IA : qu'en est-il concrètement ? Comment est-ce
utilisé ? Comment ça fonctionne ? Si l'IA se distingue enn plusieurs
types, en faire la liste et identifier leurs différentes branches et
applications possibles serait fastidieux et s'écarterait de l'objectif
de ce chapitre introductif. Cependant, pour ceux désirant en savoir plus
sur le sujet, les articles de McCarthy (2007) et de Hanchen et al.
(2023), ainsi que le Cambridge Handbook of Artificial Intelligence
(König et al. 2022) sont très riches et illustratifs sur ce qu'est l'IA
ainsi que sur ses champs d'applications.

Avant toute chose, il est important de distinguer l'IA général (strong)
du précis (narrow). Le premier, et le moins populaire en nombre de
recherches et d'applications, cherche à développer une machine qui
aurait les mêmes capacités cognitives que l'humain, non seulement en
termes de résolution de problème, d'apprentissage et de planification,
mais aussi qui serait dotée d'une conscience de soi (IBM 2023a ; König
et al. 2022). Le deuxième est plus restrictif, se limitant à la
réalisation d'un ou de plusieurs objectifs spécifiques. De ces deux
visées, il y a trois principaux champs de recherche qui se penchent sur
les méthodes de fonctionnement de l'IA : l'apprentissage machine
(machine learning), les réseaux neuronaux artificiel (artificial neural
networks) ainsi que l'apprentissage profond (deep learning).

L'apprentissage machine : « {[}\ldots{]} consiste à programmer des
ordinateurs pour optimiser un critère de performance à l'aide de données
d'exemple ou d'expériences passées. Nous avons un modèle défini jusqu'à
certains paramètres, et l'apprentissage est l'exécution d'un programme
informatique pour optimiser les paramètres du modèle à l'aide des
données d'entraînement ou de l'expérience passée. » {[}Traduction
DeepL{]} (Alpaydin et Bach 2014, 3). Le but est d'entraîner le modèle
afin qu'il puisse reconnaître des tendances, et qu'il puisse décrire
et/ou faire des prédictions à partir de ces tendances (Alpaydin and Bach
2014, 3). Il est le champ le plus populaire dans la recherche faite sur
l'IA, notamment parce qu'il constitue une base importante pour les
autres recherches dans le domaine (Devedzic 2022).

Ensuite, un sous-champ de l'apprentissage machine, l'apprentissage
profond : « {[}\ldots{]} fait référence à un réseau neuronal composé de
plus de trois couches {[}\ldots{]}. L'apprentissage en profondeur
automatise une grande partie de l'extraction des caractéristiques,
éliminant ainsi une partie de l'intervention humaine manuelle nécessaire
et permettant l'utilisation d'ensembles de données plus importants. Il
peut ingérer des données non structurées dans leur forme brute et
déterminer automatiquement la hiérarchie des caractéristiques qui
distinguent les différentes catégories de données les unes des autres,
ne nécessitant pas d'intervention humaine. » {[}Traduction DeepL{]} (IBM
2023a). Ainsi, l'apprentissage profond permet une certaine forme
d'automatisation des tâches demandées à l'IA, en lui fournissant les
capacités nécessaires d'apprendre par lui-même pour corriger et
améliorer son fonctionnement. Pour ce faire, on doit développer des
structures neuronales artificielles, qui s'inspirent des neurones du
cerveau humain.

C'est d'ailleurs la tâche de ceux qui s'intéressent aux réseaux
neuronaux artificiels : « Les réseaux neuronaux artificiels (RNA) sont
constitués d'une couche de nœuds, contenant une couche d'entrée, une ou
plusieurs couches cachées et une couche de sortie. Chaque nœud, ou
neurone artificiel, se connecte à un autre et possède un poids et un
seuil associés. Si la sortie d'un nœud individuel est supérieure à la
valeur seuil spécifiée, ce nœud est activé et envoie des données à la
couche suivante du réseau. Dans le cas contraire, aucune donnée n'est
transmise à la couche suivante du réseau. » {[}Traduction DeepL{]} (IBM
2023b). Ainsi, le but est de reproduire les structures cognitives
humaines, afin de permettre à l'IA d'accomplir des tâches plus
complexes. L'une des principales utilités de ce sous-champ est qu'il
permet d'augmenter la rapidité du classement de données ; la
reconnaissance vocale ou d'image, par exemple, ne prend que quelques
minutes grâce à cela, contrairement à plusieurs heures lorsque fait par
des humains (IBM 2023b).

\hypertarget{luxe9volution-constante-de-lia}{%
\section{L'évolution constante de
l'IA}\label{luxe9volution-constante-de-lia}}

Plusieurs chercheurs, dont le professeur Yoshua Bengio de l'Université
de Montréal, ont lancé plusieurs avertissements sur le développement de
l'IA, notamment à cause de la rapidité de son évolution. Dans un article
paru dans The Economist, M. Bengio (2023) nous dit qu'il prévoyait le
développement d'une IA avec des capacités similaires à celles de
l'humain d'ici quelques décennies, peut-être un siècle. Depuis l'arrivée
de ChatGPT-4, celui-ci a revu sa prédiction pour la situer entre
quelques années et quelques décennies (Bengio 2023). Dans les dix
dernières années seulement, les systèmes de reconnaissance d'images et
de langages en sont venus à dépasser les capacités humaines (Roser
2022). La figure 1 présente cette évolution.

\begin{figure}

{\centering \includegraphics{images/chapitre9_figure1.png}

}

\caption{Évolution de l'IA depuis 1998}

\end{figure}

\newpage{}

Comme on le remarque, cette évolution ne suit pas une trajectoire
linéaire. Depuis 2015, la plupart de ces technologies ont évolué de
manière quasi-exponentielle. Cette progression fulgurante nous permet de
faire quelque constat pour appréhender l'évolution future. Ce qui est
intéressant de la recherche dans ce domaine, c'est que le développement
des capacités de l'IA permet, en retour, de la développer encore plus
rapidement. L'IA peut rendre l'IA exponentiellement plus puissante
(Harari 2023). Elle est capable de se faire évoluer à une vitesse plus
grande, grâce à ses capacités de traitement de données et
d'auto-améliorassions, que si elle était uniquement dépendante de
l'humain.

En tant que chercheur en sciences sociales, pourquoi devrait-on être
préoccupé par le développement de l'IA? Tout d'abord, il est intéressant
de commencer à réfléchir ainsi qu'à analyser les différents impacts que
l'IA a sur nos sociétés. Les avancés dans le domaine en plus de
l'accessibilité à ces technologies, tel que Large Language Model (LLM),
en font de nouveaux objets d'étude actuel, et surtout sans grandes
réponses. Yuval Noah Harrari (2023), historien et philosophe, explique
dans un article que la capacité de l'IA à manipuler ainsi qu'à générer
du langage en font des outils puissants qui ont le potentiel d'avoir de
profond impact sur nos civilisations. Par conséquent, l'étude des effets
de l'IA sur nos sociétés devient un nouveau sujet de recherche dont tous
les champs des sciences sociales et sciences humaines ont intérêt à se
pencher. Présentement, il est anticipé que cette technologie puisse être
utilisée pour « générer et partager de fausses informations, érodant la
confiance sociale et la démocratie; pour surveiller, manipuler et
maîtriser les citoyens, nuisant aux libertés individuelles et
collectives; ou pour créer de puissantes armes physiques ou digitales
qui menaceraient la vie humaine. » {[}Traduction libre{]} (Bremmer and
Suleyman 2023, 32). Compte tenu des conséquences potentielles que ces
technologies peuvent avoir sur le monde social, tous les domaines
scientifiques ont un fort incitatif à décloisonner leur savoir et leurs
analyses, en plus de maximiser l'interdisciplinarité et la recherche
collaborative. Une compréhension plus complète de ces changements ainsi
qu'une communication de ce savoir ne pourra que générer des bénéfices
pour le monde académique, social et politique.

\hypertarget{les-diffuxe9rents-outils-de-lia}{%
\section{Les différents outils de
l'IA}\label{les-diffuxe9rents-outils-de-lia}}

Cette section vise à présenter différents outils de l'intelligence
artificielle aux lecteurs. Ce qui est important de comprendre ici, et
pour faire suite à la section précédente, c'est que les outils présenté
ici risque d'avoir changé entre le moment d'écrire ce chapitre et le
moment où les lecteurs liront le chapitre. Certaines fonctionnalités
pourraient toujours être les mêmes, certaines pourraient avoir été
améliorées et d'autres pourraient être complètement nouvelles. Par
conséquent, l'objectif ici est de présenter certaines utilités et
fonctionnalités de l'IA, et surtout d'inciter les lectuers à développer
leurs propres capacités réflexives quant à leur utilisation de l'IA.
Cette technologie offre de nouvelles opportunités, mais elle apportea
aussi son lot d'enjeux et de questions dont il vaut mieux s'y intéresser
afin de développer une utilisation saine et intégre de ces outils.

Trois catégories d'outils seront présentées: les « \emph{Large Language
Models} » (LLM), les assistants de traduction ainsi que les assistants
de revue de littérautre.

\hypertarget{les-llms}{%
\subsection{1. Les LLMs}\label{les-llms}}

\hypertarget{les-assistants-de-traduction}{%
\subsection{2. Les Assistants de
Traduction}\label{les-assistants-de-traduction}}

\hypertarget{les-assistants-de-revue-de-littuxe9rature}{%
\subsection{3. Les Assistants de Revue de
littérature}\label{les-assistants-de-revue-de-littuxe9rature}}

Dans cette section, deux principaux outils seront présentés:
\emph{research rabbit} et \emph{ellicit}. Il s'agit de deux ressources
gratuites en ligne qui facilite le début d'une revue des écrits,
notamment lorsque l'on ne connaît pas beaucoup la littérature sur un
sujet et/ou sur un champs d'étude. Ces deux outils sont d'ailleurs
compatibles aves \emph{Zotero}, qui fut présenté au chapitre 4 de ce
livre.

Débutons avec \emph{Ellicit}. Avec l'intelligence artificielle, cet
outils suggère des articles et des livres scientifiques à partir d'une
question de recherche préliminaire, ou à partir de concepts.
L'application offre une version gratuite, mais qui est toutefois limité
dans le nombre de crédits disponibles mensuellement afin de faire des
recherches. Par conséquent, il faut l'utiliser avec parcimonie. Afin de
démontrer l'utilité de ce logiciel, utilision un exemple concret.

\begin{figure}

{\centering \includegraphics{images/chapitre9_image2.png}

}

\caption{\label{fig-ellicit}Menu d'accueil d'Ellicit}

\end{figure}

Par exemple, disons que je m'intéresse à la question suivante: Qu'est-ce
que la démocratie? Cependant, je ne sais par où commencé pour me faire
une tête sur le sujet. \emph{Ellicit} offre une solution à ce problème.
La Figure~\ref{fig-ellicit} correspond au menu d'accueil du site web.
Une fois là, je n'aurais qu'à écrire ma question dans la case «
\emph{Ask a research question} ». Les résultats générés sont représentés
dans la Figure~\ref{fig-results1} et Figure~\ref{fig-results2}

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{images/chapitre9_image3.png}

}

\caption{\label{fig-results1}Court résumé du sujet}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{images/chapitre9_image4.png}

}

\caption{\label{fig-results2}Suggestions de lectures}

}

\end{minipage}%

\end{figure}

Le logiciel offre donc un moyen intéressant afin de faire un premier «
filtrage » de la littérature sur un sujet, tout en fournissant un résumé
des sources suggérées à partir desquelles nous pouvons juger de sa
pertinence en fonction de nos besoins.

Cependant, il est fortement recommandé d'utiliser le résumé produit par
le logiciel, dans la Figure~\ref{fig-results1}, et ceux de chaque
suggestion, dans la Figure~\ref{fig-results2}, à titre indicatif et
uniquement pour notre propre réflexion. En d'autres termes, \textbf{ne
jamais faire un copier-coller de ces résumés afin de les inclures dans
notre travail de recherche}. Ce logiciel doit impérativement être
accompagné d'une utilisation intègre de la littérature. Une bonne
utilisation de ce logiciel devrait se limiter à trouver des articles
et/ou des livres scientifiques selon nos besoins, que nous consulterons
par la suite pour rédiger notre revue des écrits et trouver des
références supplémentaires.

Un autre outils soffre à nous afin de trouver des références
supplémentaires: \emph{Research Rabbit}. Ce logiciel est gratuit, mais
demande de se créer un compte afin de pouvoir utiliser ses services. Une
fois que j'ai lu plusieurs articles et/ou chapitres de livre, et que
j'ai incorporé les documents dans Zotero, je peux importer mon fichier
contenant mes références dans \emph{Research Rabbit}. Reprenons notre
exemple de la démocratie de tout à l'heure.

\textbf{Mettre les exemples dans Research Rabbit et montrer les
différentes fonctionnalités, notamment le mapping}

\hypertarget{quelle-est-la-place-du-chercheur-maintenant}{%
\section{Quelle est la place du chercheur
maintenant?}\label{quelle-est-la-place-du-chercheur-maintenant}}

Avec l'avènement de l'IA, il est tout à fait raisonnable de se demander
quelle est la place du chercheur aujourd'hui. L'avenir du chercheur
est-il en danger? Pourrait-on assister au développement des sciences
sociales sans chercheur humain derrière? Si tel est le cas, est-ce que
ça ne constituerait pas un paradoxe important? Est-ce que la machine est
mieux placée pour comprendre la réalité du monde sociale, ainsi que ses
mécanismes, que l'humain? D'une part, certains pensent que l'IA risque
de générer des « laboratoires autonomes » (Hanchen Wang, Tianfan Fu,
Yuanqi Du, Wenhao Gao, Kexin Huang, et al. 2023, 55). Il n'est pas
difficile d'imaginer un monde où tout le processus scientifique, de la
conception jusqu'à la communication, serait fait par l'IA. Le chercheur
perdrait ainsi sa profession, et se limiterait à n'être qu'une partie de
l'auditoire vers qui les résultats sont présentés.

Bien que nous n'en sommes pas encore là, il est important de réfléchir
aux différents enjeux qui se poseraient dans une telle situation. Par
exemple, étant donné que l'IA est, pour l'instant, très opaque dans tout
le processus qui le mène de l'intrant vers le résultat, comment
pourrait-on s'assurer que la machine a pris toutes les précautions
nécessaires pour respecter les différents enjeux éthiques? A-t-elle eu
le consentement libre et éclairé de tous les participants? De plus, quel
est le niveau de confiance que nous pouvons avoir envers des résultats
dont on ne connait pas le processus qui y a mené? Sur cette dernière
question, l'une des caractéristiques fondamentales de la science est la
reproductibilité des protocoles scientifiques (King, Keohane, and Verba
2021; Bourgeois 2021). Toutes les recherches doivent présenter, d'une
manière très précise, comment les données ont choisi, comment elles ont
été collectées et comment elles ont été analysées. Le terme transparence
est très important, et résume l'esprit de toute communication
scientifique. Or, c'est une limite importante de l'IA en ce moment: nous
n'avons pas accès aux processus qui mènent de l'intrant à l'extrant
(Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, et al.
2023, 56). Cette opacité nous empêche d'évaluer correctement la validité
d'un protocole scientifique qui serait réalisée par l'IA. La conséquence
logique de cette situation est de toujours rester vigilant et de
questionner constamment les informations fournies par le robot
conversationnel. L'utilisation exclusive de tel logiciel, sans se
référer à des sources scientifiques qui ont été publiées par des revues
scientifiques ou des éditeurs scientifiques, ne devrait jamais être une
option. Il n'est pas seulement question d'intégrité et d'éthique, mais
de toute la conception de ce que constitue un savoir scientifique.
L'utilisation de ChatGPT, par exemple, pour produire un savoir
quelconque met au défi nos conceptions épistémologiques. Générer des
textes entiers avec l'aide de l'IA ne devrait donc pas être considéré.

Comme nous le voyons, l'avènement de l'IA en recherche amène des
questions et des réflexions épistémologiques\footnote{L'épistémologie
  est l'une des branches de la philosophie des sciences qui s'intéresse
  au savoir et à la connaissance. De manière générale, et très
  simplifié, l'une des questions fondamentales de l'épistémologie et de
  se questionner quant à savoir ce qu'est un savoir qui serait
  scientifique.} et méthodologiques\footnote{La méthodologie fait
  référence à la branche de la philosophie des sciences qui s'intéresse
  aux outils de collecte et d'analyse de données.} importantes. Ces
questions sont cruciales et doivent être abordées le plus rapidement
possible. En tant que chercheur, nous devons nous questionner par
rapport à l'utilisation de ces nouvelles technologies. Il faut être
proactif et initier les réflexions sur la place du chercheur maintenant.

En ce sens, il faut élargir notre perspective et dépasser la question
quant à savoir si la profession de chercheur va disparaître ou non. En
fait, il faut se questionner par rapport au rôle du chercheur. Qu'est-ce
qu'il devient à l'ère de l'intelligence artificielle? Comment se
transforme-t-il? Pour l'instant, il est encore difficile de répondre et
d'anticiper efficacement ce qui arrivera. Il n'en reste pas moins pour
autant qu'initier la réflexion est nécessaire. Encore une fois, les
universités sont des lieux privilégiés pour avoir ce genre de réflexion,
tant venant des étudiants que des enseignants.

\hypertarget{lia-en-sciences-sociales}{%
\section{L'IA en sciences sociales}\label{lia-en-sciences-sociales}}

Malgré que l'utilisation de l'IA en sciences sociales soit relativement
récente, il y a déjà quelques recherches qui se sont penchées sur son
utilisation dans un contexte scientifique. Tout d'abord, la recherche de
Peterson et al. (2021), qui s'intéresse à comprendre comment les
individus prennent des décisions, utilise l'IA afin de traiter une
importante quantité de donnée rapidement. Leur base de données est
environ trente fois plus importante que celles des études précédentes.
De plus, ils ont programmé différentes théories afin de vérifier
laquelle correspondait le mieux aux patterns présents dans les données.
L'utilisation de l'IA dans cette recherche est très intéressante
puisqu'elle permet de tester des théories existantes sur un vaste
ensemble de données, qui auraient pris beaucoup plus de temps si cela
avait été réalisé « manuellement ».

Ensuite, la recherche de Park et al. (2023) utilise l'IA pour créer des
``generative agents'' qui interagissent entre eux reproduisant des
comportements individuels et collectifs humains. Leur objectif et d'en
faire des proxys1 pour étudier le comportement humain. Leur modèle prend
la forme d'une simulation interactive, similaire au jeu vidéo Sims. Bien
que ce ne soit pas encore au point, cette application de l'IA
permettrait de tester des prototypes de systèmes sociaux ainsi que des
théories (Park et al. 2023).

Similairement, la recherche de Argyle et al. (2023) utilise ChatGPT
comme proxy pour étudier l'opinion publique certains sous-groupes
sociaux. Leur objectif est de démontrer que ChatGPT peut être utilisé,
avec un bon niveau de confiance, pour explorer et tester des hypothèses
qui seraient coûteuses si c'était fait avec des sujets humains. En
effet, le déploiement d'un sondage est toujours une sorte pari; ils sont
en général relativement coûteux, et il est difficile de prévoir si les
résultats obtenus seront significatifs. ChatGPT permettrait donc de
faire un prétest afin d'améliorer et de corriger un questionnaire avant
de le déployer sur des sujets humains.

Malgré ces avantages, il est important de souligner quelques limites
quant à l'utilisation de l'IA en sciences sociales. La limite la plus
importante est que l'IA ne remplace en aucun cas un être humain. Étant
des chercheurs en sciences sociales et sciences humaines, nous
dénaturerions nos disciplines si l'on se limitait à l'IA pour répondre à
nos questions. Par conséquent, un test réalisé avec l'IA ne pourra
jamais avoir le même niveau de confiance qu'une recherche réalisée avec
des humains. Il est donc important de limiter l'utilisation de l'IA à
des fins exploratoires. Une autre limite importante, mentionnée par Park
et al. (2023) dans leur recherche, est que l'IA peut être sujette à des
hallucinations. En d'autres termes, l'IA peut fabriquer des informations
de toute pièce (Weise and Metz 2023). Cela pose un sérieux problème
quant à la fiabilité des informations générées par l'IA. C'est pour
cette raison, notamment, que nous devons toujours rester vigilants. Les
conséquences d'attribuer une valeur scientifique à des informations qui
seraient fausses seraient graves. La science ne vise pas à construire
une compréhension fictive de la réalité. Une fausse compréhension des
interactions et des dynamiques sociales pourrait avoir des conséquences
vraiment importantes sur le bien-être de nos sociétés. Restons
vigilants.

\hypertarget{conclusion-5}{%
\section{Conclusion}\label{conclusion-5}}

En guise de conclusion, nous souhaitons lancer une dernière réflexion un
peu plus philosophique, mais tout aussi importante: le transhumanisme,
«{[}\ldots{]} un mouvement international, culturel et intellectuel,
prônant l'usage des sciences et des techniques dans le but d'améliorer
la condition humaine, notamment par l'augmentation des capacités
physiques et mentales des êtres humains » (Forestier and Ansermet 2021).
La question principale qui se pose est: est-ce vraiment raisonnable
d'augmenter les capacités humaines au-delà de leur limite biologique?
Surtout, que deviendront les sciences sociales et humaines si le
principal sujet d'étude, soit l'humain, n'est plus tout à fait lui-même?
Peut-on faire des sciences sociales sur des sujets qui sont que
partiellement humain? Ce qui rend les sciences sociales aussi
intéressantes et pertinentes c'est peut-être, justement, parce que
l'humain n'a pas d'essence. Pour reprendre les mots de Sartre (1996,
26), « l'existence précède l'essence ». En d'autres termes, et comme
Sartre l'explique, l'humain n'existe pas pour remplir une fonction
prédéterminée, contrairement à un crayon qui a été conçu pour remplir la
tâche spécifique d'écrire. C'est dans cette liberté, et c'est par
l'expérience, que l'humain se construit et se définit. Notre existence
en tant que scientifique du monde social et humain est possible que
grâce à cette condition fondamentale : l'absence d'une essence qui
précède l'existence. Sinon, à quoi bon étudier le monde social s'il a
une fonction prédéterminée et fixe, dans lequel les humains n'auraient
aucune agentivité?

Cependant, avec l'arrivée de l'IA et ce désir de constamment repousser
les limites humaines, ne sommes-nous pas en train de prouver à Sartre
qu'il a tort? En fait, l'humain se serait imposé une essence, soit celle
d'être une pièce indispensable pour faire fonctionner les rouages du
système capitaliste. Face à la conception de la « croissance infinie »
qui est entretenue par ce système, l'être humain a besoin de quelque
chose pour « briser » ses capacités qui elles sont limitées : «
{[}\ldots{]} un remodelage technoscientifique et biomédical des corps et
des vies, dans leur matérialité biologique même, afin d'adapter les
individus au régime capitaliste globalisé de l'accélération. » (Dévédec
2021, 100). L'IA sera-t-elle une pièce de plus vers la réalisation de
cet idéal transhumaniste?

Le propos ici n'est pas d'encourager les lecteurs à ne pas utiliser
l'IA. En fait, nous souhaitons tout simplement appeler à une certaine
prudence. Le risque que cette technologie nous pose est d'en devenir
dépendant, voir trop (Park et al. 2023). Bien qu'elle offre de nombreux
avantages, elle a aussi le piège d'offrir beaucoup de raccourcis et de
nuire à plusieurs de nos capacités intellectuelles et physiques. Il faut
donc développer un jugement critique dans notre utilisation de l'IA. Se
poser des questions sur notre utilisation personnelle de ces outils
constitue, dès aujourd'hui, le fondement des chercheurs en sciences
sociales numériques.

Toutefois, à moins que la recherche porte directement sur ChatGPT, nous
déconseillons fortement d'utiliser uniquement ce que le robot
conversationnel fournit comme réponse. Reprenons l'exemple du
totalitarisme. Dans ce cas, bien que la définition fournie soit
relativement adéquate, nous recommandons d'aller consulter des sources
académiques afin de trianguler la définition générée, d'une part, et
surtout de présenter une définition qui est reconnue par les pairs
scientifiques. Pour ce faire, il est possible d'utiliser ChatGPT. Nous
pouvons lui demander de nous fournir 5 livres qui portent sur le sujet.
À partir de ces recommandations, nous pouvons nous référer directement à
ces ouvrages et débuter notre revue des écrits. Surtout, à moins que ce
ne soit que pour vous faire une idée du contenu, n'utilisez pas ChatGPT
pour résumer un livre et utiliser le résumé pour votre travail ! Cette
pratique constitue une forme de plagiat. Aller consulter les ouvrages
directement plutôt que d'utiliser les définitions fournies par ChatGPT
fait partie des bonnes pratiques. Développer un esprit de synthèse est
fondamental pour chaque étudiant.e universitaire, ainsi que pour les
futurs chercheur.euse.s. Commencez dès maintenant à développer ces
capacités plutôt que de demander à ChatGPT de le faire à votre place.

\hypertarget{utilisation-du-package-openai}{%
\section{Utilisation du package
OpenAI}\label{utilisation-du-package-openai}}

\hypertarget{installation-et-chargement-du-package}{%
\subsection{Installation et chargement du
package}\label{installation-et-chargement-du-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"openai"}\NormalTok{) }\DocumentationTok{\#\# au besoin}
\FunctionTok{library}\NormalTok{(openai)}
\end{Highlighting}
\end{Shaded}

\hypertarget{configuration-de-lapi}{%
\section{Configuration de l'API}\label{configuration-de-lapi}}

Procurez vous une clé API sur le site d'OpenAI. Soyez conscient que vous
aurez besoin d'une carte de crédit pour vous inscrire et que
l'utilisation de l'API est payante. Renseignez-vous sur les modèles
disponibles et leurs frais d'utilisation. En date de la publication du
livre, le modèle de tarification d'OpenAi est de charger un prix
spécifique par 1000 tokens. Le prix des Tokens en entrée est moins élevé
que celui des tokens en sortie.

Lorsque vous aurez votre clé API, utilisez le package usethis pour la
configurer dans votre environnement R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"usethis"}\NormalTok{) }\DocumentationTok{\#\# au besoin}
\NormalTok{usethis}\SpecialCharTok{::}\FunctionTok{edit\_r\_environ}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Ajoutez la ligne suivante à votre fichier \texttt{.Renviron}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OPENAI\_API\_KEY}\OtherTok{=}\NormalTok{inserez}\SpecialCharTok{{-}}\NormalTok{votre}\SpecialCharTok{{-}}\NormalTok{cle}\SpecialCharTok{{-}}\NormalTok{api}\SpecialCharTok{{-}}\NormalTok{ici}\SpecialCharTok{{-}}\NormalTok{sans}\SpecialCharTok{{-}}\NormalTok{guillemets}
\end{Highlighting}
\end{Shaded}

\hypertarget{utilisation-de-lapi}{%
\section{Utilisation de l'API}\label{utilisation-de-lapi}}

La fonction principale du package openai est create\_chat\_completion().
Elle prend en entrée le modèle que vous souhaitez utiliser ainsi que le
message que vous souhaitez envoyer au modèle en format list. Voici un
modèle d'utilisation de la fonction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chat\_prompt }\OtherTok{\textless{}{-}} \FunctionTok{create\_chat\_completion}\NormalTok{(}
    \AttributeTok{model =} \StringTok{"gpt{-}3.5{-}turbo"}\NormalTok{,}
    \AttributeTok{messages =} \FunctionTok{list}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}
            \StringTok{"role"} \OtherTok{=} \StringTok{"system"}\NormalTok{,}
            \StringTok{"content"} \OtherTok{=} \StringTok{"You are a helpful assistant."}
\NormalTok{        ),}
        \FunctionTok{list}\NormalTok{(}
            \StringTok{"role"} \OtherTok{=} \StringTok{"user"}\NormalTok{,}
            \StringTok{"content"} \OtherTok{=} \StringTok{"Please do the following:"}\NormalTok{)}
\NormalTok{        )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Le résultat de votre requête sera contenu dans l'objet chat\_prompt
formatté en JSON. Vous pouvez accéder aux variables de la même façon
qu'un dataframe normal. Le contenu de la réponse sera dans
\texttt{chat\_prompt\$choices\$content}.

Utiliser chatgpt de cette façon ouvre plein de possibilités. Appliquer
des instructions sur un ensemble d'observations à l'aide de boucles,
utiliser des fonctions pour générer des messages et les appliquer à
travers d'autres API, analyser des sites webs en temps réel en scraping
avec des paquets tels rvest, etc. Ce sera à vous de réfléchir aux
possibilités que vous souhaitez explorer.

\hypertarget{notes}{%
\section{Notes}\label{notes}}

\begin{itemize}
\item
  Il est possible d'accéder aux statistiques d'utilisation de token dans
  \texttt{chat\_prompt\$usage\$prompt\_tokens} et
  \texttt{chat\_prompt\$usage\$completion\_tokens}. Vous pouvez donc
  calculer le coût de votre requête en fonction du modèle que vous
  utilisez.
\item
  Ne pas oublier d'inclure .Renviron dans votre gitignore pour ne pas
  vous faire voler votre clé API.
\item
  Il est possible de créer des images avec la fonction
  create\_image(``Inserez votre texte ici'')
\item
  Il est possible d'effectuer du speech-to-text avec la fonction
  create\_transcription() et create\_translation()
\item
  Plus de documentation est disponible au
  https://irudnyts.github.io/openai/
\item
  Plus de fonctionalités sont disponibles en python mais le package R
  est suffisant pour la plupart des utilisations.
\end{itemize}

\hypertarget{annexe-1}{%
\section{Annexe 1}\label{annexe-1}}

Bonnes pratiques d'utilisation Harvard. 2023. « Guidelines for using
ChatGPT and other Generative AI tools at Harvard ».
https://provost.harvard.edu/guidelines-using-chatgpt-and-other-generative-ai-tools-harvard

MIT. 2023. « Advice and responses from faculty on ChatGTP and
A.I.-assisted writing ».
https://cmsw.mit.edu/advice-and-responses-from-faculty-on-chatgpt-and-a-i-assisted-writing/

\hypertarget{annexe-2}{%
\section{Annexe 2}\label{annexe-2}}

Q : Qu'est-ce que le totalitarisme? R : Le totalitarisme est un système
politique et idéologique caractérisé par un contrôle absolu et
centralisé du gouvernement sur tous les aspects de la vie publique et
privée d'une société. Dans un régime totalitaire, le gouvernement exerce
un pouvoir autoritaire et oppressif, limitant sévèrement les libertés
individuelles, supprimant les droits de l'homme, et éliminant ou
réprimant toute opposition ou critique.

Bonnes pratiques d'utilisation Harvard. 2023. « Guidelines for using
ChatGPT and other Generative AI tools at Harvard ».
https://provost.harvard.edu/guidelines-using-chatgpt-and-other-generative-ai-tools-harvard

MIT. 2023. « Advice and responses from faculty on ChatGTP and
A.I.-assisted writing ».
https://cmsw.mit.edu/advice-and-responses-from-faculty-on-chatgpt-and-a-i-assisted-writing/

\hypertarget{annexe-2-1}{%
\section{Annexe 2}\label{annexe-2-1}}

Q : Qu'est-ce que le totalitarisme? R : Le totalitarisme est un système
politique et idéologique caractérisé par un contrôle absolu et
centralisé du gouvernement sur tous les aspects de la vie publique et
privée d'une société. Dans un régime totalitaire, le gouvernement exerce
un pouvoir autoritaire et oppressif, limitant sévèrement les libertés
individuelles, supprimant les droits de l'homme, et éliminant ou
réprimant toute opposition ou critique.

Les caractéristiques clés du totalitarisme comprennent :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Contrôle gouvernemental absolu :} Le gouvernement exerce un
  contrôle total sur tous les aspects de la vie, y compris l'économie,
  la culture, l'éducation, les médias, la religion et les activités
  sociales.
\item
  \textbf{Idéologie dominante :} Le régime totalitaire est basé sur une
  idéologie spécifique qui est imposée à la population. Cette idéologie
  est souvent promue comme la seule vérité et toute forme de pensée
  dissidente est sévèrement réprimée.
\item
  \textbf{Culte de la personnalité :} Les dirigeants du régime
  totalitaire sont souvent présentés comme des figures quasi-divines ou
  héroïques, et un culte de la personnalité est créé autour d'eux pour
  renforcer leur autorité.
\item
  \textbf{Répression politique :} Toute forme d'opposition politique est
  réprimée de manière brutale. Les opposants sont souvent emprisonnés,
  torturés, voire exécutés. La surveillance et la dénonciation sont
  courantes pour empêcher toute critique du régime.
\item
  \textbf{Propagande intensive :} Le gouvernement contrôle les médias et
  utilise la propagande pour manipuler l'opinion publique, répandre sa
  vision du monde et justifier ses actions.
\item
  \textbf{Absence de droits individuels :} Les droits individuels tels
  que la liberté d'expression, la liberté de la presse, le droit de
  réunion et d'association sont souvent supprimés au profit de la
  suprématie de l'État.
\item
  \textbf{Contrôle de l'éducation :} L'éducation est utilisée pour
  propager l'idéologie du régime et façonner la mentalité des citoyens
  conformément à ses objectifs.
\item
  \textbf{Utilisation de la terreur :} La terreur et l'intimidation sont
  utilisées pour maintenir le contrôle sur la population. Les purges et
  les arrestations arbitraires sont fréquentes pour maintenir un climat
  de peur.
\end{enumerate}

Des exemples historiques de régimes totalitaires incluent l'Allemagne
nazie sous Adolf Hitler, l'Union soviétique sous Joseph Staline, et la
Chine sous Mao Zedong. Le totalitarisme est généralement considéré comme
une forme extrême et oppressive de gouvernement, en contraste avec les
systèmes démocratiques qui mettent l'accent sur les libertés
individuelles, la séparation des pouvoirs et la participation citoyenne.

\{\textless\textless{} pagebreak \textgreater\textgreater\}

\bookmarksetup{startatroot}

\hypertarget{uxe0-voir}{%
\chapter{À voir}\label{uxe0-voir}}

\hypertarget{duxe9fis-et-enjeux-uxe9thiques-de-lia-focus-sur-chatgpt}{%
\section{Défis et enjeux éthiques de l'IA, focus sur
ChatGPT}\label{duxe9fis-et-enjeux-uxe9thiques-de-lia-focus-sur-chatgpt}}

En tant qu'étudiant, professeur, professionnel ou chercheur, il faut se
questionner par rapport à notre propre utilisation des différents outils
de l'IA. Tout d'abord, il est important de comprendre qu'il y a plus de
questions que de réponses pour l'instant. Face à ce constat, nous
n'avons pas la prétention d'être en mesure de cibler toutes les
questions qui émergent actuellement, et encore moins d'avoir les
réponses. Cependant, cela ne justifie pas pour autant d'être passif.
Nous devons essayer d'être réflexifs et critiques dans la limite de nos
capacités et de nos connaissances. En ce sens, les étudiants et les
enseignants doivent aussi être proactifs en entreprenant les démarches
nécessaires ainsi qu'en s'engageant dans la réflexion.

Dans un premier temps, un bon usage de ces outils débute avec une bonne
réflexion quant à leur utilisation. Par conséquent, les universités
constituent des endroits privilégiés pour favoriser les discussions et
les réflexions quant à l'utilisation de ces technologies. D'ailleurs,
certaines universités se sont déjà dotées de lignes directrices quant à
l'utilisation de robots conversationnels et de l'IA
générative\footnote{Sur ce sujet, nous recommandons de consulter les
  ressources dans la section Bonnes pratiques d'utilisation de l'IA dans
  l'Annexe 1 du chapitre.}. Nous sommes d'avis que chaque université
aurait intérêt à se doter de tel document, afin de fournir les
ressources nécessaires aux étudiants ainsi qu'aux membres du corps
professoral dans leur utilisation de ces outils. L'accompagnement et
l'encadrement dans l'exploration et l'utilisation de l'IA nous
paraissent être une bonne stratégie à adopter afin de permettre le
développement de bonnes pratiques.

Actuellement, un enjeu majeur, surtout avec les robots conversationnels,
est le plagiat. Notre but ici est de présenter les différentes
ressources qui s'offrent aux lecteurs pour qu'ils puissent développer
les bonnes pratiques d'utilisation de ces outils tout en restant
intègres. Pour ce faire, nous présenterons dans les paragraphes suivants
les bonnes pratiques de citation selon l'American Psychological
Association et The Chicago Manual of Style. Avant cela, il est important
de spécifier que notre point de vue, et les propos tenus dans ce livre,
ne remplace en aucun cas les règlements disciplinaires et/ou codes de
conduite établis par une institution académique quelconque. Par
conséquent, nous invitons fortement les lecteurs à consulter les sites
web de ces associations, à se référer aux personnels appropriés pour
toutes questions relatives à l'utilisation de texte généré par l'IA
ainsi qu'à tout document relatif au plagiat produit par l'institution
académique fréquentée. Passons maintenant à la présentation de ces
manuels de style.

L'American Psychological Association (APA) encourage les utilisateurs à
être transparents quant à leur utilisation de logiciels tel que ChatGPT.
Lorsqu'utilisés, les chercheurs devraient spécifier clairement qu'ils
ont utilisé le logiciel, en plus de décrire comment ils ont utilisé le
logiciel, quel prompt ont-ils utilisé et quel a été le résultat en plus
de fournir des extraits textuels (McAdoo 2023). Il est recommandé de
documenter chaque utilisation. Se créer un document qui inclue la date
d'utilisation, la question demandée ainsi que la réponse obtenue doit
faire partie des bonnes pratiques de chacun. Ces éléments peuvent être
ajoutés en annexe au besoin (McAdoo 2023). Sans grande surprise, il est
impératif de citer l'auteur lorsqu'on utilise des idées qui ne sont pas
les nôtres. Dans le cas de robots conversationnels, nous devons citer le
développeur (McAdoo 2023). Par exemple, pour une citation de ChatGPT, il
faudra référer à OpenAI, soit le développeur du logiciel.

Utilisons un exemple concret afin d'illustrer le tout. Supposons que je
m'intéresse au concept du totalitarisme, mais que je n'ai pas une
compréhension claire de ce que ça signifie. Je pourrais utiliser ChatGPT
pour me fournir une définition du concept. Si je souhaite l'inclure dans
mon travail, je procèderais de la façon suivante : nous avons utilisé
ChatGPT afin de nous donner une définition du totalitarisme. Pour ce
faire, nous lui avons posé la question suivante : « Qu'est-ce que le
totalitarisme? ». Le logiciel nous a fourni la définition suivante : «
Le totalitarisme est un système politique et idéologique caractérisé par
un contrôle absolu et centralisé du gouvernement sur tous les aspects de
la vie publique et privée d'une société. Dans un régime totalitaire, le
gouvernement exerce un pouvoir autoritaire et oppressif, limitant
sévèrement les libertés individuelles, supprimant les droits de l'homme,
et éliminant ou réprimant toute opposition ou critique. » (OpenAI 2023)

En bibliographie, la référence serait insérée comme suit, et ensuite
j'irai insérer la question et la réponse complète en annexe\footnote{Référez-vous
  à l'Annexe 2 pour un exemple.}.

OpenAI. (2023). ChatGPT (Version du 3 août 2023) {[}Large Language
Model{]}. https://chat.openai.com/auth/login (exemple tiré de McAdoo
2023)

Quant au manuel de style Chicago, il est recommandé de mentionner et
d'expliquer que nous avons utilisé ChatGPT pour accomplir une certaine
tâche dans notre texte. Toutefois, comme le lien généré lors de
l'utilisation individuelle de ChatGPT n'est pas public et ne peut pas
être consulté par les autres, il n'est pas recommandé d'insérer la
référence en bibliographie.

Toutefois, à moins que la recherche porte directement sur ChatGPT, nous
déconseillons fortement d'utiliser uniquement ce que le robot
conversationnel fournit comme réponse. Reprenons l'exemple du
totalitarisme. Dans ce cas, bien que la définition fournît soit
relativement bonne, nous recommandons d'aller consulter des sources
académiques afin de trianguler la définition qui a été générée, d'une
part, et surtout de présenter une définition qui est reconnue par les
pairs scientifiques. Toutefois, pour ce faire, il est possible
d'utiliser ChatGPT. Nous pouvons lui demander de nous fournir 5 livres
qui portent sur le sujet. À partir de ces recommandations, nous pouvons
nous référer directement à ces ouvrages et débuter notre revue des
écrits. Surtout, à moins que ce ne soit que pour vous faire une idée du
contenue, n'utilisez pas ChatGPT pour résumer un livre et utiliser le
résumé pour votre travail! Cette pratique constitue une forme de
plagiat. Aller consulter les ouvrages directement plutôt que d'utiliser
les définitions fournies par ChatGPT fait partie des bonnes pratiques.
De plus, développer un esprit de synthèse est fondamental pour chaque
étudiant universitaire, ainsi que pour les futurs chercheurs. Commencez
dès maintenant à vous pratiquer pour développer ces capacités. Ne
demandez pas à ChatGPT de le faire à votre place.

\bookmarksetup{startatroot}

\hypertarget{kit-de-duxe9marrage-pour-les-sciences-sociales-numuxe9riques}{%
\chapter{Kit de démarrage pour les sciences sociales
numériques}\label{kit-de-duxe9marrage-pour-les-sciences-sociales-numuxe9riques}}

\begin{center}

Marc-Antoine Rancourt, Flavie Lachance, Justine Béchard, William Poirier

\end{center}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

Alors que les chapitres précédents se sont consacrés à la présentation
théorique et pratique des sciences sociales numériques, le présent
chapitre s'efforcera à aider le lecteur à faire sens de la grande
quantité d'information que contient l'ouvrage et à commencer sa propre
démarche d'apprentissage numérique. À ce propos, le titre n'est pas
anodin. Il est possible de voir ce chapitre comme l'endroit où débuter
son apprentissage des nouveaux outils numériques présentés dans cet
ouvrage.

En plus d'aider de lecteur à installer et à utiliser plusieurs des
outils présentés précedemment, ce chapitre offre également des conseils
afin d'éviter de communs pièges lors de l'apprentissage de nouveaux
outils. Le corps du texte du présent chapitre est divisé en trois
parties en lien avec le niveau de difficulté associés à l'apprentissage
des différents outils numériques: débutant, intermédiaire et avancé. De
plus, chaque partie termine par une liste de pièges à éviter et qui est
associé au niveau d'apprentissage qui lui est propre.

\hypertarget{duxe9butant}{%
\section{Débutant}\label{duxe9butant}}

Cette section peut s'adresser à tous les lecteurs, mais elle vise
particulièrement ceux et celles qui débutent leurs parcours dans le
monde numérique. Elle se divisera en deux sous-sections portant chacune
sur un type d'outils nécessaires à la pratique des sciences sociales
numériques. La première partie couvre le langage de programmation R et
l'environnement de programmation qui lui est associé, RStudio. Elle
commence par présenter au lecteur comment télécharger R et RStudio
correctement. Ensuite, elle offre différentes ressources afin de pouvoir
apprendre à utiliser R et naviguer RStudio adéquatement. La seconde
section guide le lecteur dans l'installation de \LaTeX, puis présente
des ressources utiles à son utilisation.

\hypertarget{r-et-rstudio}{%
\subsection{R et RStudio}\label{r-et-rstudio}}

Le chapitre 4 du présent livre introduit le langage de programmation R,
ses avantages, ses inconvénients et son utilité. Cette partie du livre
se veut un complément à ce chapitre visant à aider le lecteur à se
familiariser avec le langage R. La première étape pour mener à bien
cette tâche est de le télécharger. R fonctionne sur une grande quantité
de plateformes, notamment sur les différentes distributions de Windows,
de macOS et de Linux. Pour le télécharger, il faut commencer par se
rendre sur le \emph{Comprehensive R Archive Network}. Il est fortement
conseillé d'utiliser ce site puisqu'il est l'endroit principal où se
trouve la plupart des choses liées à R. Il est aussi facile
d'utilisation, très bien documenté et régulièrement mis à jour. Il est
possible d'accéder le site à partir de l'adresse suivante :
https://cran.r-project.org/. Dans le haut de la page, un lien pour
chacun des trois systèmes d'exploitation principaux -- Windows, macOS et
Linux -- redirige vers la page appropriée. Il suffit de cliquer sur la
distribution présente sur l'ordinateur et l'installer.

Il existe plusieurs ressources qu'un nouvel utilisateur de R peut
consulter afin de se familiariser avec le langage de programmation.
Comme pour de nombreuses autres choses, il est possible d'apprendre en
faisant des exercises en ligne et en suivant des tutoriels. Les
tutoriels sont une manière tout autant enrichissante que divertissante
pour se familiariser avec certains outils du monde numérique, notamment
le langage R. DataCamp est un des sites les plus populaires, mais aussi
les plus complets et accessibles, à cette fin. Datacamp est une
plateforme d'apprentissage en ligne proposant des exercices interactifs
en ligne, axé principalement sur l'analyse et les données. La plateforme
est facile à utiliser et contient une grande quantité de module sur les
différents aspects du langage R. En date de 2023, selon le site de
l'entreprise, DataCamp contient plus de 140 cours interactifs portant
sur le langage R, en plus de contenir plus de 100 tutoriels variés. La
plateforme a également un forum où il est possible d'interagir avec les
autres utilisateurs et poser des questions. Un autre avantage de
Datacamp est son accessibilité. En plus des cours accessibles
gratuitement, Datacamp offre des réductions sur les abonnements pour les
étudiants et les enseignants. Il est également possible pour les
enseignants d'obtenir gratuitement un compte Datacamp Entreprise à des
fins d'utilisation éducative. D'autres plateformes dans le même genre
que DataCamp existent. Les alternatives les plus populaires sont
CodeAcademy et Coursera. Bien qu'il n'y ait rien de mal à proprement
parler à utiliser ces alternatives, nous conseillons DataCamp car cette
plateforme apparaît être la meilleure pour l'apprentissage de nouveaux
outils numériques destinés à la science des données en sciences
sociales.

En plus de ces différentes ressources en ligne, plusieurs excellent
ouvrages d'introduction au langage de programmation R sont disponibles.
Les auteurs du présent ouvrage proposent trois ouvrages aux nouveaux
utilisateurs. Le livre \emph{R For Data Science} d'Hadley Wickham et de
Garret Grolemund est un classique en la matière et est un excellent
guide sur les différents éléments à apprendre pour bien maîtriser le
langage de programmation R ainsi que son utilité en sciences sociales
computationnelles. Publié pour la première fois en 2017, il a été
réédité une seconde fois en 2023 et est disponible gratuitement en ligne
à l'adresse suivante: https://r4ds.hadley.nz/. Un second ouvrage qui
peut être utile est \emph{The Book of R: A First Course in Programming
and Statistics} de Tilman Davies. Le \emph{Book of R}, comme certains
l'appelle, est un guide complet et convivial pour les débutants en R.
Contrairement à d'autres ressources du même genre, il ne demande aucune
expérience en programmation et à peine quelques connaissances de base en
mathématiques afin de commencer à apprendre à utiliser R efficacement
pour l'analyse statistique. Finalement, le troisième ouvrage,
\emph{Beyond Spreadsheets with R: A beginner's guide to R and RStudio}
de Jonathan Caroll, montre comment prendre des données brutes et les
transformer pour les utiliser dans des calculs, des tableaux, des
graphiques, etc. Le livre a pour but d'aider le lecteur à bâtir des
bases solides afin de lui permettre d'analyser et visualiser des données
de toutes sortes à l'aide de R. Un avantage qu'a cet ouvrage sur les
autres est qu'il est également un guide d'apprentissage pour RStudio. Ce
livre, ainsi que celui de Tilman Davies, sont disponibles sur Amazon et
autres plateformes de vente de livres.

En ce qui a trait à RStudio, outre que le livre de Tilman Davies
susmentionné, il existe plusieurs ressources en ligne pouvant aider les
utilisateurs débutants. DataCamp a un long tutoriel dédié à apprendre à
utiliser l'environnement de programmation RStudio. Le site web de
RStudio a également une section sur les bases de l'environnement de
programmation. Plusieurs institutions offrent également de courts
tutoriels par le biais de pages webs ou de vidéos accessibles sur des
plateformes telles que Youtube. Autant pour R que pour RStudio, une
grande quantité de ressources d'aide sont disponibles en ligne. Des
forums tels que Stack Overflow et la section discussion de GitHub
peuvent être très utiles pour avoir une réponse rapide à une question
technique.

\hypertarget{les-alternatives-uxe0-word-les-langages-de-balisage}{%
\subsection{Les alternatives à Word : les langages de
balisage}\label{les-alternatives-uxe0-word-les-langages-de-balisage}}

Le chapitre 5 du présent ouvrage introduit les langages de balisage
\LaTeX et Markdown, leurs historiques ainsi que leurs avantages et
inconvénients. Cette partie du livre se veut un complément au chapitre 5
visant à aider le lecteur à se familiariser avec les principaux langages
de balisage utilisés en sciences sociales numériques. La première étape
pour afin d'utiliser les langages de balisage \LaTeX et Markdown est de
les télécharger. Bien que LaTeX peuvent être téléchargé à différents
endroits, celui qui est généralement considéré comme étant officiel est
le \emph{Comprehensive TEX Archive Network} (CTAN). Le CTAN est
disponible à l'adresse suivante : https://www.ctan.org/. Le CTAN est le
lieu central pour tout ce qui touche \LaTeX. CTAN compte actuellement
6483 packages, auxquels ont contribués 2946 usagers. La plupart des
packages sont gratuits et peuvent être téléchargés et utilisés
immédiatement. Les deux distributions TeX les plus couramment utilisées
sont TEX Live et MiKTeX. Bien que leurs avantages aient été relativement
différents dans le passé, en 2023, un nouvel usager ne verrait
probablement pas la différence. Les deux distributions peuvent être
téléchargées à partir du site Web du CTAN et utilisées rapidement. Elles
sont supportés sur tous les versions principales de Linux, MacOS et
Windows.

Plusieurs ressources pour \LaTeX peuvent se trouver en ligne. \emph{The
LaTeX Project} est une excellente source d'information sur le langage de
balisage LaTeX. Cette ressource est disponible à l'adresse suivante :
https://www.latex-project.org/. Elle contient une grande quantité de
documentation à l'usage de nouveaux usagers, des nouvelles sur les mises
à jours \LaTeX ainsi qu'une liste de publications portant sur \LaTeX et
son utilisation. Le site Web contient également plusieurs liens utiles
vers GitHub ainsi que des informations pertinentes sur l'historique du
langage. D'autres ressources sous formes de livres et d'articles
existent et peuvent être très utiles aux nouveaux usagers de \LaTeX. Le
livre \emph{Learning LATEX} par David Griffiths et collègues, publié en
1997, tient encore la route aujourd'hui. On peut y apprendre les bases
du langages \LaTeX qui n'ont pas changé depuis des décennies. En 2017,
un autre livre important pour les nouveaux usagers est paru. \emph{LaTeX
in 24 Hours: A Practical Guide for Scientific Writing}, écrit par Dilip
Datta, offre un aperçu de LaTeX aux académiques peu d'expérience
technique préalable. Le livre présente aux lecteurs des exercices et des
exemples simples et compréhensibles pour se familiariser rapidement avec
le langage. Une grande quantité d'autres ressources sont disponibles sur
le Web.

Comme noté au chapitre 5, Markdown est un langage de balisage servant à
ajouter des éléments de formatage à des documents en texte brut. Il n'a
pas à être téléchargé, il est natif à la plupart des systèmes
d'opérations régulièrement utilisés. Afin de commencer à l'utiliser, il
suffit d'ajouter des éléments de formatage Markdown à un fichier texte
brut à l'aide d'un éditeur de texte. Il est également possible
d'utiliser l'une des nombreuses applications Markdown pour les systèmes
d'exploitation macOS, Windows, Linux, iOS et Android. Il existe
également plusieurs applications Web spécialement conçues pour écrire en
Markdown. L'environnement de programmation RStudio, susmentionné ainsi
que présenté au chapitre 4, est un excellent endroit pour écrire en
Markdown. RStudio permet également à l'utilisateur d'écrire en RMarkdown
ainsi qu'en Quarto.

Comme \LaTeX, Markdown est un langage de balisage qui existe depuis
longtemps et dont les bases n'ont pas particulièrement changées dans les
dernières décennies. Ainsi, il existe une panoplie de ressources en
ligne aidant les nouveaux usagers à se familiariser avec le langage. Le
créateur de Markdown, John Gruber, a un site Web contenant les bases du
langage Markdown. Ce dernier peut être trouvé à l'adresse suivante :
https://daringfireball.net/projects/markdown/. Le site Web
\emph{Markdown Tutorial}, comme son nom l'indique, est un site Web open
source qui permet d'essayer Markdown dans le navigateur Web. Il est
disponible à l'adresse suivante : https://www.markdowntutorial.com/. En
format livre, le livre \emph{The Markdown Guide} par Matt Cone est une
courte mais complète introduction aux bases de Markdown. De la mise en
forme à la publication, ce livre contient une grande quantité
d'information et de ressources qui peuvent être même utiles aux usagers
avancés. Comme pour \LaTeX, une grande quantité d'autres ressources sont
facilement accessibles sur le Web.

\hypertarget{piuxe8ges-pour-usagers-duxe9butants}{%
\subsection{Pièges pour usagers
débutants}\label{piuxe8ges-pour-usagers-duxe9butants}}

Beaucoup de nouvelles informations ont été présentées jusqu'à présent
dans ce livre. Il est normal de se sentir dépassé et de ne pas tout
comprendre. En fait, il aurait été surprenant qu'un lecteur qui débute
l'aventure numérique ait tout compris. L'important est de garder une
attitude propice à l'apprentissage et se rappeler que rien de ceci n'est
inatteignable. C'est au tout début du parcours que se trouve le premier
des pièges pour débutants : \textbf{croire qu'il sera trop difficile
d'apprendre, que c'est un objectif impossible à atteindre}. Même les
auteurs de ce livre ont, un jour, commencés par faire \emph{Hello
World!} dans la console de RStudio. Le premier piège est souvent lié à
un autre piège qui frappe les codeurs débutants : \textbf{la peur de
demander de l'aide}. Il faut garder à l'esprit qu'une grande quantité
des utilisateurs des outils présentés dans le présent livre sont passés
par l'incertitude du début et la crainte du jugement des autres. N'ayez
pas peur de poser vos questions, c'est comme cela qu'on apprend.

Une autre catégorie de pièges pour débutants pour les débutants concerne
la pratique des connaissances nouvellement acquises. Les pièges pour
débutants de cette catégorie sont au nombre de trois. Tout d'abord, on
retrouve \textbf{la croyance qu'il est possible d'apprendre sans
pratiquer}. Bien que cela puisse être possible pour quelques personnes
ayant une mémoire phénoménale, la réalité est qu'il sera difficile pour
le lecteur moyen de retenir l'information contenue dans ce livre et dans
les exercices sans pratiquer les nouvelles notions. Le second piège de
cette catégorie est lié à ce dernier point : DataCamp -- où il y a des
indices et du code déjà écrit -- ne forme pas à lui seul des codeurs. Il
faut faire attention à \textbf{ne pas rester pris dans une boucle
infinie de tutoriels}. Faire des tests avec des projets personnels aide
à assimiler les nouvelles connaissances en plus d'être plus intéressant.
Le troisième pièges pour débutants de cette catégorie est de \textbf{ne
pas être constant dans ses apprentissages}. Avec les exercices comme
Datacamp, il est facile d'apprendre très rapidement. Toutefois, les
apprentissages peuvent se perdre aussi rapidement qu\textquotesingle ils
ont été acquis. Il est donc important de suivre une certaine continuité
et même parfois de refaire certains exercices afin de se rafraichir la
mémoire pour s'assurer de bien comprendre les connaissances de base.

Le dernier pièges pour débutants est le suivant : \textbf{ne pas
construire des bases solides avant d'aller plus loin}. Plusieurs
nouveaux codeurs, excités par les nouveaux outils qu'ils apprennent,
oublient qu'il est primordial de bien comprendre les éléments de base de
la programmation et de la gestion de données avant de se lancer dans des
projets plus complexes. Bien qu'il ne soit pas requis de connaître la
mécanique pour conduire une automobile, il est tout de même parfois
utile -- voir nécessaire -- de comprendre comment entretenir celle-ci.

\hypertarget{intermuxe9diaire}{%
\section{Intermédiaire}\label{intermuxe9diaire}}

Tout comme la section précédente, cette section peut tout à fait être
utile à tous les lecteurs. Cependent, elle vise particulièrement ceux et
celles qui ont commencé leurs parcours dans le monde numérique, mais qui
cherchent à complémenter leur parcours d'outils qui leur facilitera la
vie. Cette section se divise en deux sous-sections portant chacune sur
un type d'outils nécessaires à la pratique des sciences sociales
numériques. La première partie couvre des ressources de gestion
bibliographique. Elle commence par présenter au lecteur comment
télécharger correctement les différents outils. Ensuite, elle offre
différentes ressources afin de pouvoir apprendre les utiliser
adéquatement. La seconde section guide le lecteur l'apprentissage de la
visualisation graphique en R, puis présente des ressources utiles à sa
pratique.

\hypertarget{la-gestion-des-ruxe9fuxe9rences}{%
\subsection{La gestion des
références}\label{la-gestion-des-ruxe9fuxe9rences}}

Le chapitre 6 du présent livre porte sur la gestion des références. Il
présente deux ressources de gestion bibliographique largement utilisés :
Zotero et BibLaTex. Cette partie du livre se veut un complément au
chapitre 6. La première étape pour afin d'utiliser deux outils de
gestion des références susmentionnés est de les télécharger. En ce que
concerne Zotero, celui-ci peut être téléchargé à partir du site Web
officiel de l'outil à l'adresse suivante :
https://www.zotero.org/download/. Le site Web de Zotero contient une
grande quantité de documentation pouvant aidant les différents niveaux
d'utilisateurs. Ladite documentation aide notamment à apprendre à créer
des bibliographies, comment travailler en collaboration et les
différents \emph{plug-ins} et \emph{add-ons} qui peuvent être utilisés.
Parmi ceux-ci, on retrouve notamment \emph{Better BibTex} -- présenté au
chapitre 6 -- qui aide grandement la gestion de données bibliographiques
pour ceux utilisant Zotero et un langage de balisage tel que \LaTeX ou
Markdown. \emph{Better BibTex} peut être téléchargé à l'adresse suivante
: https://retorque.re/zotero-better-bibtex/installation/. Le site Web de
\emph{Better BibTex} contient également une foule de documentation
aisant l'utilisation de l'outil.

La littérature académique sur les différents outils aidant à la gestion
des données bibliographiques indiquent que beaucoup des professionnels
de différents milieux utilisent Zotero. Il est aussi possible de
constater que plusieurs préfèrent Zotero à d'autres options connues
telles que EndNotes, RefWorks et Mendeley. Behera et Meher (2022) notent
13 avantages de Zotero sur sa compétition. Parmi ceux-ci, on retrouve
notamment la capacité de Zotero à supporter une grande quantité de
formats d'écriture, son caractère \emph{open source} et gratuit, son
dévelopement constant par de nombreux chercheurs assurant sa qualité, en
plus de son utilité pour le travail collaboratif. Ivey et Crum (2018)
comparent les quatre options les plus populaires pour la gestion des
données bibliographiques : Zotero, EndNotes, RefWorks et Mendeley. Ils
notent pour leur part que Zotero était l'outil le plus précis pour la
capture et la transformation de données Web en notices bibliographiques.
Selon eux, Zotero créerait les notices bibilographiques les plus
exactes. Parmi les options populaires grautuites Zotero et Mendeley,
Gilmour et CobusKuo (2011) concluent que Zotero est la meilleure. Les
auteurs notent notamment la précision des bibliographiques extractées et
le peu d'erreurs produites par l'outil. Winslow et al.~(2016) montrent,
pour leur part, que Zotero peut aussi être utilisé à des fins
pédagogiques. L'outil peut aider les chercheurs avec la littérature sur
un sujet en plus d'avoir un impact concret sur les pratiques
scientifiques.

Pour sa part, BibLaTeX n'a pas besoin d'être téléchargé puisque c'est un
package qui vient avec essentiellement toutes les principales
distributions TeX. Il ne suffit que d'utiliser la commande
\texttt{\textbackslash{}usepackage\{biblatex\}} pour y avoir accès.
C'est une des trois alternatives populaires afin de citer avec LaTeX,
les autres étant \emph{natbib} et \emph{bibtex}. Biblatex est
généralement considéré comme étant l'option \LaTeX moderne pour traiter
les données bibliographiques. Plusieurs apprécient grandement son
interface simple et flexible. BibLaTeX supporte aussi mieux des langages
autres que l'anglais en comparaison avec les deux autres options.

Le susmentionné CTAN contient une grande quantité de documentation
portant sur BibLaTeX à l'adresse suivante :
https://www.ctan.org/pkg/biblatex. De plus, Philip Kime, Moritz Wemheuer
et Philipp Lehman ont mis en ligne un excellent guide intitulé « The
biblatex Package » portant sur le package. Les auteurs ont compilé en
357 pages toutes les possiblités du package avec des exemples précis. Le
document contient un guide pour les usagers de plus de 100 pages, en
plus d'un historique des versions du package depuis 2012. Le document
est à jour pour l'année 2023, et il est constamment mis à jour par les
auteurs.

\hypertarget{visualisation-graphique-en-r}{%
\subsection{Visualisation graphique en
R}\label{visualisation-graphique-en-r}}

Le chapitre 7 du présent ouvrage porte sur la visualisation graphique en
R, les différentes options pour visualiser des données et une discussion
concrète de la manière de faire avec \emph{base R}, \emph{lattice} et
\emph{gpglot2}. Pour plusieurs raisons présentées aux chapitre 7,
\emph{ggplot2} est présentement considéré comme étant la meilleure
alternative de visualisation graphique avec R en sciences sociales
numériques. Comme BibLaTeX, \emph{ggplot2} n'a pas besoin d'être
directement téléchargé à partir du Web. Il peut être téléchargé seul
avec la fonction R \emph{install.packages} ou avec le super-package
\emph{tidyverse}. Il ne suffit que le l'appeler avec la fonction
\emph{library} pour y avoir accès une fois que cela est fait.

Outre le contenu du chapitre 7, une grande quantité de ressources est
disponible en ligne afin de guider les utilisateurs de R dans leurs
démarches de visualisation graphique. Youtube, StackOverflow et les
autres sites Webs contenant des tutoriels ou de l'aide à la
programmation sont remplis de guides pouvant être utiles. Toutefois,
nous considérons que certaines ressources sont plus utiles que d'autres
afin de bien comprendre les bases de \emph{ggplot2}. Le susmentionné
livre \emph{R For Data Science} d'Hadley Wickham et de Garret Grolemund
contient une importante section sur la création de graphiques avec
\emph{ggplot2} en R. Cette section du livre contient également des
exercices pour le lecteur. Hadley Wickham a beaucoup travaillé sur ce
sujet. Il a écrit plusieurs livres et articles sur le langage de
programmation R et la visualisation graphique. Parmi lesdits écrits,
l'article de Wickham (2010) est une ressource édifiante pour le
compréhension de la \emph{grammar of graphics}, à la base du projet
\emph{ggplot}. Wickham et collègues ont également écrit \emph{ggplot2:
Elegant Graphics for Data Analysis (3e)} qui porte également sur la
philosophie derrière la création et l'utilisation de \emph{ggplot2}.
D'autres ouvrages, plus pratiques, ont été publié sur la visualisation
graphique en R avec \emph{ggplot2}. C'est notamment le cas du livre de
Robert Kabacoff sur l'apprentissage de la visualisation graphique avec
\emph{ggplot2} « Modern Data Visualization with R ». Il est disponible
en ligne à l'adresse suivante : https://rkabacoff.github.io/datavis/. Il
couvre une grande quantité d'analyses statistiques possibles à faire en
R et les meilleures manières de les présenter visuellement. Chaque type
de graphique est accompagné de plusieurs exemples et d'une base de
données. Finalement, DataCamp, la plateforme d'apprentissage mentionnée
précédemment dans le présent chapitre, contient plusieurs tutoriels
permettant à de nouveaux utilisateurs de se pratiquer avec
\emph{ggplot2}. DataCamp contient trois cours complets, pour 12 heures
de tutoriels, sur la visualisation en R avec \emph{ggplot2}, allant du
niveau de débutant jusqu'au niveau avancé.

\hypertarget{piuxe8ges-pour-usagers-intermuxe9diaires}{%
\subsection{Pièges pour usagers intermédiaires
:}\label{piuxe8ges-pour-usagers-intermuxe9diaires}}

À la suite des différents exercices et lectures complètés dans le cadre
de cette familiarisation aux sciences sociales numériques, le lecteur
doit s'assurer d'éviter certains pièges qui se dressent sur le chemin
des chercheurs de niveau intermédiaire. Le premier d'entre eux est
\textbf{vouloir apprendre plusieurs langages et n'en maîtriser aucun}.
Plusieurs chercheurs, lorsqu'ils commencent à maîtriser de nouveaux
outils, s'emballent et souhaitent en apprendre davantage. C'est une
bonne chose, mais il faut faire attention à ne pas apprendre que
quelques éléments de plusieurs langages de programmation, et plutôt en
maîtriser un. Comme le dit un diction populaire, « qui trop embrasse mal
étreint ».

Un second pièges pour usagers intermédiaires auquel de jeunes chercheurs
sont la proie est \textbf{coder en n'utilisant pas un style et une
planification cohérente et constante}. En n'adoptant pas un style
standard -- ou en n'utilisant pas le plus souvent le même style -- il
peut devenir difficle pour les autres et pour soi-même de se retrouver
dans le code. Cela peut causer d'importants problèmes de compréhension
ou des problèmes techniques. Il est rare qu'un même code ne serve qu'une
seule fois. Il est donc de viser à ce que le code qu'on produit soit
compréhensible, transférable et -- idéalement -- optimisé. Un autre
pièges pour usagers intermédiaires s'inscrivant dans la lignée du
précédent est \textbf{écrire du code mais ne pas le commenter}.
Commenter son code contribue grandement à la transférabilité et la
pérennité de son travail. Bien que la fonction d'une section de code
peut sembler évidente pour son créateur le jour où elle est produite,
elle ne le sera pas nécessairement pour d'autres ou pour lui-même dans
le futur.

Le troisième pièges pour usagers intermédiaires concerne l'utilisation
des packages R. De nombreux packages r sont disponibles sur Internet.
Dans certaines situations, l'utilisation de ceux-ci peut représenter un
gain de temps et résoudre certains problèmes spécifiques. Toutefois,
pour des tâches relativement simples, utiliser un package r risque
d'ajouter une complexité inutile. En effet, comprendre un package R et
l'adapter en fonction de son projet peut être long et laborieux. Il est
donc souvent beaucoup plus efficace d'écrire son propre code plutôt que
d'utiliser un package R.

Le dernier piège se dressant sur le chemin d'un chercheur de niveau
intermédiaire est de \textbf{croire qu'il a suffisamment de
connaissances et ne pas sortir de sa zone de confort}. L'apprentissage
de techniques plus complexes demande de sortir de sa zone de confort et
de se confronter à l'inconnu. Cela demande également d'accepter qu'on ne
connait pas tout et qu'il y aura des échecs et des frustrations. C'est
ainsi qu'un chercheur intermédiaire peut dépasser ses limites et devenir
un chercheur de niveau avancé.

\hypertarget{avancuxe9}{%
\section{Avancé}\label{avancuxe9}}

\hypertarget{le-travail-collaboratif}{%
\subsection{Le travail collaboratif}\label{le-travail-collaboratif}}

Le chapitre 8 du présent ouvrage présente principalement trois types
d'outils complémentaires pour le travail collaboratif : les outils de
communications, les outils de gestion de versions et les outils
d'entreposage de données. Cette section est complémentaire au chapitre 8
qui présente les différents outils et leurs plusieurs avantages et
inconvénients. Elle vous présentera des façons d'en apprendre plus sur
ces trois types d'outils et de commencer à les utiliser.

L'outil de communication priorisé par les auteurs du chapitre 8 est
Slack, une plateforme de communication utilisée dans le monde
professionnel et académique où se trouvent différents salons de
conversations et qui possède une tonne de fonctionnalités telles que les
appels de groupes et le partage de documents. Gofine et Clark (2017)
notent que Slack est un outil spécialement utile pour la communiaction,
la planification et le partage de document en milieu académique. En ce
qui concerne la documentation existente, le site Web de Slack contient
plusieurs pages portant sur les différentes options de l'application
ainsi que des liens vers des vidéos réalisées par leur équipe afin
d'aider les utilisateurs à utiliser l'outil de manière efficace. La
chaîne Youtube de la compagnie contient plusieurs vidéos aidant les
nouveaux usagers à naviguer l'application. Du côté livre, le court
ouvrage de Jonathan Miller \emph{Getting Started with Slack: A Quick
Start Guide for Everyone}, disponible sur le Web en format Kindle, est
une bonne introduction à l'utilisation de Slack. L'auteur présente
différentes techniques afin d'optimiser les communications d'équipe et
les bonnes pratiques en terme d'utilisation de l'outil.

En ce qui a trait aux outils de gestion de versions, Git est le logiciel
priorisé. Git est souvent utilisé à partir de GitHub, la plateforme Web
principale pour l'outil. Le chapitre 8 présente Git et GitHub ensemble
puisqu'ils sont utilisés de manière complémentaire par de nombreux
usagers. Pour beaucoup d'utilisateurs, Git est la façon dont ils
intéragissent avec le site Web GitHub, où se trouve leur données, à
partir de la ligne de commande de leur ordinateur. D'autres téléchargent
directement leurs fichiers à partir du Web et utilisent Git à partir de
l'interface GitHub. Bien que certains utilisent Git mais pas GitHub,
nous pensons que pour commencer, il est souhaiter d'utiliser les deux
outils ensemble. Outre le contenu du chapitre 8, afin de commencer à
utiliser Git et GitHub, il est possible de consulter le site Web de
GitHub, qui contient beaucoup de documentation, et sur lequel il faut
d'ailleurs se créer un compte. Plus d'une dizaine de livres sur Git et
GitHub sont accessibles sur le Web. Après les avoir consulté, aucun
d'entre eux ne semblent particulièrement pertinent à recommander ici.
Rien ne semble battre l'abondance de ressources disponible sur le site
de GitHub. Il contient même plusieurs dizaines de tutoriels afin d'en
apprendre davantage sur les différentes commandes Git et les façons
d'utiliser GitHub. Il est possible de trouver cela à l'adresse suivante
: https://skills.github.com/. Consulter les différentes pages de
documentation sur GitHub et commencer à utiliser l'outil semble être la
meilleure manière d'apprendre.

Finalement, pour ce qui concerne les outils d'entreposage de données,
les options proposées au chapitre 8 sont les plateformes Dropbox et
Amazon Web Services (AWS). Commencer à utiliser Dropbox est assez
intuitif. C'est similaire à l'arborescence de fichier d'un ordinateur
commun, mais en ligne. Et puisque Dropbox peut être téléchargé et
installé comme d'autres applications sur les différentes versions de
Windows, de MacOS et de Linux, il est possible de synchroniser un
dossier Dropbox en local avec son Dropbox en ligne. La seule chose qu'il
faut pour utiliser Dropbox, c'est de se créer un compte. Il existe des
forfaits payants pour avoir accès a plus d'espace de stockage, mais à la
base, c'est gratuit. L'application peut être téléchargé sur le site Web
de Dropbox. En ce qui concerne AWS, il existe plusieurs solutions et
autant de forfaits qui y sont associés. Le site de AWS a énormément de
documentation sur les différents services offerts et les fonctionalités
de l'outil. Ils ont leur guide d'apprentissage et plus de 100 tutoriels.
Leur documentation est très complète et il y a peu de raisons de
recommander un des quelques livres en ligne qui existent et qui sont
essentiellement des copiés-collés de leur site Web. AWS a aussi un
chatbot qui peut répondre à vos question en temps réel et il apparaît
assez efficace.

\hypertarget{outils-dintelligence-artificielle-1}{%
\subsection{Outils d'intelligence
artificielle}\label{outils-dintelligence-artificielle-1}}

Le chapitre 9 porte sur les différents outils liés à l'utilisation de
l'intelligence artificielle. Plus spécifiquement, il présente le site
d'OpenAI, qui contient notamment le fameux ChatGPT. Une grande quantité
de publications, autant académiques que non académiques, sont parues
dans les dernières années sur les différentes facettes de l'intelligence
artificielle. La présente section fait état de quelques ressources
utiles choisies pour leur pertinence et leur utilité dans
l'apprentissage des outils d'intelligence artificielle d'OpenAI.

Le site Web de la compagnie OpenAI contient une grande quantité
d'information sur l'utilisation de leurs différents outils. Leur site
Web contient notamment un index des différents articles portant sur
leurs produits. Il est possible de consulter ledit index à l'adresse
suivante : https://openai.com/research. Tous les ouvrages disponibles ne
sont pas nécessairement publiés dans des journaux revues par les pairs.
Il faut faire attention avec les sources qu'on utilise qui sont
disponibles gratuitement en ligne et dont les auteurs n'ont
potentiellement pas eu à rendre de compte à des pairs avant leur
publication.

L'ouvrage \emph{ChatGPT for Higher Education and Professional
Development: A Guide to Conversational AI} de Stephen Atlas est
intéressant puisqu'il discute des différents mythes entourant ChatGPT
avant de se lancer dans comment l'utiliser à différentes fins. Il offre
une vision édifiante de son utilisation dans le monde universitaire et
répond à de nombreuses questions fréquemment posées. Le livre de Sinan
Ozdemir intitulé \emph{Quick Start Guide to Large Language Models:
Strategies and Best Practices for Using ChatGPT and Other LLMs} est
aussi considéré comme un bon outil pour apprendre comment fonctionne
ChatGPT. Il offre aux lecteurs une grande quantité d'information utiles
et touche également notamment aux meilleures pratiques à utiliser lors
de l'utilisation d'outils tels que ChatGPT. Il présente également se qui
se trouve derrière de tels outils et comment en tirer le plus possible.
Enfin, plusieurs publications récentes traitent de trucs et astuces afin
de bien commencer à utiliser. C'est notamment le cas de Lubiana et
al.~(2023) qui présente 10 choses à considérer lors de l'utilisation des
récentes versions de ChatGPT, et de Patton et al.~(2023) qui écrivent
sur les opportunités et les défis de ChatGPT en sciences sociales
numériques.

\hypertarget{piuxe8ges-pour-usagers-avancuxe9s}{%
\subsection{Pièges pour usagers
avancés}\label{piuxe8ges-pour-usagers-avancuxe9s}}

L'un des pièges importants à éviter lorsque le chercheur se retrouve à
un niveau avancé est la peur de partager son code. Ceci est spécialement
vrai pour ceux pour qui l'apprentissage c'est fait en silo. Estimant
leur code comme étant une propriété intellectuelle, plusieurs chercheurs
développent cette réticence et refusent de partager le fruit de leur
labeur. Toutefois, partager son code comporte de nombreux avantages, non
seulement pour les autres membres de la communauté, mais également pour
le chercheur lui-même. D'un côté, cela permet de recevoir des
rétroactions de la part d'autres chercheurs et développeurs. Cette
collaboration peut donc grandement contribuer à l'amélioration de son
code. De plus, partager son code représente une opportunité
d'apprentissage pour les autres membres de la communauté qui peuvent
s'en inspirer pour développer leurs compétences ou même le réutiliser
dans leur propre projet. Cette transparence et cette collaboration sont
donc avantageuses pour tous les partis.

Le deuxième piège pour usagers avancés duquel le chercheur avancé doit
se méfier est de laisser le parfait devenir l'ennemi du bien. Certains
chercheurs ont parfois tendance à être perfectionnistes et à perdre du
temps et de l'énergie sur des détails mineurs qui n'ont, en fin de
compte, aucune retombée majeure sur la qualité globale du projet, comme
chercher à optimiser son code de manière excessive. Se soucier de la
qualité de son travail est essentiel, mais le chercheur avancé doit
également apprendre à savoir quand s'arrêter.

Après avoir consacré de nombreuses heures et travaillé d'arrache-pied
pour acquérir des connaissances avancées en codage, le chercheur a de
quoi être fière. Toutefois, il doit se méfier de l'ultime pièges pour
usagers avancés : manquer d'empathie et de compréhension envers les
nouveaux utilisateurs. Certains chercheurs de niveau avancé peuvent
oublier qu'ils ont déjà été, eux aussi, des débutants. Il faut éviter de
prendre pour acquis certaines connaissances de base qui peuvent sembler
très simple pour un chercheur avancé, mais très complexe pour un
débutant. Soutenir les nouveaux utilisateurs dans leur apprentissage
avec patients et empathie permet une meilleure transmission des
connaissances.

\hypertarget{conclusion-6}{%
\section{Conclusion}\label{conclusion-6}}

Le but du présent chapitre était d'offrir des informations
additionnelles aux chapitres précédents dans le but de faciliter
l'apprentissage des différents outils de travail en sciences sociales
numériques présentés. Nous avons ici mis l'accent sur deux choses afin
d'aider le lecteur. Premièrement, nous avons offert plusieurs ressources
à consulter afin d'accéder et d'apprendre à utiliser les outils mis de
l'avant. Les différents outils ont été classé selon la difficulté perçue
et relative de leur apprentissage. Nous avons classé les outils ainsi
puisque c'est généralement l'ordre dans laquelle les practiciens des
sciences sociales numériques les apprennent. Afin de faciliter
l'expérience d'apprentissage des lecteurs, nous avons pré-sélectionné
certaines lectures considérées pertinentes dans le but d'éviter aux
lecteurs une surchage d'information. Nous avons choisi des ressources
qui sont facilement accessibles en ligne et qui sont gratuites pour la
plupart.

Ensuite, à la fin de chaque section, nous avons présenté les différents
pièges associés aux divers niveaux d'apprentissage. Notre expérience
indique que plusieurs comportements et attitudes sont liés à certains
stades d'apprentissage. Évidemment, certains peuvent survenir plus tôt
ou tard que d'autres. L'important est d'être au courant des mauvais plis
qu'il est possible d'adopter et de les adresser en amont. Plusieurs des
pièges susmentionnés suivent de nombreux profesionnels pendant longtemps
et compliquent leur travail individuel et collaboratif. Un practicien
des sciences sociales numériques avertit en vaut deux.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-adcock_collier01}{}}%
Adcock, Robert, and David Collier. 2001. {``Measurement {Validity}: {A
Shared Standard} for {Qualitative} and {Quantitative Research}.''}
\emph{American Political Science Review} 95 (3): 529--46.
\url{https://doi.org/10.1017/S0003055401003100}.

\leavevmode\vadjust pre{\hypertarget{ref-allaire22}{}}%
Allaire, JJ. 2022. \emph{Announcing {Quarto}, a New Scientific and
Technical Publishing System}.
\url{https://posit.co/blog/announcing-quarto-a-new-scientific-and-technical-publishing-system/}.

\leavevmode\vadjust pre{\hypertarget{ref-alpaydin_bach14}{}}%
Alpaydin, Ethem, and Francis Bach. 2014. \emph{Introduction to {Machine
Learning}}. 3e ed. Massachusetts: MIT Press.
\url{https://ebookcentral.proquest.com/lib/umontreal-ebooks/detail.action?docID=3339851}.

\leavevmode\vadjust pre{\hypertarget{ref-argyle_etal23}{}}%
Argyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler,
Christopher Rytting, and David Wingate. 2023. {``Out of {One}, {Many}:
{Using Language Models} Ot {Simulate Human Samples}.''} \emph{Political
Analysis} 31 (3): 337--51. \url{https://doi.org/10.1017/pan.2023.2}.

\leavevmode\vadjust pre{\hypertarget{ref-ballhausen19}{}}%
Ballhausen, Miriam. 2019. {``Free and {Open Source Software Licenses
Explained}.''} \emph{Computer} 52 (6): 82--86.
\url{https://doi.org/10.1109/MC.2019.2907766}.

\leavevmode\vadjust pre{\hypertarget{ref-bengio23}{}}%
Bengio, Yoshua. 2023. {``One of the "Godfathers of {AI}" Airs His
Concerns.''} \emph{The Economist}, July 21, 2023.
\url{https://www.economist.com/by-invitation/2023/07/21/one-of-the-godfathers-of-ai-airs-his-concerns}.

\leavevmode\vadjust pre{\hypertarget{ref-beraud07}{}}%
Béraud, Cyrille. 2007. {``Le Logiciel Libre, Une Cause Nationale, Une
Opportunité Pour Le {Québec}.''} FACIL.
\url{https://facil.qc.ca/files/2008-11-06_Assnat_0.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-bertolini20}{}}%
Bertolini, Andrea. 2020. {``Artificial {Intelligence} and {Civil
Liability}.''} Policy Department for Citizen\textquotesingle s Rights
and Constitutional Affairs. Brussel: European Union.
\url{https://www.europarl.europa.eu/RegData/etudes/STUD/2020/621926/IPOL_STU(2020)621926_EN.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-bessen02}{}}%
Bessen, James. 2002. {``What {Good Is Free Software}?''} In
\emph{Government {Policy} Toward {Open Source Software}}, edited by
Robert W. Hahn, 12--33. Brookings Institution Press.
\url{https://www.jstor.org/stable/10.7864/j.ctvbd8kmv.5}.

\leavevmode\vadjust pre{\hypertarget{ref-bourgeois21}{}}%
Bourgeois, Isabelle. 2021. {``Qu'est-Ce Que La Recherche Sociale?''} In
\emph{Recherche Sociale. {De} La Problématique à La Collecte Des
Données}, 7e ed., 1--14. Boisbriand: Presses de l'Université du Québec.

\leavevmode\vadjust pre{\hypertarget{ref-bremmer_suleyman23}{}}%
Bremmer, Ian, and Mustafa Suleyman. 2023. {``The {AI Power Paradox}.''}
\emph{Foreign Affairs}, 2023.

\leavevmode\vadjust pre{\hypertarget{ref-broca13}{}}%
Broca, Sébastien. 2013. \emph{Utopie Du Lociel Libre. {Du} Bricolage
Informatique à La Réinvention Sociale.} Lyon: Le passage clandestin.

\leavevmode\vadjust pre{\hypertarget{ref-couture14}{}}%
Couture, Stéphane. 2014. {``Les logiciels libres, un an plus tard.''}
Institut de recherche et d'informations socioéconomiques. 2014.
\url{https://iris-recherche.qc.ca/blogue/secteur-public-et-communautaire/les-logiciels-libres-un-an-plus-tard/}.

\leavevmode\vadjust pre{\hypertarget{ref-couture20}{}}%
---------. 2020. {``Free and {Open Source Software}.''} In \emph{The
{Handbook} of {Peer Production}}, 153--68. John Wiley \& Sons, Ltd.
\url{https://doi.org/10.1002/9781119537151.ch12}.

\leavevmode\vadjust pre{\hypertarget{ref-devedec21}{}}%
Dévédec, Nicolas Le. 2021. {``"{Sans} Limites": Une Critique Politique
Et Écologique Du Transhumanisme Et de Son Monde.''} \emph{Cahiers
Société} 3: 99--122. \url{https://doi.org/10.7202/1090180ar}.

\leavevmode\vadjust pre{\hypertarget{ref-devedzic22}{}}%
Devedzic, Vladan. 2022. {``Identity of {AI}.''} \emph{Discover
Artificial Intelligence} 2 (23).
\url{https://doi.org/10.1007/s44163-022-00038-0}.

\leavevmode\vadjust pre{\hypertarget{ref-encyclopaediabritannica23}{}}%
Encyclopaedia Britannica. 2023. \emph{{LaTeX}}.
\url{https://www.britannica.com/technology/LaTeX-computer-programming-language}.

\leavevmode\vadjust pre{\hypertarget{ref-forestier_ansermet21}{}}%
Forestier, François, and François Ansermet. 2021. {``Le
Transhumanisme.''} In \emph{La {Dévoration Numérique}}, 19--54. Paris:
Odile Jacob.
\url{https://www.cairn.info/la-devoration-numerique--9782415000240-page-19.htm}.

\leavevmode\vadjust pre{\hypertarget{ref-fortunato_galassi21}{}}%
Fortunato, Laura, and Mark Galassi. 2021. {``The Case for Free and Open
Source Software in Research and Scholarship.''} \emph{Philosophical
Transactions of the Royal Society A: Mathematical, Physical and
Engineering Sciences} 379 (2197): 20200079.
\url{https://doi.org/10.1098/rsta.2020.0079}.

\leavevmode\vadjust pre{\hypertarget{ref-gaudeul07}{}}%
Gaudeul, Alex. 2007. {``Do {Open Source Developers Respond} to
{Competition}? {The LATEX Case Study}.''} \emph{Review of Network
Economics} 6 (2). \url{https://doi.org/10.2202/1446-9022.1119}.

\leavevmode\vadjust pre{\hypertarget{ref-goldfarb96}{}}%
Goldfarb, Charles F. 1996. \emph{The {Roots} of {SGML} -- {A Personal
Recollection}}. \url{http://www.sgmlsource.com/history/roots.htm}.

\leavevmode\vadjust pre{\hypertarget{ref-hameed23}{}}%
Hameed, Sharqa. 2023. \emph{{HTML History} \textbar{} {Explained}}.
\url{https://linuxhint.com/html-history/}.

\leavevmode\vadjust pre{\hypertarget{ref-hanchenwangtianfanfuyuanqiduwenhaogaokexinhuang_etal23}{}}%
Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming
Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima
Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli,
Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks,
Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veličković, Max
Welling, Linfeng Zhang, Connor W. Coley, and Yoshua Bengio, \& Marinka
Zitnik. 2023. {``Scientific Discovery in the Age of Artificial
Intelligence.''} \emph{Nature} 620: 47--60.
\url{https://www.nature.com/articles/s41586-023-06221-2}.

\leavevmode\vadjust pre{\hypertarget{ref-harari23}{}}%
Harari, Yuval Noah. 2023. {``Yuval {Noah Harari} Argues That {AI} Has
Hacked the Operating System of Human Civilisation.''} \emph{The
Economist}, April 28, 2023.
\url{https://www.economist.com/by-invitation/2023/04/28/yuval-noah-harari-argues-that-ai-has-hacked-the-operating-system-of-human-civilisation}.

\leavevmode\vadjust pre{\hypertarget{ref-hughes13}{}}%
Hughes, Jonathan. 2013. {``Cameos, Supporting Roles and Stars: Citation
and Reflection in the Context of Initial Teacher Education.''}
\emph{Educational Research} 55 (1): 16--30.
\url{https://doi.org/10.1080/00131881.2013.767023}.

\leavevmode\vadjust pre{\hypertarget{ref-ibm23}{}}%
IBM. 2023a. {``What Are {Neural Networks}? \textbar{} {IBM}.''} 2023.
\url{https://www.ibm.com/topics/neural-networks}.

\leavevmode\vadjust pre{\hypertarget{ref-ibm23a}{}}%
---------. 2023b. {``What Is {Artificial Intelligence} ({AI}) ?
\textbar{} {IBM}.''} IBM. 2023.
\url{https://www.ibm.com/topics/artificial-intelligence}.

\leavevmode\vadjust pre{\hypertarget{ref-janssen17}{}}%
Janssen, Marco A. 2017. {``The {Practice} of {Archiving Model Code} of
{Agent-Based Models}.''} \emph{Journal of Artificial Societies and
Social Simulation} 20 (1): 2.

\leavevmode\vadjust pre{\hypertarget{ref-janssen_etal20}{}}%
Janssen, Marco A., Calvin Pritchard, and Allen Lee. 2020. {``On Code
Sharing and Model Documentation of Published Individual and Agent-Based
Models.''} \emph{Environmental Modelling \& Software} 134 (December):
104873. \url{https://doi.org/10.1016/j.envsoft.2020.104873}.

\leavevmode\vadjust pre{\hypertarget{ref-just13}{}}%
Just, Jérémy. 2013. {``Les Distributions - {Groupe} Francophone Des
{Utilisateurs} de {TeX}, {LaTeX} Et Logiciels Compagnons.''} March 25,
2013. \url{https://www.gutenberg-asso.fr/Les-distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-kabacoff22}{}}%
Kabacoff, Robert. 2022. \emph{R in Action: Data Analysis and Graphics
with {R} and {Tidyverse}}. Third edition. Shelter Island, NY: Manning
Publications.

\leavevmode\vadjust pre{\hypertarget{ref-karjalainen10}{}}%
Karjalainen, Martti. 2010. {``Large-Scale Migration to an Open Source
Office Suite: {An} Innovation Adoption Study in {Finland}.''} University
of Tampere.
\url{https://trepo.tuni.fi/bitstream/handle/10024/66646/978-951-44-8216-8.pdf?sequence=1}.

\leavevmode\vadjust pre{\hypertarget{ref-kibbee23}{}}%
Kibbee, Matthew. 2023. {``A {Guide} to {Evidence Synthesis}:''} Cornell
University Library Evidence Synthesis Service. 2023.
\url{https://guides.library.cornell.edu/evidence-synthesis/service}.

\leavevmode\vadjust pre{\hypertarget{ref-king_etal21}{}}%
King, Gary, Robert O. Keohane, and Sidney Verba. 2021. \emph{Designing
{Social Inquiry}. {Scientific Inference} in {Qualitative Research}.}
Nouvelle édition. Princenton: Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-knauff_nejasmic14}{}}%
Knauff, Markus, and Jelica Nejasmic. 2014. {``An {Efficiency Comparison}
of {Document Preparation Systems Used} in {Academic Research} and
{Development}.''} Edited by Cynthia Gibas. \emph{PLoS ONE} 9 (12):
e115069. \url{https://doi.org/10.1371/journal.pone.0115069}.

\leavevmode\vadjust pre{\hypertarget{ref-konig_etal22}{}}%
König, Pascal D., Tobias D. Krafft, Wolfgang Schulz, and Katharina A.
Zweig. 2022. {``Essence of {AI}. {What Is AI}?''} In \emph{The
{Cambridge Handbook} of {Artificial Intelligence}}, edited by and Michel
Cannarsa Cristina Poncibò, 18--34. Cambrigde: Cambridge University
Press. \url{https://doi.org/10.1017/9781009072168.005}.

\leavevmode\vadjust pre{\hypertarget{ref-kostoff_cummings13}{}}%
Kostoff, Ronald N., and Russell M. Cummings. 2013. {``Highly Cited
Literature of High-Speed Compressible Flow Research.''} \emph{Aerospace
Science and Technology} 26 (1): 216--34.
\url{https://doi.org/10.1016/j.ast.2012.04.006}.

\leavevmode\vadjust pre{\hypertarget{ref-krahmer_etal23}{}}%
Krähmer, Daniel, Laura Schächtele, and Andreas Schneck. 2023. {``Care to
Share? {Experimental} Evidence on Code Sharing Behavior in the Social
Sciences.''} \emph{PLOS ONE} 18 (8): e0289380.
\url{https://doi.org/10.1371/journal.pone.0289380}.

\leavevmode\vadjust pre{\hypertarget{ref-markdownguide23}{}}%
Markdown Guide. 2023. \emph{Getting {Started}}.
\url{https://www.markdownguide.org/getting-started/}.

\leavevmode\vadjust pre{\hypertarget{ref-marres17}{}}%
Marres, Noortje. 2017. \emph{Digital {Sociology}. {The Reinvention} of
{Social Research}.} Cambridge: Polity.

\leavevmode\vadjust pre{\hypertarget{ref-mcadoo23}{}}%
McAdoo, Timothy. 2023. {``How to Cite {ChatGPT}.''}
https://apastyle.apa.org. April 7, 2023.
\url{https://apastyle.apa.org/blog/how-to-cite-chatgpt}.

\leavevmode\vadjust pre{\hypertarget{ref-mccarthy07}{}}%
McCarthy, John. 2007. {``{WHAT IS ARTIFICIAL INTELLIGENCE}?''}

\leavevmode\vadjust pre{\hypertarget{ref-morandat_etal12}{}}%
Morandat, Floréal, Brandon Hill, Leo Osvald, and Jan Vitek. 2012.
{``Evaluating the {Design} of the {R Language}.''} In \emph{{ECOOP} 2012
-- {Object-Oriented Programming}: 26th {European Conference}, {Beijing},
{China}, {June} 11-16, 2012. {Proceedings}}, edited by James Noble,
7313:104--31. Lecture {Notes} in {Computer Science}. Berlin, Heidelberg:
Springer Berlin Heidelberg.
\url{https://doi.org/10.1007/978-3-642-31057-7}.

\leavevmode\vadjust pre{\hypertarget{ref-muenchen11}{}}%
Muenchen, Robert A. 2011. \emph{R for {SAS} and {SPSS} Users}. 2nd ed.
Statistics and Computing. New York: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-opensourceinitiative06}{}}%
Open Source Initiative. 2006. {``The {Open Source Definition}.''} Open
Source Initiative. July 7, 2006. \url{https://opensource.org/osd/}.

\leavevmode\vadjust pre{\hypertarget{ref-park_etal23}{}}%
Park, Joon Sung, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel
Morris, Percy Liang, and Michael S. Bernstein. 2023. {``Generative
{Agents}: {Interactive Simulacra} of {Human Behavior}.''} \emph{arXiv}.
\url{https://doi.org/10.48550/arXiv.2304.03442}.

\leavevmode\vadjust pre{\hypertarget{ref-paura_arhipova12}{}}%
Paura, Liga, and Irina Arhipova. 2012. {``Advantages and {Disadvantages}
of {Professional} and {Free Software} for {Teaching Statistics}.''}
\emph{Information Technology and Management Science} 15 (1).
\url{https://doi.org/10.2478/v10313-012-0001-z}.

\leavevmode\vadjust pre{\hypertarget{ref-peterson_etal21}{}}%
Peterson, Joshua C., David D. Bourgin, Mayank Agrawal, Daniel Reichman,
and Thomas L. Griffiths. 2021. {``Using {Large-Scale Experiments} and
{Machine Learning} to {Discover Theories} of {Human Decision-Making}.''}
\emph{Science} 372 (6547): 1209--14.
\url{https://doi.org/10.1126/science.abe2629}.

\leavevmode\vadjust pre{\hypertarget{ref-quarto23}{}}%
Quarto. 2023. \emph{Get {Started}}.
\url{https://quarto.org/docs/get-started/}.

\leavevmode\vadjust pre{\hypertarget{ref-racz_markovic18}{}}%
Racz, Aleksandar, and Suzana Marković. 2018. {``{`{Worth}(less) Papers'}
-- Are Journal Impact Factor and Number of Citations Suitable Indicators
to Evaluate Quality of Scientists?''} \emph{Nova Prisutnost} XVI (2):
369--88. \url{https://doi.org/10.31192/np.16.2.10}.

\leavevmode\vadjust pre{\hypertarget{ref-roser22}{}}%
Roser, Max. 2022. {``The {Brief History} of {Artificial Intelligence}:
{The World Has Changed Fast} -- {What Might Be Next}?''} Our World in
Data. December 6, 2022.
\url{https://ourworldindata.org/brief-history-of-ai}.

\leavevmode\vadjust pre{\hypertarget{ref-santillan-anguiano_gonzalez-machado23}{}}%
Santillán-Anguiano, Ernesto Israel, and Emilia Cristina
González-Machado. 2023. {``Advantages of a {Free Software Culture} for
{Qualitative Researchers} in the {Social Sciences}.''}
\emph{International Journal of Qualitative Research} 3 (1): 97--103.
\url{https://www.ojs.literacyinstitute.org/index.php/ijqr/article/view/841}.

\leavevmode\vadjust pre{\hypertarget{ref-sarkar08}{}}%
Sarkar, Deepayan. 2008. \emph{Lattice: {Multivariate Data Visualization}
with {R}}. New York, NY: Springer New York.
\url{https://doi.org/10.1007/978-0-387-75969-2}.

\leavevmode\vadjust pre{\hypertarget{ref-sarkar23}{}}%
---------. 2023. {``Trellis {Graphics} for {R}.''}
\url{https://cran.r-project.org/web/packages/lattice/lattice.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-sartre96}{}}%
Sartre, Jean-Paul. 1996. \emph{L'existentialisme Est Un Humanisme}.
Paris: Gallimard.

\leavevmode\vadjust pre{\hypertarget{ref-smith02}{}}%
Smith, Bradford L. 2002. {``The {Future} of {Software}: {Enabling} the
{Marketplace} to {Decide}.''} In \emph{Government {Policy} Toward {Open
Source Software}}, edited by Robert W. Hahn, 69--86. Brookings
Institution Press.
\url{https://www.jstor.org/stable/10.7864/j.ctvbd8kmv.8}.

\leavevmode\vadjust pre{\hypertarget{ref-stackoverflow23}{}}%
Stack Overflow. 2023. \emph{Stack {Overflow}}.
\url{https://stackoverflow.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-stallman86}{}}%
Stallman, Richard. 1986. {``{GNU}'s {Bulletin}.''} 1986.
\url{https://www.gnu.org/bulletins/bull1.txt}.

\leavevmode\vadjust pre{\hypertarget{ref-stallman22}{}}%
---------. 2022. {``En Quoi l'open Source Perd de Vue l'éthique Du
Logiciel Libre - {Projet GNU} - {Free Software Foundation}.''} Système
d'exploitation GNU. 2022.
\url{https://www.gnu.org/philosophy/open-source-misses-the-point.html}.

\leavevmode\vadjust pre{\hypertarget{ref-systemedexploitationgnu23}{}}%
Système d'exploitation GNU. 2023. {``Catégories de Logiciels Libres Et
Non Libres - {Projet GNU} - {Free Software Foundation}.''} Système
d'exploitation GNU. 2023.
\url{https://www.gnu.org/philosophy/categories.html\#}.

\leavevmode\vadjust pre{\hypertarget{ref-tippmann15}{}}%
Tippmann, Sylvia. 2015. {``Programming Tools: {Adventures} with {R}.''}
\emph{Nature} 517 (7532): 109--10.
\url{https://doi.org/10.1038/517109a}.

\leavevmode\vadjust pre{\hypertarget{ref-wang19}{}}%
Wang, Pei. 2019. {``On {Defining Artificial Intelligence}.''}
\emph{Journal of Artificial General Intelligence} 10 (2): 1--37.
\url{https://doi.org/10.2478/jagi-2019-0002}.

\leavevmode\vadjust pre{\hypertarget{ref-weise_metz23}{}}%
Weise, Karen, and Cade Metz. 2023. {``When {A}.{I}. {Chatbots
Hallucinate}.''} \emph{The New York Times}, May 9, 2023.
\url{https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham09}{}}%
Wickham, Hadley. 2009. \emph{Ggplot2: {Elegant Graphics} for {Data
Analysis}}. New York, NY: Springer New York.
\url{https://doi.org/10.1007/978-0-387-98141-3}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_etal19}{}}%
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy
McGowan, Romain François, Garrett Grolemund, et al. 2019. {``Welcome to
the {Tidyverse}.''} \emph{Journal of Open Source Software} 4 (43): 1686.
\url{https://doi.org/10.21105/joss.01686}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_etal23}{}}%
Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.
\emph{R for Data Science: Import, Tidy, Transform, Visualize, and Model
Data}. Second edition. Beijing: O'Reilly.

\leavevmode\vadjust pre{\hypertarget{ref-williams_etal10}{}}%
Williams, Sam, Richard M Stallman, and Christophe Masutti. 2010.
\emph{Richard Stallman et la révolution du logiciel libre - Une
biographie autorisée}. s.l.: Eyrolles.
\url{https://iso.framadvd.org/standard/content2011/ubuntu/Data/Documents/pdf/framabooks/framabook6_stallman_v1_gnu-fdl.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-worldwidewebconsortiumw3c98}{}}%
World Wide Web Consortium (W3C). 1998. \emph{Extensible {Markup
Language} ({XML}) 1.0}.
\url{https://www.w3.org/TR/1998/REC-xml-19980210.html}.

\leavevmode\vadjust pre{\hypertarget{ref-Wow}{}}%
{``Wow! {Ten} Million Users!''} n.d. Accessed March 9, 2024.
\url{https://www.overleaf.com/blog/wow-ten-million-users}.

\leavevmode\vadjust pre{\hypertarget{ref-xie23}{}}%
Xie, Yihui. 2023. \emph{Markdown}.
\url{https://github.com/rstudio/markdown}.

\leavevmode\vadjust pre{\hypertarget{ref-yu_munoz-justicia22}{}}%
Yu, Jingyuan, and Juan Muñoz-Justicia. 2022. {``Free and {Low-Cost
Twitter Research Software Tools} for {Social Science}.''} \emph{Social
Science Computer Review} 40 (1): 124--49.
\url{https://doi.org/10.1177/0894439320904318}.

\leavevmode\vadjust pre{\hypertarget{ref-zaid_etal17}{}}%
Zaid, Yasmin Hanafi, Sarimah Shamsudin, and Hadina Habil. 2017.
{``Exploring {Citations} in {Chemical Engineering Literature Review}.''}
\emph{LSP International Journal} 4 (1, 1).
\url{https://doi.org/10.11113/lspi.v4n1.46}.

\end{CSLReferences}


\backmatter

\end{document}
